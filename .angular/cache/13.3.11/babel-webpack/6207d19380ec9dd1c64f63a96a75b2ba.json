{"ast":null,"code":"import _asyncToGenerator from \"/Users/imalzy/Documents/angular/technical-req-angular-13/node_modules/@babel/runtime/helpers/esm/asyncToGenerator.js\";\nimport { _getProvider, getApp, _removeServiceInstance, _registerComponent, registerVersion, SDK_VERSION as SDK_VERSION$1 } from '@firebase/app';\nimport { Component } from '@firebase/component';\nimport { Logger, LogLevel } from '@firebase/logger';\nimport { inspect, TextEncoder, TextDecoder } from 'util';\nimport { FirebaseError, createMockUserToken, getModularInstance, deepEqual, getDefaultEmulatorHostnameAndPort, getUA, isIndexedDBAvailable, isSafari } from '@firebase/util';\nimport { randomBytes as randomBytes$1 } from 'crypto';\nimport { Integer, Md5 } from '@firebase/webchannel-wrapper';\nimport * as grpc from '@grpc/grpc-js';\nimport * as protoLoader from '@grpc/proto-loader';\nconst name = \"@firebase/firestore\";\nconst version$1 = \"3.13.0\";\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Simple wrapper around a nullable UID. Mostly exists to make code more\r\n * readable.\r\n */\n\nclass User {\n  constructor(uid) {\n    this.uid = uid;\n  }\n\n  isAuthenticated() {\n    return this.uid != null;\n  }\n  /**\r\n   * Returns a key representing this user, suitable for inclusion in a\r\n   * dictionary.\r\n   */\n\n\n  toKey() {\n    if (this.isAuthenticated()) {\n      return 'uid:' + this.uid;\n    } else {\n      return 'anonymous-user';\n    }\n  }\n\n  isEqual(otherUser) {\n    return otherUser.uid === this.uid;\n  }\n\n}\n/** A user with a null UID. */\n\n\nUser.UNAUTHENTICATED = new User(null); // TODO(mikelehen): Look into getting a proper uid-equivalent for\n// non-FirebaseAuth providers.\n\nUser.GOOGLE_CREDENTIALS = new User('google-credentials-uid');\nUser.FIRST_PARTY = new User('first-party-uid');\nUser.MOCK_USER = new User('mock-user');\nconst version = \"9.23.0\";\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\nlet SDK_VERSION = version;\n\nfunction setSDKVersion(version) {\n  SDK_VERSION = version;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Formats an object as a JSON string, suitable for logging. */\n\n\nfunction formatJSON(value) {\n  // util.inspect() results in much more readable output than JSON.stringify()\n  return inspect(value, {\n    depth: 100\n  });\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst logClient = new Logger('@firebase/firestore'); // Helper methods are needed because variables can't be exported as read/write\n\nfunction getLogLevel() {\n  return logClient.logLevel;\n}\n/**\r\n * Sets the verbosity of Cloud Firestore logs (debug, error, or silent).\r\n *\r\n * @param logLevel - The verbosity you set for activity and error logging. Can\r\n *   be any of the following values:\r\n *\r\n *   <ul>\r\n *     <li>`debug` for the most verbose logging level, primarily for\r\n *     debugging.</li>\r\n *     <li>`error` to log errors only.</li>\r\n *     <li><code>`silent` to turn off logging.</li>\r\n *   </ul>\r\n */\n\n\nfunction setLogLevel(logLevel) {\n  logClient.setLogLevel(logLevel);\n}\n\nfunction logDebug(msg, ...obj) {\n  if (logClient.logLevel <= LogLevel.DEBUG) {\n    const args = obj.map(argToString);\n    logClient.debug(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\n  }\n}\n\nfunction logError(msg, ...obj) {\n  if (logClient.logLevel <= LogLevel.ERROR) {\n    const args = obj.map(argToString);\n    logClient.error(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\n  }\n}\n/**\r\n * @internal\r\n */\n\n\nfunction logWarn(msg, ...obj) {\n  if (logClient.logLevel <= LogLevel.WARN) {\n    const args = obj.map(argToString);\n    logClient.warn(`Firestore (${SDK_VERSION}): ${msg}`, ...args);\n  }\n}\n/**\r\n * Converts an additional log parameter to a string representation.\r\n */\n\n\nfunction argToString(obj) {\n  if (typeof obj === 'string') {\n    return obj;\n  } else {\n    try {\n      return formatJSON(obj);\n    } catch (e) {\n      // Converting to JSON failed, just log the object directly\n      return obj;\n    }\n  }\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Unconditionally fails, throwing an Error with the given message.\r\n * Messages are stripped in production builds.\r\n *\r\n * Returns `never` and can be used in expressions:\r\n * @example\r\n * let futureVar = fail('not implemented yet');\r\n */\n\n\nfunction fail(failure = 'Unexpected state') {\n  // Log the failure in addition to throw an exception, just in case the\n  // exception is swallowed.\n  const message = `FIRESTORE (${SDK_VERSION}) INTERNAL ASSERTION FAILED: ` + failure;\n  logError(message); // NOTE: We don't use FirestoreError here because these are internal failures\n  // that cannot be handled by the user. (Also it would create a circular\n  // dependency between the error and assert modules which doesn't work.)\n\n  throw new Error(message);\n}\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * Messages are stripped in production builds.\r\n */\n\n\nfunction hardAssert(assertion, message) {\n  if (!assertion) {\n    fail();\n  }\n}\n/**\r\n * Fails if the given assertion condition is false, throwing an Error with the\r\n * given message if it did.\r\n *\r\n * The code of callsites invoking this function are stripped out in production\r\n * builds. Any side-effects of code within the debugAssert() invocation will not\r\n * happen in this case.\r\n *\r\n * @internal\r\n */\n\n\nfunction debugAssert(assertion, message) {\n  if (!assertion) {\n    fail();\n  }\n}\n/**\r\n * Casts `obj` to `T`. In non-production builds, verifies that `obj` is an\r\n * instance of `T` before casting.\r\n */\n\n\nfunction debugCast(obj, // eslint-disable-next-line @typescript-eslint/no-explicit-any\nconstructor) {\n  return obj;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst Code = {\n  // Causes are copied from:\n  // https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\n\n  /** Not an error; returned on success. */\n  OK: 'ok',\n\n  /** The operation was cancelled (typically by the caller). */\n  CANCELLED: 'cancelled',\n\n  /** Unknown error or an error from a different error domain. */\n  UNKNOWN: 'unknown',\n\n  /**\r\n   * Client specified an invalid argument. Note that this differs from\r\n   * FAILED_PRECONDITION. INVALID_ARGUMENT indicates arguments that are\r\n   * problematic regardless of the state of the system (e.g., a malformed file\r\n   * name).\r\n   */\n  INVALID_ARGUMENT: 'invalid-argument',\n\n  /**\r\n   * Deadline expired before operation could complete. For operations that\r\n   * change the state of the system, this error may be returned even if the\r\n   * operation has completed successfully. For example, a successful response\r\n   * from a server could have been delayed long enough for the deadline to\r\n   * expire.\r\n   */\n  DEADLINE_EXCEEDED: 'deadline-exceeded',\n\n  /** Some requested entity (e.g., file or directory) was not found. */\n  NOT_FOUND: 'not-found',\n\n  /**\r\n   * Some entity that we attempted to create (e.g., file or directory) already\r\n   * exists.\r\n   */\n  ALREADY_EXISTS: 'already-exists',\n\n  /**\r\n   * The caller does not have permission to execute the specified operation.\r\n   * PERMISSION_DENIED must not be used for rejections caused by exhausting\r\n   * some resource (use RESOURCE_EXHAUSTED instead for those errors).\r\n   * PERMISSION_DENIED must not be used if the caller can not be identified\r\n   * (use UNAUTHENTICATED instead for those errors).\r\n   */\n  PERMISSION_DENIED: 'permission-denied',\n\n  /**\r\n   * The request does not have valid authentication credentials for the\r\n   * operation.\r\n   */\n  UNAUTHENTICATED: 'unauthenticated',\n\n  /**\r\n   * Some resource has been exhausted, perhaps a per-user quota, or perhaps the\r\n   * entire file system is out of space.\r\n   */\n  RESOURCE_EXHAUSTED: 'resource-exhausted',\n\n  /**\r\n   * Operation was rejected because the system is not in a state required for\r\n   * the operation's execution. For example, directory to be deleted may be\r\n   * non-empty, an rmdir operation is applied to a non-directory, etc.\r\n   *\r\n   * A litmus test that may help a service implementor in deciding\r\n   * between FAILED_PRECONDITION, ABORTED, and UNAVAILABLE:\r\n   *  (a) Use UNAVAILABLE if the client can retry just the failing call.\r\n   *  (b) Use ABORTED if the client should retry at a higher-level\r\n   *      (e.g., restarting a read-modify-write sequence).\r\n   *  (c) Use FAILED_PRECONDITION if the client should not retry until\r\n   *      the system state has been explicitly fixed. E.g., if an \"rmdir\"\r\n   *      fails because the directory is non-empty, FAILED_PRECONDITION\r\n   *      should be returned since the client should not retry unless\r\n   *      they have first fixed up the directory by deleting files from it.\r\n   *  (d) Use FAILED_PRECONDITION if the client performs conditional\r\n   *      REST Get/Update/Delete on a resource and the resource on the\r\n   *      server does not match the condition. E.g., conflicting\r\n   *      read-modify-write on the same resource.\r\n   */\n  FAILED_PRECONDITION: 'failed-precondition',\n\n  /**\r\n   * The operation was aborted, typically due to a concurrency issue like\r\n   * sequencer check failures, transaction aborts, etc.\r\n   *\r\n   * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n   * and UNAVAILABLE.\r\n   */\n  ABORTED: 'aborted',\n\n  /**\r\n   * Operation was attempted past the valid range. E.g., seeking or reading\r\n   * past end of file.\r\n   *\r\n   * Unlike INVALID_ARGUMENT, this error indicates a problem that may be fixed\r\n   * if the system state changes. For example, a 32-bit file system will\r\n   * generate INVALID_ARGUMENT if asked to read at an offset that is not in the\r\n   * range [0,2^32-1], but it will generate OUT_OF_RANGE if asked to read from\r\n   * an offset past the current file size.\r\n   *\r\n   * There is a fair bit of overlap between FAILED_PRECONDITION and\r\n   * OUT_OF_RANGE. We recommend using OUT_OF_RANGE (the more specific error)\r\n   * when it applies so that callers who are iterating through a space can\r\n   * easily look for an OUT_OF_RANGE error to detect when they are done.\r\n   */\n  OUT_OF_RANGE: 'out-of-range',\n\n  /** Operation is not implemented or not supported/enabled in this service. */\n  UNIMPLEMENTED: 'unimplemented',\n\n  /**\r\n   * Internal errors. Means some invariants expected by underlying System has\r\n   * been broken. If you see one of these errors, Something is very broken.\r\n   */\n  INTERNAL: 'internal',\n\n  /**\r\n   * The service is currently unavailable. This is a most likely a transient\r\n   * condition and may be corrected by retrying with a backoff.\r\n   *\r\n   * See litmus test above for deciding between FAILED_PRECONDITION, ABORTED,\r\n   * and UNAVAILABLE.\r\n   */\n  UNAVAILABLE: 'unavailable',\n\n  /** Unrecoverable data loss or corruption. */\n  DATA_LOSS: 'data-loss'\n};\n/** An error returned by a Firestore operation. */\n\nclass FirestoreError extends FirebaseError {\n  /** @hideconstructor */\n  constructor(\n  /**\r\n   * The backend error code associated with this error.\r\n   */\n  code,\n  /**\r\n   * A custom error description.\r\n   */\n  message) {\n    super(code, message);\n    this.code = code;\n    this.message = message; // HACK: We write a toString property directly because Error is not a real\n    // class and so inheritance does not work correctly. We could alternatively\n    // do the same \"back-door inheritance\" trick that FirebaseError does.\n\n    this.toString = () => `${this.name}: [code=${this.code}]: ${this.message}`;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass Deferred {\n  constructor() {\n    this.promise = new Promise((resolve, reject) => {\n      this.resolve = resolve;\n      this.reject = reject;\n    });\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass OAuthToken {\n  constructor(value, user) {\n    this.user = user;\n    this.type = 'OAuth';\n    this.headers = new Map();\n    this.headers.set('Authorization', `Bearer ${value}`);\n  }\n\n}\n/**\r\n * A CredentialsProvider that always yields an empty token.\r\n * @internal\r\n */\n\n\nclass EmptyAuthCredentialsProvider {\n  getToken() {\n    return Promise.resolve(null);\n  }\n\n  invalidateToken() {}\n\n  start(asyncQueue, changeListener) {\n    // Fire with initial user.\n    asyncQueue.enqueueRetryable(() => changeListener(User.UNAUTHENTICATED));\n  }\n\n  shutdown() {}\n\n}\n/**\r\n * A CredentialsProvider that always returns a constant token. Used for\r\n * emulator token mocking.\r\n */\n\n\nclass EmulatorAuthCredentialsProvider {\n  constructor(token) {\n    this.token = token;\n    /**\r\n     * Stores the listener registered with setChangeListener()\r\n     * This isn't actually necessary since the UID never changes, but we use this\r\n     * to verify the listen contract is adhered to in tests.\r\n     */\n\n    this.changeListener = null;\n  }\n\n  getToken() {\n    return Promise.resolve(this.token);\n  }\n\n  invalidateToken() {}\n\n  start(asyncQueue, changeListener) {\n    this.changeListener = changeListener; // Fire with initial user.\n\n    asyncQueue.enqueueRetryable(() => changeListener(this.token.user));\n  }\n\n  shutdown() {\n    this.changeListener = null;\n  }\n\n}\n\nclass FirebaseAuthCredentialsProvider {\n  constructor(authProvider) {\n    this.authProvider = authProvider;\n    /** Tracks the current User. */\n\n    this.currentUser = User.UNAUTHENTICATED;\n    /**\r\n     * Counter used to detect if the token changed while a getToken request was\r\n     * outstanding.\r\n     */\n\n    this.tokenCounter = 0;\n    this.forceRefresh = false;\n    this.auth = null;\n  }\n\n  start(asyncQueue, changeListener) {\n    var _this = this;\n\n    let lastTokenId = this.tokenCounter; // A change listener that prevents double-firing for the same token change.\n\n    const guardedChangeListener = user => {\n      if (this.tokenCounter !== lastTokenId) {\n        lastTokenId = this.tokenCounter;\n        return changeListener(user);\n      } else {\n        return Promise.resolve();\n      }\n    }; // A promise that can be waited on to block on the next token change.\n    // This promise is re-created after each change.\n\n\n    let nextToken = new Deferred();\n\n    this.tokenListener = () => {\n      this.tokenCounter++;\n      this.currentUser = this.getUser();\n      nextToken.resolve();\n      nextToken = new Deferred();\n      asyncQueue.enqueueRetryable(() => guardedChangeListener(this.currentUser));\n    };\n\n    const awaitNextToken = () => {\n      const currentTokenAttempt = nextToken;\n      asyncQueue.enqueueRetryable( /*#__PURE__*/_asyncToGenerator(function* () {\n        yield currentTokenAttempt.promise;\n        yield guardedChangeListener(_this.currentUser);\n      }));\n    };\n\n    const registerAuth = auth => {\n      logDebug('FirebaseAuthCredentialsProvider', 'Auth detected');\n      this.auth = auth;\n      this.auth.addAuthTokenListener(this.tokenListener);\n      awaitNextToken();\n    };\n\n    this.authProvider.onInit(auth => registerAuth(auth)); // Our users can initialize Auth right after Firestore, so we give it\n    // a chance to register itself with the component framework before we\n    // determine whether to start up in unauthenticated mode.\n\n    setTimeout(() => {\n      if (!this.auth) {\n        const auth = this.authProvider.getImmediate({\n          optional: true\n        });\n\n        if (auth) {\n          registerAuth(auth);\n        } else {\n          // If auth is still not available, proceed with `null` user\n          logDebug('FirebaseAuthCredentialsProvider', 'Auth not yet detected');\n          nextToken.resolve();\n          nextToken = new Deferred();\n        }\n      }\n    }, 0);\n    awaitNextToken();\n  }\n\n  getToken() {\n    // Take note of the current value of the tokenCounter so that this method\n    // can fail (with an ABORTED error) if there is a token change while the\n    // request is outstanding.\n    const initialTokenCounter = this.tokenCounter;\n    const forceRefresh = this.forceRefresh;\n    this.forceRefresh = false;\n\n    if (!this.auth) {\n      return Promise.resolve(null);\n    }\n\n    return this.auth.getToken(forceRefresh).then(tokenData => {\n      // Cancel the request since the token changed while the request was\n      // outstanding so the response is potentially for a previous user (which\n      // user, we can't be sure).\n      if (this.tokenCounter !== initialTokenCounter) {\n        logDebug('FirebaseAuthCredentialsProvider', 'getToken aborted due to token change.');\n        return this.getToken();\n      } else {\n        if (tokenData) {\n          hardAssert(typeof tokenData.accessToken === 'string');\n          return new OAuthToken(tokenData.accessToken, this.currentUser);\n        } else {\n          return null;\n        }\n      }\n    });\n  }\n\n  invalidateToken() {\n    this.forceRefresh = true;\n  }\n\n  shutdown() {\n    if (this.auth) {\n      this.auth.removeAuthTokenListener(this.tokenListener);\n    }\n  } // Auth.getUid() can return null even with a user logged in. It is because\n  // getUid() is synchronous, but the auth code populating Uid is asynchronous.\n  // This method should only be called in the AuthTokenListener callback\n  // to guarantee to get the actual user.\n\n\n  getUser() {\n    const currentUid = this.auth && this.auth.getUid();\n    hardAssert(currentUid === null || typeof currentUid === 'string');\n    return new User(currentUid);\n  }\n\n}\n/*\r\n * FirstPartyToken provides a fresh token each time its value\r\n * is requested, because if the token is too old, requests will be rejected.\r\n * Technically this may no longer be necessary since the SDK should gracefully\r\n * recover from unauthenticated errors (see b/33147818 for context), but it's\r\n * safer to keep the implementation as-is.\r\n */\n\n\nclass FirstPartyToken {\n  constructor(sessionIndex, iamToken, authTokenFactory) {\n    this.sessionIndex = sessionIndex;\n    this.iamToken = iamToken;\n    this.authTokenFactory = authTokenFactory;\n    this.type = 'FirstParty';\n    this.user = User.FIRST_PARTY;\n    this._headers = new Map();\n  }\n  /**\r\n   * Gets an authorization token, using a provided factory function, or return\r\n   * null.\r\n   */\n\n\n  getAuthToken() {\n    if (this.authTokenFactory) {\n      return this.authTokenFactory();\n    } else {\n      return null;\n    }\n  }\n\n  get headers() {\n    this._headers.set('X-Goog-AuthUser', this.sessionIndex); // Use array notation to prevent minification\n\n\n    const authHeaderTokenValue = this.getAuthToken();\n\n    if (authHeaderTokenValue) {\n      this._headers.set('Authorization', authHeaderTokenValue);\n    }\n\n    if (this.iamToken) {\n      this._headers.set('X-Goog-Iam-Authorization-Token', this.iamToken);\n    }\n\n    return this._headers;\n  }\n\n}\n/*\r\n * Provides user credentials required for the Firestore JavaScript SDK\r\n * to authenticate the user, using technique that is only available\r\n * to applications hosted by Google.\r\n */\n\n\nclass FirstPartyAuthCredentialsProvider {\n  constructor(sessionIndex, iamToken, authTokenFactory) {\n    this.sessionIndex = sessionIndex;\n    this.iamToken = iamToken;\n    this.authTokenFactory = authTokenFactory;\n  }\n\n  getToken() {\n    return Promise.resolve(new FirstPartyToken(this.sessionIndex, this.iamToken, this.authTokenFactory));\n  }\n\n  start(asyncQueue, changeListener) {\n    // Fire with initial uid.\n    asyncQueue.enqueueRetryable(() => changeListener(User.FIRST_PARTY));\n  }\n\n  shutdown() {}\n\n  invalidateToken() {}\n\n}\n\nclass AppCheckToken {\n  constructor(value) {\n    this.value = value;\n    this.type = 'AppCheck';\n    this.headers = new Map();\n\n    if (value && value.length > 0) {\n      this.headers.set('x-firebase-appcheck', this.value);\n    }\n  }\n\n}\n\nclass FirebaseAppCheckTokenProvider {\n  constructor(appCheckProvider) {\n    this.appCheckProvider = appCheckProvider;\n    this.forceRefresh = false;\n    this.appCheck = null;\n    this.latestAppCheckToken = null;\n  }\n\n  start(asyncQueue, changeListener) {\n    const onTokenChanged = tokenResult => {\n      if (tokenResult.error != null) {\n        logDebug('FirebaseAppCheckTokenProvider', `Error getting App Check token; using placeholder token instead. Error: ${tokenResult.error.message}`);\n      }\n\n      const tokenUpdated = tokenResult.token !== this.latestAppCheckToken;\n      this.latestAppCheckToken = tokenResult.token;\n      logDebug('FirebaseAppCheckTokenProvider', `Received ${tokenUpdated ? 'new' : 'existing'} token.`);\n      return tokenUpdated ? changeListener(tokenResult.token) : Promise.resolve();\n    };\n\n    this.tokenListener = tokenResult => {\n      asyncQueue.enqueueRetryable(() => onTokenChanged(tokenResult));\n    };\n\n    const registerAppCheck = appCheck => {\n      logDebug('FirebaseAppCheckTokenProvider', 'AppCheck detected');\n      this.appCheck = appCheck;\n      this.appCheck.addTokenListener(this.tokenListener);\n    };\n\n    this.appCheckProvider.onInit(appCheck => registerAppCheck(appCheck)); // Our users can initialize AppCheck after Firestore, so we give it\n    // a chance to register itself with the component framework.\n\n    setTimeout(() => {\n      if (!this.appCheck) {\n        const appCheck = this.appCheckProvider.getImmediate({\n          optional: true\n        });\n\n        if (appCheck) {\n          registerAppCheck(appCheck);\n        } else {\n          // If AppCheck is still not available, proceed without it.\n          logDebug('FirebaseAppCheckTokenProvider', 'AppCheck not yet detected');\n        }\n      }\n    }, 0);\n  }\n\n  getToken() {\n    const forceRefresh = this.forceRefresh;\n    this.forceRefresh = false;\n\n    if (!this.appCheck) {\n      return Promise.resolve(null);\n    }\n\n    return this.appCheck.getToken(forceRefresh).then(tokenResult => {\n      if (tokenResult) {\n        hardAssert(typeof tokenResult.token === 'string');\n        this.latestAppCheckToken = tokenResult.token;\n        return new AppCheckToken(tokenResult.token);\n      } else {\n        return null;\n      }\n    });\n  }\n\n  invalidateToken() {\n    this.forceRefresh = true;\n  }\n\n  shutdown() {\n    if (this.appCheck) {\n      this.appCheck.removeTokenListener(this.tokenListener);\n    }\n  }\n\n}\n/**\r\n * An AppCheck token provider that always yields an empty token.\r\n * @internal\r\n */\n\n\nclass EmptyAppCheckTokenProvider {\n  getToken() {\n    return Promise.resolve(new AppCheckToken(''));\n  }\n\n  invalidateToken() {}\n\n  start(asyncQueue, changeListener) {}\n\n  shutdown() {}\n\n}\n/**\r\n * Builds a CredentialsProvider depending on the type of\r\n * the credentials passed in.\r\n */\n\n\nfunction makeAuthCredentialsProvider(credentials) {\n  if (!credentials) {\n    return new EmptyAuthCredentialsProvider();\n  }\n\n  switch (credentials['type']) {\n    case 'firstParty':\n      return new FirstPartyAuthCredentialsProvider(credentials['sessionIndex'] || '0', credentials['iamToken'] || null, credentials['authTokenFactory'] || null);\n\n    case 'provider':\n      return credentials['client'];\n\n    default:\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'makeAuthCredentialsProvider failed due to invalid credential type');\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Generates `nBytes` of random bytes.\r\n *\r\n * If `nBytes < 0` , an error will be thrown.\r\n */\n\n\nfunction randomBytes(nBytes) {\n  return randomBytes$1(nBytes);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass AutoId {\n  static newId() {\n    // Alphanumeric characters\n    const chars = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789'; // The largest byte value that is a multiple of `char.length`.\n\n    const maxMultiple = Math.floor(256 / chars.length) * chars.length;\n    let autoId = '';\n    const targetLength = 20;\n\n    while (autoId.length < targetLength) {\n      const bytes = randomBytes(40);\n\n      for (let i = 0; i < bytes.length; ++i) {\n        // Only accept values that are [0, maxMultiple), this ensures they can\n        // be evenly mapped to indices of `chars` via a modulo operation.\n        if (autoId.length < targetLength && bytes[i] < maxMultiple) {\n          autoId += chars.charAt(bytes[i] % chars.length);\n        }\n      }\n    }\n\n    return autoId;\n  }\n\n}\n\nfunction primitiveComparator(left, right) {\n  if (left < right) {\n    return -1;\n  }\n\n  if (left > right) {\n    return 1;\n  }\n\n  return 0;\n}\n/** Helper to compare arrays using isEqual(). */\n\n\nfunction arrayEquals(left, right, comparator) {\n  if (left.length !== right.length) {\n    return false;\n  }\n\n  return left.every((value, index) => comparator(value, right[index]));\n}\n/**\r\n * Returns the immediate lexicographically-following string. This is useful to\r\n * construct an inclusive range for indexeddb iterators.\r\n */\n\n\nfunction immediateSuccessor(s) {\n  // Return the input string, with an additional NUL byte appended.\n  return s + '\\0';\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// The earliest date supported by Firestore timestamps (0001-01-01T00:00:00Z).\n\n\nconst MIN_SECONDS = -62135596800; // Number of nanoseconds in a millisecond.\n\nconst MS_TO_NANOS = 1e6;\n/**\r\n * A `Timestamp` represents a point in time independent of any time zone or\r\n * calendar, represented as seconds and fractions of seconds at nanosecond\r\n * resolution in UTC Epoch time.\r\n *\r\n * It is encoded using the Proleptic Gregorian Calendar which extends the\r\n * Gregorian calendar backwards to year one. It is encoded assuming all minutes\r\n * are 60 seconds long, i.e. leap seconds are \"smeared\" so that no leap second\r\n * table is needed for interpretation. Range is from 0001-01-01T00:00:00Z to\r\n * 9999-12-31T23:59:59.999999999Z.\r\n *\r\n * For examples and further specifications, refer to the\r\n * {@link https://github.com/google/protobuf/blob/master/src/google/protobuf/timestamp.proto | Timestamp definition}.\r\n */\n\nclass Timestamp {\n  /**\r\n   * Creates a new timestamp.\r\n   *\r\n   * @param seconds - The number of seconds of UTC time since Unix epoch\r\n   *     1970-01-01T00:00:00Z. Must be from 0001-01-01T00:00:00Z to\r\n   *     9999-12-31T23:59:59Z inclusive.\r\n   * @param nanoseconds - The non-negative fractions of a second at nanosecond\r\n   *     resolution. Negative second values with fractions must still have\r\n   *     non-negative nanoseconds values that count forward in time. Must be\r\n   *     from 0 to 999,999,999 inclusive.\r\n   */\n  constructor(\n  /**\r\n   * The number of seconds of UTC time since Unix epoch 1970-01-01T00:00:00Z.\r\n   */\n  seconds,\n  /**\r\n   * The fractions of a second at nanosecond resolution.*\r\n   */\n  nanoseconds) {\n    this.seconds = seconds;\n    this.nanoseconds = nanoseconds;\n\n    if (nanoseconds < 0) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\n    }\n\n    if (nanoseconds >= 1e9) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp nanoseconds out of range: ' + nanoseconds);\n    }\n\n    if (seconds < MIN_SECONDS) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\n    } // This will break in the year 10,000.\n\n\n    if (seconds >= 253402300800) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Timestamp seconds out of range: ' + seconds);\n    }\n  }\n  /**\r\n   * Creates a new timestamp with the current date, with millisecond precision.\r\n   *\r\n   * @returns a new timestamp representing the current date.\r\n   */\n\n\n  static now() {\n    return Timestamp.fromMillis(Date.now());\n  }\n  /**\r\n   * Creates a new timestamp from the given date.\r\n   *\r\n   * @param date - The date to initialize the `Timestamp` from.\r\n   * @returns A new `Timestamp` representing the same point in time as the given\r\n   *     date.\r\n   */\n\n\n  static fromDate(date) {\n    return Timestamp.fromMillis(date.getTime());\n  }\n  /**\r\n   * Creates a new timestamp from the given number of milliseconds.\r\n   *\r\n   * @param milliseconds - Number of milliseconds since Unix epoch\r\n   *     1970-01-01T00:00:00Z.\r\n   * @returns A new `Timestamp` representing the same point in time as the given\r\n   *     number of milliseconds.\r\n   */\n\n\n  static fromMillis(milliseconds) {\n    const seconds = Math.floor(milliseconds / 1000);\n    const nanos = Math.floor((milliseconds - seconds * 1000) * MS_TO_NANOS);\n    return new Timestamp(seconds, nanos);\n  }\n  /**\r\n   * Converts a `Timestamp` to a JavaScript `Date` object. This conversion\r\n   * causes a loss of precision since `Date` objects only support millisecond\r\n   * precision.\r\n   *\r\n   * @returns JavaScript `Date` object representing the same point in time as\r\n   *     this `Timestamp`, with millisecond precision.\r\n   */\n\n\n  toDate() {\n    return new Date(this.toMillis());\n  }\n  /**\r\n   * Converts a `Timestamp` to a numeric timestamp (in milliseconds since\r\n   * epoch). This operation causes a loss of precision.\r\n   *\r\n   * @returns The point in time corresponding to this timestamp, represented as\r\n   *     the number of milliseconds since Unix epoch 1970-01-01T00:00:00Z.\r\n   */\n\n\n  toMillis() {\n    return this.seconds * 1000 + this.nanoseconds / MS_TO_NANOS;\n  }\n\n  _compareTo(other) {\n    if (this.seconds === other.seconds) {\n      return primitiveComparator(this.nanoseconds, other.nanoseconds);\n    }\n\n    return primitiveComparator(this.seconds, other.seconds);\n  }\n  /**\r\n   * Returns true if this `Timestamp` is equal to the provided one.\r\n   *\r\n   * @param other - The `Timestamp` to compare against.\r\n   * @returns true if this `Timestamp` is equal to the provided one.\r\n   */\n\n\n  isEqual(other) {\n    return other.seconds === this.seconds && other.nanoseconds === this.nanoseconds;\n  }\n  /** Returns a textual representation of this `Timestamp`. */\n\n\n  toString() {\n    return 'Timestamp(seconds=' + this.seconds + ', nanoseconds=' + this.nanoseconds + ')';\n  }\n  /** Returns a JSON-serializable representation of this `Timestamp`. */\n\n\n  toJSON() {\n    return {\n      seconds: this.seconds,\n      nanoseconds: this.nanoseconds\n    };\n  }\n  /**\r\n   * Converts this object to a primitive string, which allows `Timestamp` objects\r\n   * to be compared using the `>`, `<=`, `>=` and `>` operators.\r\n   */\n\n\n  valueOf() {\n    // This method returns a string of the form <seconds>.<nanoseconds> where\n    // <seconds> is translated to have a non-negative value and both <seconds>\n    // and <nanoseconds> are left-padded with zeroes to be a consistent length.\n    // Strings with this format then have a lexiographical ordering that matches\n    // the expected ordering. The <seconds> translation is done to avoid having\n    // a leading negative sign (i.e. a leading '-' character) in its string\n    // representation, which would affect its lexiographical ordering.\n    const adjustedSeconds = this.seconds - MIN_SECONDS; // Note: Up to 12 decimal digits are required to represent all valid\n    // 'seconds' values.\n\n    const formattedSeconds = String(adjustedSeconds).padStart(12, '0');\n    const formattedNanoseconds = String(this.nanoseconds).padStart(9, '0');\n    return formattedSeconds + '.' + formattedNanoseconds;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A version of a document in Firestore. This corresponds to the version\r\n * timestamp, such as update_time or read_time.\r\n */\n\n\nclass SnapshotVersion {\n  constructor(timestamp) {\n    this.timestamp = timestamp;\n  }\n\n  static fromTimestamp(value) {\n    return new SnapshotVersion(value);\n  }\n\n  static min() {\n    return new SnapshotVersion(new Timestamp(0, 0));\n  }\n\n  static max() {\n    return new SnapshotVersion(new Timestamp(253402300799, 1e9 - 1));\n  }\n\n  compareTo(other) {\n    return this.timestamp._compareTo(other.timestamp);\n  }\n\n  isEqual(other) {\n    return this.timestamp.isEqual(other.timestamp);\n  }\n  /** Returns a number representation of the version for use in spec tests. */\n\n\n  toMicroseconds() {\n    // Convert to microseconds.\n    return this.timestamp.seconds * 1e6 + this.timestamp.nanoseconds / 1000;\n  }\n\n  toString() {\n    return 'SnapshotVersion(' + this.timestamp.toString() + ')';\n  }\n\n  toTimestamp() {\n    return this.timestamp;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst DOCUMENT_KEY_NAME = '__name__';\n/**\r\n * Path represents an ordered sequence of string segments.\r\n */\n\nclass BasePath {\n  constructor(segments, offset, length) {\n    if (offset === undefined) {\n      offset = 0;\n    } else if (offset > segments.length) {\n      fail();\n    }\n\n    if (length === undefined) {\n      length = segments.length - offset;\n    } else if (length > segments.length - offset) {\n      fail();\n    }\n\n    this.segments = segments;\n    this.offset = offset;\n    this.len = length;\n  }\n\n  get length() {\n    return this.len;\n  }\n\n  isEqual(other) {\n    return BasePath.comparator(this, other) === 0;\n  }\n\n  child(nameOrPath) {\n    const segments = this.segments.slice(this.offset, this.limit());\n\n    if (nameOrPath instanceof BasePath) {\n      nameOrPath.forEach(segment => {\n        segments.push(segment);\n      });\n    } else {\n      segments.push(nameOrPath);\n    }\n\n    return this.construct(segments);\n  }\n  /** The index of one past the last segment of the path. */\n\n\n  limit() {\n    return this.offset + this.length;\n  }\n\n  popFirst(size) {\n    size = size === undefined ? 1 : size;\n    return this.construct(this.segments, this.offset + size, this.length - size);\n  }\n\n  popLast() {\n    return this.construct(this.segments, this.offset, this.length - 1);\n  }\n\n  firstSegment() {\n    return this.segments[this.offset];\n  }\n\n  lastSegment() {\n    return this.get(this.length - 1);\n  }\n\n  get(index) {\n    return this.segments[this.offset + index];\n  }\n\n  isEmpty() {\n    return this.length === 0;\n  }\n\n  isPrefixOf(other) {\n    if (other.length < this.length) {\n      return false;\n    }\n\n    for (let i = 0; i < this.length; i++) {\n      if (this.get(i) !== other.get(i)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  isImmediateParentOf(potentialChild) {\n    if (this.length + 1 !== potentialChild.length) {\n      return false;\n    }\n\n    for (let i = 0; i < this.length; i++) {\n      if (this.get(i) !== potentialChild.get(i)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  forEach(fn) {\n    for (let i = this.offset, end = this.limit(); i < end; i++) {\n      fn(this.segments[i]);\n    }\n  }\n\n  toArray() {\n    return this.segments.slice(this.offset, this.limit());\n  }\n\n  static comparator(p1, p2) {\n    const len = Math.min(p1.length, p2.length);\n\n    for (let i = 0; i < len; i++) {\n      const left = p1.get(i);\n      const right = p2.get(i);\n\n      if (left < right) {\n        return -1;\n      }\n\n      if (left > right) {\n        return 1;\n      }\n    }\n\n    if (p1.length < p2.length) {\n      return -1;\n    }\n\n    if (p1.length > p2.length) {\n      return 1;\n    }\n\n    return 0;\n  }\n\n}\n/**\r\n * A slash-separated path for navigating resources (documents and collections)\r\n * within Firestore.\r\n *\r\n * @internal\r\n */\n\n\nclass ResourcePath extends BasePath {\n  construct(segments, offset, length) {\n    return new ResourcePath(segments, offset, length);\n  }\n\n  canonicalString() {\n    // NOTE: The client is ignorant of any path segments containing escape\n    // sequences (e.g. __id123__) and just passes them through raw (they exist\n    // for legacy reasons and should not be used frequently).\n    return this.toArray().join('/');\n  }\n\n  toString() {\n    return this.canonicalString();\n  }\n  /**\r\n   * Creates a resource path from the given slash-delimited string. If multiple\r\n   * arguments are provided, all components are combined. Leading and trailing\r\n   * slashes from all components are ignored.\r\n   */\n\n\n  static fromString(...pathComponents) {\n    // NOTE: The client is ignorant of any path segments containing escape\n    // sequences (e.g. __id123__) and just passes them through raw (they exist\n    // for legacy reasons and should not be used frequently).\n    const segments = [];\n\n    for (const path of pathComponents) {\n      if (path.indexOf('//') >= 0) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid segment (${path}). Paths must not contain // in them.`);\n      } // Strip leading and traling slashed.\n\n\n      segments.push(...path.split('/').filter(segment => segment.length > 0));\n    }\n\n    return new ResourcePath(segments);\n  }\n\n  static emptyPath() {\n    return new ResourcePath([]);\n  }\n\n}\n\nconst identifierRegExp = /^[_a-zA-Z][_a-zA-Z0-9]*$/;\n/**\r\n * A dot-separated path for navigating sub-objects within a document.\r\n * @internal\r\n */\n\nclass FieldPath$1 extends BasePath {\n  construct(segments, offset, length) {\n    return new FieldPath$1(segments, offset, length);\n  }\n  /**\r\n   * Returns true if the string could be used as a segment in a field path\r\n   * without escaping.\r\n   */\n\n\n  static isValidIdentifier(segment) {\n    return identifierRegExp.test(segment);\n  }\n\n  canonicalString() {\n    return this.toArray().map(str => {\n      str = str.replace(/\\\\/g, '\\\\\\\\').replace(/`/g, '\\\\`');\n\n      if (!FieldPath$1.isValidIdentifier(str)) {\n        str = '`' + str + '`';\n      }\n\n      return str;\n    }).join('.');\n  }\n\n  toString() {\n    return this.canonicalString();\n  }\n  /**\r\n   * Returns true if this field references the key of a document.\r\n   */\n\n\n  isKeyField() {\n    return this.length === 1 && this.get(0) === DOCUMENT_KEY_NAME;\n  }\n  /**\r\n   * The field designating the key of a document.\r\n   */\n\n\n  static keyField() {\n    return new FieldPath$1([DOCUMENT_KEY_NAME]);\n  }\n  /**\r\n   * Parses a field string from the given server-formatted string.\r\n   *\r\n   * - Splitting the empty string is not allowed (for now at least).\r\n   * - Empty segments within the string (e.g. if there are two consecutive\r\n   *   separators) are not allowed.\r\n   *\r\n   * TODO(b/37244157): we should make this more strict. Right now, it allows\r\n   * non-identifier path components, even if they aren't escaped.\r\n   */\n\n\n  static fromServerFormat(path) {\n    const segments = [];\n    let current = '';\n    let i = 0;\n\n    const addCurrentSegment = () => {\n      if (current.length === 0) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid field path (${path}). Paths must not be empty, begin ` + `with '.', end with '.', or contain '..'`);\n      }\n\n      segments.push(current);\n      current = '';\n    };\n\n    let inBackticks = false;\n\n    while (i < path.length) {\n      const c = path[i];\n\n      if (c === '\\\\') {\n        if (i + 1 === path.length) {\n          throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has trailing escape character: ' + path);\n        }\n\n        const next = path[i + 1];\n\n        if (!(next === '\\\\' || next === '.' || next === '`')) {\n          throw new FirestoreError(Code.INVALID_ARGUMENT, 'Path has invalid escape sequence: ' + path);\n        }\n\n        current += next;\n        i += 2;\n      } else if (c === '`') {\n        inBackticks = !inBackticks;\n        i++;\n      } else if (c === '.' && !inBackticks) {\n        addCurrentSegment();\n        i++;\n      } else {\n        current += c;\n        i++;\n      }\n    }\n\n    addCurrentSegment();\n\n    if (inBackticks) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Unterminated ` in path: ' + path);\n    }\n\n    return new FieldPath$1(segments);\n  }\n\n  static emptyPath() {\n    return new FieldPath$1([]);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * @internal\r\n */\n\n\nclass DocumentKey {\n  constructor(path) {\n    this.path = path;\n  }\n\n  static fromPath(path) {\n    return new DocumentKey(ResourcePath.fromString(path));\n  }\n\n  static fromName(name) {\n    return new DocumentKey(ResourcePath.fromString(name).popFirst(5));\n  }\n\n  static empty() {\n    return new DocumentKey(ResourcePath.emptyPath());\n  }\n\n  get collectionGroup() {\n    return this.path.popLast().lastSegment();\n  }\n  /** Returns true if the document is in the specified collectionId. */\n\n\n  hasCollectionId(collectionId) {\n    return this.path.length >= 2 && this.path.get(this.path.length - 2) === collectionId;\n  }\n  /** Returns the collection group (i.e. the name of the parent collection) for this key. */\n\n\n  getCollectionGroup() {\n    return this.path.get(this.path.length - 2);\n  }\n  /** Returns the fully qualified path to the parent collection. */\n\n\n  getCollectionPath() {\n    return this.path.popLast();\n  }\n\n  isEqual(other) {\n    return other !== null && ResourcePath.comparator(this.path, other.path) === 0;\n  }\n\n  toString() {\n    return this.path.toString();\n  }\n\n  static comparator(k1, k2) {\n    return ResourcePath.comparator(k1.path, k2.path);\n  }\n\n  static isDocumentKey(path) {\n    return path.length % 2 === 0;\n  }\n  /**\r\n   * Creates and returns a new document key with the given segments.\r\n   *\r\n   * @param segments - The segments of the path to the document\r\n   * @returns A new instance of DocumentKey\r\n   */\n\n\n  static fromSegments(segments) {\n    return new DocumentKey(new ResourcePath(segments.slice()));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * The initial mutation batch id for each index. Gets updated during index\r\n * backfill.\r\n */\n\n\nconst INITIAL_LARGEST_BATCH_ID = -1;\n/**\r\n * The initial sequence number for each index. Gets updated during index\r\n * backfill.\r\n */\n\nconst INITIAL_SEQUENCE_NUMBER = 0;\n/**\r\n * An index definition for field indexes in Firestore.\r\n *\r\n * Every index is associated with a collection. The definition contains a list\r\n * of fields and their index kind (which can be `ASCENDING`, `DESCENDING` or\r\n * `CONTAINS` for ArrayContains/ArrayContainsAny queries).\r\n *\r\n * Unlike the backend, the SDK does not differentiate between collection or\r\n * collection group-scoped indices. Every index can be used for both single\r\n * collection and collection group queries.\r\n */\n\nclass FieldIndex {\n  constructor(\n  /**\r\n   * The index ID. Returns -1 if the index ID is not available (e.g. the index\r\n   * has not yet been persisted).\r\n   */\n  indexId,\n  /** The collection ID this index applies to. */\n  collectionGroup,\n  /** The field segments for this index. */\n  fields,\n  /** Shows how up-to-date the index is for the current user. */\n  indexState) {\n    this.indexId = indexId;\n    this.collectionGroup = collectionGroup;\n    this.fields = fields;\n    this.indexState = indexState;\n  }\n\n}\n/** An ID for an index that has not yet been added to persistence.  */\n\n\nFieldIndex.UNKNOWN_ID = -1;\n/** Returns the ArrayContains/ArrayContainsAny segment for this index. */\n\nfunction fieldIndexGetArraySegment(fieldIndex) {\n  return fieldIndex.fields.find(s => s.kind === 2\n  /* IndexKind.CONTAINS */\n  );\n}\n/** Returns all directional (ascending/descending) segments for this index. */\n\n\nfunction fieldIndexGetDirectionalSegments(fieldIndex) {\n  return fieldIndex.fields.filter(s => s.kind !== 2\n  /* IndexKind.CONTAINS */\n  );\n}\n/**\r\n * Returns the order of the document key component for the given index.\r\n *\r\n * PORTING NOTE: This is only used in the Web IndexedDb implementation.\r\n */\n\n\nfunction fieldIndexGetKeyOrder(fieldIndex) {\n  const directionalSegments = fieldIndexGetDirectionalSegments(fieldIndex);\n  return directionalSegments.length === 0 ? 0\n  /* IndexKind.ASCENDING */\n  : directionalSegments[directionalSegments.length - 1].kind;\n}\n/**\r\n * Compares indexes by collection group and segments. Ignores update time and\r\n * index ID.\r\n */\n\n\nfunction fieldIndexSemanticComparator(left, right) {\n  let cmp = primitiveComparator(left.collectionGroup, right.collectionGroup);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  for (let i = 0; i < Math.min(left.fields.length, right.fields.length); ++i) {\n    cmp = indexSegmentComparator(left.fields[i], right.fields[i]);\n\n    if (cmp !== 0) {\n      return cmp;\n    }\n  }\n\n  return primitiveComparator(left.fields.length, right.fields.length);\n}\n/** Returns a debug representation of the field index */\n\n\nfunction fieldIndexToString(fieldIndex) {\n  return `id=${fieldIndex.indexId}|cg=${fieldIndex.collectionGroup}|f=${fieldIndex.fields.map(f => `${f.fieldPath}:${f.kind}`).join(',')}`;\n}\n/** An index component consisting of field path and index type.  */\n\n\nclass IndexSegment {\n  constructor(\n  /** The field path of the component. */\n  fieldPath,\n  /** The fields sorting order. */\n  kind) {\n    this.fieldPath = fieldPath;\n    this.kind = kind;\n  }\n\n}\n\nfunction indexSegmentComparator(left, right) {\n  const cmp = FieldPath$1.comparator(left.fieldPath, right.fieldPath);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  return primitiveComparator(left.kind, right.kind);\n}\n/**\r\n * Stores the \"high water mark\" that indicates how updated the Index is for the\r\n * current user.\r\n */\n\n\nclass IndexState {\n  constructor(\n  /**\r\n   * Indicates when the index was last updated (relative to other indexes).\r\n   */\n  sequenceNumber,\n  /** The the latest indexed read time, document and batch id. */\n  offset) {\n    this.sequenceNumber = sequenceNumber;\n    this.offset = offset;\n  }\n  /** The state of an index that has not yet been backfilled. */\n\n\n  static empty() {\n    return new IndexState(INITIAL_SEQUENCE_NUMBER, IndexOffset.min());\n  }\n\n}\n/**\r\n * Creates an offset that matches all documents with a read time higher than\r\n * `readTime`.\r\n */\n\n\nfunction newIndexOffsetSuccessorFromReadTime(readTime, largestBatchId) {\n  // We want to create an offset that matches all documents with a read time\n  // greater than the provided read time. To do so, we technically need to\n  // create an offset for `(readTime, MAX_DOCUMENT_KEY)`. While we could use\n  // Unicode codepoints to generate MAX_DOCUMENT_KEY, it is much easier to use\n  // `(readTime + 1, DocumentKey.empty())` since `> DocumentKey.empty()` matches\n  // all valid document IDs.\n  const successorSeconds = readTime.toTimestamp().seconds;\n  const successorNanos = readTime.toTimestamp().nanoseconds + 1;\n  const successor = SnapshotVersion.fromTimestamp(successorNanos === 1e9 ? new Timestamp(successorSeconds + 1, 0) : new Timestamp(successorSeconds, successorNanos));\n  return new IndexOffset(successor, DocumentKey.empty(), largestBatchId);\n}\n/** Creates a new offset based on the provided document. */\n\n\nfunction newIndexOffsetFromDocument(document) {\n  return new IndexOffset(document.readTime, document.key, INITIAL_LARGEST_BATCH_ID);\n}\n/**\r\n * Stores the latest read time, document and batch ID that were processed for an\r\n * index.\r\n */\n\n\nclass IndexOffset {\n  constructor(\n  /**\r\n   * The latest read time version that has been indexed by Firestore for this\r\n   * field index.\r\n   */\n  readTime,\n  /**\r\n   * The key of the last document that was indexed for this query. Use\r\n   * `DocumentKey.empty()` if no document has been indexed.\r\n   */\n  documentKey,\n  /*\r\n   * The largest mutation batch id that's been processed by Firestore.\r\n   */\n  largestBatchId) {\n    this.readTime = readTime;\n    this.documentKey = documentKey;\n    this.largestBatchId = largestBatchId;\n  }\n  /** Returns an offset that sorts before all regular offsets. */\n\n\n  static min() {\n    return new IndexOffset(SnapshotVersion.min(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\n  }\n  /** Returns an offset that sorts after all regular offsets. */\n\n\n  static max() {\n    return new IndexOffset(SnapshotVersion.max(), DocumentKey.empty(), INITIAL_LARGEST_BATCH_ID);\n  }\n\n}\n\nfunction indexOffsetComparator(left, right) {\n  let cmp = left.readTime.compareTo(right.readTime);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  cmp = DocumentKey.comparator(left.documentKey, right.documentKey);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  return primitiveComparator(left.largestBatchId, right.largestBatchId);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst PRIMARY_LEASE_LOST_ERROR_MSG = 'The current tab is not in the required state to perform this operation. ' + 'It might be necessary to refresh the browser tab.';\n/**\r\n * A base class representing a persistence transaction, encapsulating both the\r\n * transaction's sequence numbers as well as a list of onCommitted listeners.\r\n *\r\n * When you call Persistence.runTransaction(), it will create a transaction and\r\n * pass it to your callback. You then pass it to any method that operates\r\n * on persistence.\r\n */\n\nclass PersistenceTransaction {\n  constructor() {\n    this.onCommittedListeners = [];\n  }\n\n  addOnCommittedListener(listener) {\n    this.onCommittedListeners.push(listener);\n  }\n\n  raiseOnCommittedEvent() {\n    this.onCommittedListeners.forEach(listener => listener());\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Verifies the error thrown by a LocalStore operation. If a LocalStore\r\n * operation fails because the primary lease has been taken by another client,\r\n * we ignore the error (the persistence layer will immediately call\r\n * `applyPrimaryLease` to propagate the primary state change). All other errors\r\n * are re-thrown.\r\n *\r\n * @param err - An error returned by a LocalStore operation.\r\n * @returns A Promise that resolves after we recovered, or the original error.\r\n */\n\n\nfunction ignoreIfPrimaryLeaseLoss(_x) {\n  return _ignoreIfPrimaryLeaseLoss.apply(this, arguments);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * PersistencePromise is essentially a re-implementation of Promise except\r\n * it has a .next() method instead of .then() and .next() and .catch() callbacks\r\n * are executed synchronously when a PersistencePromise resolves rather than\r\n * asynchronously (Promise implementations use setImmediate() or similar).\r\n *\r\n * This is necessary to interoperate with IndexedDB which will automatically\r\n * commit transactions if control is returned to the event loop without\r\n * synchronously initiating another operation on the transaction.\r\n *\r\n * NOTE: .then() and .catch() only allow a single consumer, unlike normal\r\n * Promises.\r\n */\n\n\nfunction _ignoreIfPrimaryLeaseLoss() {\n  _ignoreIfPrimaryLeaseLoss = _asyncToGenerator(function* (err) {\n    if (err.code === Code.FAILED_PRECONDITION && err.message === PRIMARY_LEASE_LOST_ERROR_MSG) {\n      logDebug('LocalStore', 'Unexpectedly lost primary lease');\n    } else {\n      throw err;\n    }\n  });\n  return _ignoreIfPrimaryLeaseLoss.apply(this, arguments);\n}\n\nclass PersistencePromise {\n  constructor(callback) {\n    // NOTE: next/catchCallback will always point to our own wrapper functions,\n    // not the user's raw next() or catch() callbacks.\n    this.nextCallback = null;\n    this.catchCallback = null; // When the operation resolves, we'll set result or error and mark isDone.\n\n    this.result = undefined;\n    this.error = undefined;\n    this.isDone = false; // Set to true when .then() or .catch() are called and prevents additional\n    // chaining.\n\n    this.callbackAttached = false;\n    callback(value => {\n      this.isDone = true;\n      this.result = value;\n\n      if (this.nextCallback) {\n        // value should be defined unless T is Void, but we can't express\n        // that in the type system.\n        this.nextCallback(value);\n      }\n    }, error => {\n      this.isDone = true;\n      this.error = error;\n\n      if (this.catchCallback) {\n        this.catchCallback(error);\n      }\n    });\n  }\n\n  catch(fn) {\n    return this.next(undefined, fn);\n  }\n\n  next(nextFn, catchFn) {\n    if (this.callbackAttached) {\n      fail();\n    }\n\n    this.callbackAttached = true;\n\n    if (this.isDone) {\n      if (!this.error) {\n        return this.wrapSuccess(nextFn, this.result);\n      } else {\n        return this.wrapFailure(catchFn, this.error);\n      }\n    } else {\n      return new PersistencePromise((resolve, reject) => {\n        this.nextCallback = value => {\n          this.wrapSuccess(nextFn, value).next(resolve, reject);\n        };\n\n        this.catchCallback = error => {\n          this.wrapFailure(catchFn, error).next(resolve, reject);\n        };\n      });\n    }\n  }\n\n  toPromise() {\n    return new Promise((resolve, reject) => {\n      this.next(resolve, reject);\n    });\n  }\n\n  wrapUserFunction(fn) {\n    try {\n      const result = fn();\n\n      if (result instanceof PersistencePromise) {\n        return result;\n      } else {\n        return PersistencePromise.resolve(result);\n      }\n    } catch (e) {\n      return PersistencePromise.reject(e);\n    }\n  }\n\n  wrapSuccess(nextFn, value) {\n    if (nextFn) {\n      return this.wrapUserFunction(() => nextFn(value));\n    } else {\n      // If there's no nextFn, then R must be the same as T\n      return PersistencePromise.resolve(value);\n    }\n  }\n\n  wrapFailure(catchFn, error) {\n    if (catchFn) {\n      return this.wrapUserFunction(() => catchFn(error));\n    } else {\n      return PersistencePromise.reject(error);\n    }\n  }\n\n  static resolve(result) {\n    return new PersistencePromise((resolve, reject) => {\n      resolve(result);\n    });\n  }\n\n  static reject(error) {\n    return new PersistencePromise((resolve, reject) => {\n      reject(error);\n    });\n  }\n\n  static waitFor( // Accept all Promise types in waitFor().\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n  all) {\n    return new PersistencePromise((resolve, reject) => {\n      let expectedCount = 0;\n      let resolvedCount = 0;\n      let done = false;\n      all.forEach(element => {\n        ++expectedCount;\n        element.next(() => {\n          ++resolvedCount;\n\n          if (done && resolvedCount === expectedCount) {\n            resolve();\n          }\n        }, err => reject(err));\n      });\n      done = true;\n\n      if (resolvedCount === expectedCount) {\n        resolve();\n      }\n    });\n  }\n  /**\r\n   * Given an array of predicate functions that asynchronously evaluate to a\r\n   * boolean, implements a short-circuiting `or` between the results. Predicates\r\n   * will be evaluated until one of them returns `true`, then stop. The final\r\n   * result will be whether any of them returned `true`.\r\n   */\n\n\n  static or(predicates) {\n    let p = PersistencePromise.resolve(false);\n\n    for (const predicate of predicates) {\n      p = p.next(isTrue => {\n        if (isTrue) {\n          return PersistencePromise.resolve(isTrue);\n        } else {\n          return predicate();\n        }\n      });\n    }\n\n    return p;\n  }\n\n  static forEach(collection, f) {\n    const promises = [];\n    collection.forEach((r, s) => {\n      promises.push(f.call(this, r, s));\n    });\n    return this.waitFor(promises);\n  }\n  /**\r\n   * Concurrently map all array elements through asynchronous function.\r\n   */\n\n\n  static mapArray(array, f) {\n    return new PersistencePromise((resolve, reject) => {\n      const expectedCount = array.length;\n      const results = new Array(expectedCount);\n      let resolvedCount = 0;\n\n      for (let i = 0; i < expectedCount; i++) {\n        const current = i;\n        f(array[current]).next(result => {\n          results[current] = result;\n          ++resolvedCount;\n\n          if (resolvedCount === expectedCount) {\n            resolve(results);\n          }\n        }, err => reject(err));\n      }\n    });\n  }\n  /**\r\n   * An alternative to recursive PersistencePromise calls, that avoids\r\n   * potential memory problems from unbounded chains of promises.\r\n   *\r\n   * The `action` will be called repeatedly while `condition` is true.\r\n   */\n\n\n  static doWhile(condition, action) {\n    return new PersistencePromise((resolve, reject) => {\n      const process = () => {\n        if (condition() === true) {\n          action().next(() => {\n            process();\n          }, reject);\n        } else {\n          resolve();\n        }\n      };\n\n      process();\n    });\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// References to `window` are guarded by SimpleDb.isAvailable()\n\n/* eslint-disable no-restricted-globals */\n\n\nconst LOG_TAG$i = 'SimpleDb';\n/**\r\n * The maximum number of retry attempts for an IndexedDb transaction that fails\r\n * with a DOMException.\r\n */\n\nconst TRANSACTION_RETRY_COUNT = 3;\n/**\r\n * Wraps an IDBTransaction and exposes a store() method to get a handle to a\r\n * specific object store.\r\n */\n\nclass SimpleDbTransaction {\n  constructor(action, transaction) {\n    this.action = action;\n    this.transaction = transaction;\n    this.aborted = false;\n    /**\r\n     * A `Promise` that resolves with the result of the IndexedDb transaction.\r\n     */\n\n    this.completionDeferred = new Deferred();\n\n    this.transaction.oncomplete = () => {\n      this.completionDeferred.resolve();\n    };\n\n    this.transaction.onabort = () => {\n      if (transaction.error) {\n        this.completionDeferred.reject(new IndexedDbTransactionError(action, transaction.error));\n      } else {\n        this.completionDeferred.resolve();\n      }\n    };\n\n    this.transaction.onerror = event => {\n      const error = checkForAndReportiOSError(event.target.error);\n      this.completionDeferred.reject(new IndexedDbTransactionError(action, error));\n    };\n  }\n\n  static open(db, action, mode, objectStoreNames) {\n    try {\n      return new SimpleDbTransaction(action, db.transaction(objectStoreNames, mode));\n    } catch (e) {\n      throw new IndexedDbTransactionError(action, e);\n    }\n  }\n\n  get completionPromise() {\n    return this.completionDeferred.promise;\n  }\n\n  abort(error) {\n    if (error) {\n      this.completionDeferred.reject(error);\n    }\n\n    if (!this.aborted) {\n      logDebug(LOG_TAG$i, 'Aborting transaction:', error ? error.message : 'Client-initiated abort');\n      this.aborted = true;\n      this.transaction.abort();\n    }\n  }\n\n  maybeCommit() {\n    // If the browser supports V3 IndexedDB, we invoke commit() explicitly to\n    // speed up index DB processing if the event loop remains blocks.\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const maybeV3IndexedDb = this.transaction;\n\n    if (!this.aborted && typeof maybeV3IndexedDb.commit === 'function') {\n      maybeV3IndexedDb.commit();\n    }\n  }\n  /**\r\n   * Returns a SimpleDbStore<KeyType, ValueType> for the specified store. All\r\n   * operations performed on the SimpleDbStore happen within the context of this\r\n   * transaction and it cannot be used anymore once the transaction is\r\n   * completed.\r\n   *\r\n   * Note that we can't actually enforce that the KeyType and ValueType are\r\n   * correct, but they allow type safety through the rest of the consuming code.\r\n   */\n\n\n  store(storeName) {\n    const store = this.transaction.objectStore(storeName);\n    return new SimpleDbStore(store);\n  }\n\n}\n/**\r\n * Provides a wrapper around IndexedDb with a simplified interface that uses\r\n * Promise-like return values to chain operations. Real promises cannot be used\r\n * since .then() continuations are executed asynchronously (e.g. via\r\n * .setImmediate), which would cause IndexedDB to end the transaction.\r\n * See PersistencePromise for more details.\r\n */\n\n\nclass SimpleDb {\n  /*\r\n   * Creates a new SimpleDb wrapper for IndexedDb database `name`.\r\n   *\r\n   * Note that `version` must not be a downgrade. IndexedDB does not support\r\n   * downgrading the schema version. We currently do not support any way to do\r\n   * versioning outside of IndexedDB's versioning mechanism, as only\r\n   * version-upgrade transactions are allowed to do things like create\r\n   * objectstores.\r\n   */\n  constructor(name, version, schemaConverter) {\n    this.name = name;\n    this.version = version;\n    this.schemaConverter = schemaConverter;\n    const iOSVersion = SimpleDb.getIOSVersion(getUA()); // NOTE: According to https://bugs.webkit.org/show_bug.cgi?id=197050, the\n    // bug we're checking for should exist in iOS >= 12.2 and < 13, but for\n    // whatever reason it's much harder to hit after 12.2 so we only proactively\n    // log on 12.2.\n\n    if (iOSVersion === 12.2) {\n      logError('Firestore persistence suffers from a bug in iOS 12.2 ' + 'Safari that may cause your app to stop working. See ' + 'https://stackoverflow.com/q/56496296/110915 for details ' + 'and a potential workaround.');\n    }\n  }\n  /** Deletes the specified database. */\n\n\n  static delete(name) {\n    logDebug(LOG_TAG$i, 'Removing database:', name);\n    return wrapRequest(window.indexedDB.deleteDatabase(name)).toPromise();\n  }\n  /** Returns true if IndexedDB is available in the current environment. */\n\n\n  static isAvailable() {\n    if (!isIndexedDBAvailable()) {\n      return false;\n    }\n\n    if (SimpleDb.isMockPersistence()) {\n      return true;\n    } // We extensively use indexed array values and compound keys,\n    // which IE and Edge do not support. However, they still have indexedDB\n    // defined on the window, so we need to check for them here and make sure\n    // to return that persistence is not enabled for those browsers.\n    // For tracking support of this feature, see here:\n    // https://developer.microsoft.com/en-us/microsoft-edge/platform/status/indexeddbarraysandmultientrysupport/\n    // Check the UA string to find out the browser.\n\n\n    const ua = getUA(); // IE 10\n    // ua = 'Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; Trident/6.0)';\n    // IE 11\n    // ua = 'Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko';\n    // Edge\n    // ua = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML,\n    // like Gecko) Chrome/39.0.2171.71 Safari/537.36 Edge/12.0';\n    // iOS Safari: Disable for users running iOS version < 10.\n\n    const iOSVersion = SimpleDb.getIOSVersion(ua);\n    const isUnsupportedIOS = 0 < iOSVersion && iOSVersion < 10; // Android browser: Disable for userse running version < 4.5.\n\n    const androidVersion = SimpleDb.getAndroidVersion(ua);\n    const isUnsupportedAndroid = 0 < androidVersion && androidVersion < 4.5;\n\n    if (ua.indexOf('MSIE ') > 0 || ua.indexOf('Trident/') > 0 || ua.indexOf('Edge/') > 0 || isUnsupportedIOS || isUnsupportedAndroid) {\n      return false;\n    } else {\n      return true;\n    }\n  }\n  /**\r\n   * Returns true if the backing IndexedDB store is the Node IndexedDBShim\r\n   * (see https://github.com/axemclion/IndexedDBShim).\r\n   */\n\n\n  static isMockPersistence() {\n    var _a;\n\n    return typeof process !== 'undefined' && ((_a = process.env) === null || _a === void 0 ? void 0 : _a.USE_MOCK_PERSISTENCE) === 'YES';\n  }\n  /** Helper to get a typed SimpleDbStore from a transaction. */\n\n\n  static getStore(txn, store) {\n    return txn.store(store);\n  } // visible for testing\n\n  /** Parse User Agent to determine iOS version. Returns -1 if not found. */\n\n\n  static getIOSVersion(ua) {\n    const iOSVersionRegex = ua.match(/i(?:phone|pad|pod) os ([\\d_]+)/i);\n    const version = iOSVersionRegex ? iOSVersionRegex[1].split('_').slice(0, 2).join('.') : '-1';\n    return Number(version);\n  } // visible for testing\n\n  /** Parse User Agent to determine Android version. Returns -1 if not found. */\n\n\n  static getAndroidVersion(ua) {\n    const androidVersionRegex = ua.match(/Android ([\\d.]+)/i);\n    const version = androidVersionRegex ? androidVersionRegex[1].split('.').slice(0, 2).join('.') : '-1';\n    return Number(version);\n  }\n  /**\r\n   * Opens the specified database, creating or upgrading it if necessary.\r\n   */\n\n\n  ensureDb(action) {\n    var _this2 = this;\n\n    return _asyncToGenerator(function* () {\n      if (!_this2.db) {\n        logDebug(LOG_TAG$i, 'Opening database:', _this2.name);\n        _this2.db = yield new Promise((resolve, reject) => {\n          // TODO(mikelehen): Investigate browser compatibility.\n          // https://developer.mozilla.org/en-US/docs/Web/API/IndexedDB_API/Using_IndexedDB\n          // suggests IE9 and older WebKit browsers handle upgrade\n          // differently. They expect setVersion, as described here:\n          // https://developer.mozilla.org/en-US/docs/Web/API/IDBVersionChangeRequest/setVersion\n          const request = indexedDB.open(_this2.name, _this2.version);\n\n          request.onsuccess = event => {\n            const db = event.target.result;\n            resolve(db);\n          };\n\n          request.onblocked = () => {\n            reject(new IndexedDbTransactionError(action, 'Cannot upgrade IndexedDB schema while another tab is open. ' + 'Close all tabs that access Firestore and reload this page to proceed.'));\n          };\n\n          request.onerror = event => {\n            const error = event.target.error;\n\n            if (error.name === 'VersionError') {\n              reject(new FirestoreError(Code.FAILED_PRECONDITION, 'A newer version of the Firestore SDK was previously used and so the persisted ' + 'data is not compatible with the version of the SDK you are now using. The SDK ' + 'will operate with persistence disabled. If you need persistence, please ' + 're-upgrade to a newer version of the SDK or else clear the persisted IndexedDB ' + 'data for your app to start fresh.'));\n            } else if (error.name === 'InvalidStateError') {\n              reject(new FirestoreError(Code.FAILED_PRECONDITION, 'Unable to open an IndexedDB connection. This could be due to running in a ' + 'private browsing session on a browser whose private browsing sessions do not ' + 'support IndexedDB: ' + error));\n            } else {\n              reject(new IndexedDbTransactionError(action, error));\n            }\n          };\n\n          request.onupgradeneeded = event => {\n            logDebug(LOG_TAG$i, 'Database \"' + _this2.name + '\" requires upgrade from version:', event.oldVersion);\n            const db = event.target.result;\n\n            _this2.schemaConverter.createOrUpgrade(db, request.transaction, event.oldVersion, _this2.version).next(() => {\n              logDebug(LOG_TAG$i, 'Database upgrade to version ' + _this2.version + ' complete');\n            });\n          };\n        });\n      }\n\n      if (_this2.versionchangelistener) {\n        _this2.db.onversionchange = event => _this2.versionchangelistener(event);\n      }\n\n      return _this2.db;\n    })();\n  }\n\n  setVersionChangeListener(versionChangeListener) {\n    this.versionchangelistener = versionChangeListener;\n\n    if (this.db) {\n      this.db.onversionchange = event => {\n        return versionChangeListener(event);\n      };\n    }\n  }\n\n  runTransaction(action, mode, objectStores, transactionFn) {\n    var _this3 = this;\n\n    return _asyncToGenerator(function* () {\n      const readonly = mode === 'readonly';\n      let attemptNumber = 0;\n\n      while (true) {\n        ++attemptNumber;\n\n        try {\n          _this3.db = yield _this3.ensureDb(action);\n          const transaction = SimpleDbTransaction.open(_this3.db, action, readonly ? 'readonly' : 'readwrite', objectStores);\n          const transactionFnResult = transactionFn(transaction).next(result => {\n            transaction.maybeCommit();\n            return result;\n          }).catch(error => {\n            // Abort the transaction if there was an error.\n            transaction.abort(error); // We cannot actually recover, and calling `abort()` will cause the transaction's\n            // completion promise to be rejected. This in turn means that we won't use\n            // `transactionFnResult` below. We return a rejection here so that we don't add the\n            // possibility of returning `void` to the type of `transactionFnResult`.\n\n            return PersistencePromise.reject(error);\n          }).toPromise(); // As noted above, errors are propagated by aborting the transaction. So\n          // we swallow any error here to avoid the browser logging it as unhandled.\n\n          transactionFnResult.catch(() => {}); // Wait for the transaction to complete (i.e. IndexedDb's onsuccess event to\n          // fire), but still return the original transactionFnResult back to the\n          // caller.\n\n          yield transaction.completionPromise;\n          return transactionFnResult;\n        } catch (e) {\n          const error = e; // TODO(schmidt-sebastian): We could probably be smarter about this and\n          // not retry exceptions that are likely unrecoverable (such as quota\n          // exceeded errors).\n          // Note: We cannot use an instanceof check for FirestoreException, since the\n          // exception is wrapped in a generic error by our async/await handling.\n\n          const retryable = error.name !== 'FirebaseError' && attemptNumber < TRANSACTION_RETRY_COUNT;\n          logDebug(LOG_TAG$i, 'Transaction failed with error:', error.message, 'Retrying:', retryable);\n\n          _this3.close();\n\n          if (!retryable) {\n            return Promise.reject(error);\n          }\n        }\n      }\n    })();\n  }\n\n  close() {\n    if (this.db) {\n      this.db.close();\n    }\n\n    this.db = undefined;\n  }\n\n}\n/**\r\n * A controller for iterating over a key range or index. It allows an iterate\r\n * callback to delete the currently-referenced object, or jump to a new key\r\n * within the key range or index.\r\n */\n\n\nclass IterationController {\n  constructor(dbCursor) {\n    this.dbCursor = dbCursor;\n    this.shouldStop = false;\n    this.nextKey = null;\n  }\n\n  get isDone() {\n    return this.shouldStop;\n  }\n\n  get skipToKey() {\n    return this.nextKey;\n  }\n\n  set cursor(value) {\n    this.dbCursor = value;\n  }\n  /**\r\n   * This function can be called to stop iteration at any point.\r\n   */\n\n\n  done() {\n    this.shouldStop = true;\n  }\n  /**\r\n   * This function can be called to skip to that next key, which could be\r\n   * an index or a primary key.\r\n   */\n\n\n  skip(key) {\n    this.nextKey = key;\n  }\n  /**\r\n   * Delete the current cursor value from the object store.\r\n   *\r\n   * NOTE: You CANNOT do this with a keysOnly query.\r\n   */\n\n\n  delete() {\n    return wrapRequest(this.dbCursor.delete());\n  }\n\n}\n/** An error that wraps exceptions that thrown during IndexedDB execution. */\n\n\nclass IndexedDbTransactionError extends FirestoreError {\n  constructor(actionName, cause) {\n    super(Code.UNAVAILABLE, `IndexedDB transaction '${actionName}' failed: ${cause}`);\n    this.name = 'IndexedDbTransactionError';\n  }\n\n}\n/** Verifies whether `e` is an IndexedDbTransactionError. */\n\n\nfunction isIndexedDbTransactionError(e) {\n  // Use name equality, as instanceof checks on errors don't work with errors\n  // that wrap other errors.\n  return e.name === 'IndexedDbTransactionError';\n}\n/**\r\n * A wrapper around an IDBObjectStore providing an API that:\r\n *\r\n * 1) Has generic KeyType / ValueType parameters to provide strongly-typed\r\n * methods for acting against the object store.\r\n * 2) Deals with IndexedDB's onsuccess / onerror event callbacks, making every\r\n * method return a PersistencePromise instead.\r\n * 3) Provides a higher-level API to avoid needing to do excessive wrapping of\r\n * intermediate IndexedDB types (IDBCursorWithValue, etc.)\r\n */\n\n\nclass SimpleDbStore {\n  constructor(store) {\n    this.store = store;\n  }\n\n  put(keyOrValue, value) {\n    let request;\n\n    if (value !== undefined) {\n      logDebug(LOG_TAG$i, 'PUT', this.store.name, keyOrValue, value);\n      request = this.store.put(value, keyOrValue);\n    } else {\n      logDebug(LOG_TAG$i, 'PUT', this.store.name, '<auto-key>', keyOrValue);\n      request = this.store.put(keyOrValue);\n    }\n\n    return wrapRequest(request);\n  }\n  /**\r\n   * Adds a new value into an Object Store and returns the new key. Similar to\r\n   * IndexedDb's `add()`, this method will fail on primary key collisions.\r\n   *\r\n   * @param value - The object to write.\r\n   * @returns The key of the value to add.\r\n   */\n\n\n  add(value) {\n    logDebug(LOG_TAG$i, 'ADD', this.store.name, value, value);\n    const request = this.store.add(value);\n    return wrapRequest(request);\n  }\n  /**\r\n   * Gets the object with the specified key from the specified store, or null\r\n   * if no object exists with the specified key.\r\n   *\r\n   * @key The key of the object to get.\r\n   * @returns The object with the specified key or null if no object exists.\r\n   */\n\n\n  get(key) {\n    const request = this.store.get(key); // We're doing an unsafe cast to ValueType.\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n    return wrapRequest(request).next(result => {\n      // Normalize nonexistence to null.\n      if (result === undefined) {\n        result = null;\n      }\n\n      logDebug(LOG_TAG$i, 'GET', this.store.name, key, result);\n      return result;\n    });\n  }\n\n  delete(key) {\n    logDebug(LOG_TAG$i, 'DELETE', this.store.name, key);\n    const request = this.store.delete(key);\n    return wrapRequest(request);\n  }\n  /**\r\n   * If we ever need more of the count variants, we can add overloads. For now,\r\n   * all we need is to count everything in a store.\r\n   *\r\n   * Returns the number of rows in the store.\r\n   */\n\n\n  count() {\n    logDebug(LOG_TAG$i, 'COUNT', this.store.name);\n    const request = this.store.count();\n    return wrapRequest(request);\n  }\n\n  loadAll(indexOrRange, range) {\n    const iterateOptions = this.options(indexOrRange, range); // Use `getAll()` if the browser supports IndexedDB v3, as it is roughly\n    // 20% faster. Unfortunately, getAll() does not support custom indices.\n\n    if (!iterateOptions.index && typeof this.store.getAll === 'function') {\n      const request = this.store.getAll(iterateOptions.range);\n      return new PersistencePromise((resolve, reject) => {\n        request.onerror = event => {\n          reject(event.target.error);\n        };\n\n        request.onsuccess = event => {\n          resolve(event.target.result);\n        };\n      });\n    } else {\n      const cursor = this.cursor(iterateOptions);\n      const results = [];\n      return this.iterateCursor(cursor, (key, value) => {\n        results.push(value);\n      }).next(() => {\n        return results;\n      });\n    }\n  }\n  /**\r\n   * Loads the first `count` elements from the provided index range. Loads all\r\n   * elements if no limit is provided.\r\n   */\n\n\n  loadFirst(range, count) {\n    const request = this.store.getAll(range, count === null ? undefined : count);\n    return new PersistencePromise((resolve, reject) => {\n      request.onerror = event => {\n        reject(event.target.error);\n      };\n\n      request.onsuccess = event => {\n        resolve(event.target.result);\n      };\n    });\n  }\n\n  deleteAll(indexOrRange, range) {\n    logDebug(LOG_TAG$i, 'DELETE ALL', this.store.name);\n    const options = this.options(indexOrRange, range);\n    options.keysOnly = false;\n    const cursor = this.cursor(options);\n    return this.iterateCursor(cursor, (key, value, control) => {\n      // NOTE: Calling delete() on a cursor is documented as more efficient than\n      // calling delete() on an object store with a single key\n      // (https://developer.mozilla.org/en-US/docs/Web/API/IDBObjectStore/delete),\n      // however, this requires us *not* to use a keysOnly cursor\n      // (https://developer.mozilla.org/en-US/docs/Web/API/IDBCursor/delete). We\n      // may want to compare the performance of each method.\n      return control.delete();\n    });\n  }\n\n  iterate(optionsOrCallback, callback) {\n    let options;\n\n    if (!callback) {\n      options = {};\n      callback = optionsOrCallback;\n    } else {\n      options = optionsOrCallback;\n    }\n\n    const cursor = this.cursor(options);\n    return this.iterateCursor(cursor, callback);\n  }\n  /**\r\n   * Iterates over a store, but waits for the given callback to complete for\r\n   * each entry before iterating the next entry. This allows the callback to do\r\n   * asynchronous work to determine if this iteration should continue.\r\n   *\r\n   * The provided callback should return `true` to continue iteration, and\r\n   * `false` otherwise.\r\n   */\n\n\n  iterateSerial(callback) {\n    const cursorRequest = this.cursor({});\n    return new PersistencePromise((resolve, reject) => {\n      cursorRequest.onerror = event => {\n        const error = checkForAndReportiOSError(event.target.error);\n        reject(error);\n      };\n\n      cursorRequest.onsuccess = event => {\n        const cursor = event.target.result;\n\n        if (!cursor) {\n          resolve();\n          return;\n        }\n\n        callback(cursor.primaryKey, cursor.value).next(shouldContinue => {\n          if (shouldContinue) {\n            cursor.continue();\n          } else {\n            resolve();\n          }\n        });\n      };\n    });\n  }\n\n  iterateCursor(cursorRequest, fn) {\n    const results = [];\n    return new PersistencePromise((resolve, reject) => {\n      cursorRequest.onerror = event => {\n        reject(event.target.error);\n      };\n\n      cursorRequest.onsuccess = event => {\n        const cursor = event.target.result;\n\n        if (!cursor) {\n          resolve();\n          return;\n        }\n\n        const controller = new IterationController(cursor);\n        const userResult = fn(cursor.primaryKey, cursor.value, controller);\n\n        if (userResult instanceof PersistencePromise) {\n          const userPromise = userResult.catch(err => {\n            controller.done();\n            return PersistencePromise.reject(err);\n          });\n          results.push(userPromise);\n        }\n\n        if (controller.isDone) {\n          resolve();\n        } else if (controller.skipToKey === null) {\n          cursor.continue();\n        } else {\n          cursor.continue(controller.skipToKey);\n        }\n      };\n    }).next(() => PersistencePromise.waitFor(results));\n  }\n\n  options(indexOrRange, range) {\n    let indexName = undefined;\n\n    if (indexOrRange !== undefined) {\n      if (typeof indexOrRange === 'string') {\n        indexName = indexOrRange;\n      } else {\n        range = indexOrRange;\n      }\n    }\n\n    return {\n      index: indexName,\n      range\n    };\n  }\n\n  cursor(options) {\n    let direction = 'next';\n\n    if (options.reverse) {\n      direction = 'prev';\n    }\n\n    if (options.index) {\n      const index = this.store.index(options.index);\n\n      if (options.keysOnly) {\n        return index.openKeyCursor(options.range, direction);\n      } else {\n        return index.openCursor(options.range, direction);\n      }\n    } else {\n      return this.store.openCursor(options.range, direction);\n    }\n  }\n\n}\n/**\r\n * Wraps an IDBRequest in a PersistencePromise, using the onsuccess / onerror\r\n * handlers to resolve / reject the PersistencePromise as appropriate.\r\n */\n\n\nfunction wrapRequest(request) {\n  return new PersistencePromise((resolve, reject) => {\n    request.onsuccess = event => {\n      const result = event.target.result;\n      resolve(result);\n    };\n\n    request.onerror = event => {\n      const error = checkForAndReportiOSError(event.target.error);\n      reject(error);\n    };\n  });\n} // Guard so we only report the error once.\n\n\nlet reportedIOSError = false;\n\nfunction checkForAndReportiOSError(error) {\n  const iOSVersion = SimpleDb.getIOSVersion(getUA());\n\n  if (iOSVersion >= 12.2 && iOSVersion < 13) {\n    const IOS_ERROR = 'An internal error was encountered in the Indexed Database server';\n\n    if (error.message.indexOf(IOS_ERROR) >= 0) {\n      // Wrap error in a more descriptive one.\n      const newError = new FirestoreError('internal', `IOS_INDEXEDDB_BUG1: IndexedDb has thrown '${IOS_ERROR}'. This is likely ` + `due to an unavoidable bug in iOS. See https://stackoverflow.com/q/56496296/110915 ` + `for details and a potential workaround.`);\n\n      if (!reportedIOSError) {\n        reportedIOSError = true; // Throw a global exception outside of this promise chain, for the user to\n        // potentially catch.\n\n        setTimeout(() => {\n          throw newError;\n        }, 0);\n      }\n\n      return newError;\n    }\n  }\n\n  return error;\n}\n\nconst LOG_TAG$h = 'IndexBackiller';\n/** How long we wait to try running index backfill after SDK initialization. */\n\nconst INITIAL_BACKFILL_DELAY_MS = 15 * 1000;\n/** Minimum amount of time between backfill checks, after the first one. */\n\nconst REGULAR_BACKFILL_DELAY_MS = 60 * 1000;\n/** The maximum number of documents to process each time backfill() is called. */\n\nconst MAX_DOCUMENTS_TO_PROCESS = 50;\n/** This class is responsible for the scheduling of Index Backfiller. */\n\nclass IndexBackfillerScheduler {\n  constructor(asyncQueue, backfiller) {\n    this.asyncQueue = asyncQueue;\n    this.backfiller = backfiller;\n    this.task = null;\n  }\n\n  start() {\n    this.schedule(INITIAL_BACKFILL_DELAY_MS);\n  }\n\n  stop() {\n    if (this.task) {\n      this.task.cancel();\n      this.task = null;\n    }\n  }\n\n  get started() {\n    return this.task !== null;\n  }\n\n  schedule(delay) {\n    var _this4 = this;\n\n    logDebug(LOG_TAG$h, `Scheduled in ${delay}ms`);\n    this.task = this.asyncQueue.enqueueAfterDelay(\"index_backfill\"\n    /* TimerId.IndexBackfill */\n    , delay, /*#__PURE__*/_asyncToGenerator(function* () {\n      _this4.task = null;\n\n      try {\n        const documentsProcessed = yield _this4.backfiller.backfill();\n        logDebug(LOG_TAG$h, `Documents written: ${documentsProcessed}`);\n      } catch (e) {\n        if (isIndexedDbTransactionError(e)) {\n          logDebug(LOG_TAG$h, 'Ignoring IndexedDB error during index backfill: ', e);\n        } else {\n          yield ignoreIfPrimaryLeaseLoss(e);\n        }\n      }\n\n      yield _this4.schedule(REGULAR_BACKFILL_DELAY_MS);\n    }));\n  }\n\n}\n/** Implements the steps for backfilling indexes. */\n\n\nclass IndexBackfiller {\n  constructor(\n  /**\r\n   * LocalStore provides access to IndexManager and LocalDocumentView.\r\n   * These properties will update when the user changes. Consequently,\r\n   * making a local copy of IndexManager and LocalDocumentView will require\r\n   * updates over time. The simpler solution is to rely on LocalStore to have\r\n   * an up-to-date references to IndexManager and LocalDocumentStore.\r\n   */\n  localStore, persistence) {\n    this.localStore = localStore;\n    this.persistence = persistence;\n  }\n\n  backfill(maxDocumentsToProcess = MAX_DOCUMENTS_TO_PROCESS) {\n    var _this5 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this5.persistence.runTransaction('Backfill Indexes', 'readwrite-primary', txn => _this5.writeIndexEntries(txn, maxDocumentsToProcess));\n    })();\n  }\n  /** Writes index entries until the cap is reached. Returns the number of documents processed. */\n\n\n  writeIndexEntries(transation, maxDocumentsToProcess) {\n    const processedCollectionGroups = new Set();\n    let documentsRemaining = maxDocumentsToProcess;\n    let continueLoop = true;\n    return PersistencePromise.doWhile(() => continueLoop === true && documentsRemaining > 0, () => {\n      return this.localStore.indexManager.getNextCollectionGroupToUpdate(transation).next(collectionGroup => {\n        if (collectionGroup === null || processedCollectionGroups.has(collectionGroup)) {\n          continueLoop = false;\n        } else {\n          logDebug(LOG_TAG$h, `Processing collection: ${collectionGroup}`);\n          return this.writeEntriesForCollectionGroup(transation, collectionGroup, documentsRemaining).next(documentsProcessed => {\n            documentsRemaining -= documentsProcessed;\n            processedCollectionGroups.add(collectionGroup);\n          });\n        }\n      });\n    }).next(() => maxDocumentsToProcess - documentsRemaining);\n  }\n  /**\r\n   * Writes entries for the provided collection group. Returns the number of documents processed.\r\n   */\n\n\n  writeEntriesForCollectionGroup(transaction, collectionGroup, documentsRemainingUnderCap) {\n    // Use the earliest offset of all field indexes to query the local cache.\n    return this.localStore.indexManager.getMinOffsetFromCollectionGroup(transaction, collectionGroup).next(existingOffset => this.localStore.localDocuments.getNextDocuments(transaction, collectionGroup, existingOffset, documentsRemainingUnderCap).next(nextBatch => {\n      const docs = nextBatch.changes;\n      return this.localStore.indexManager.updateIndexEntries(transaction, docs).next(() => this.getNewOffset(existingOffset, nextBatch)).next(newOffset => {\n        logDebug(LOG_TAG$h, `Updating offset: ${newOffset}`);\n        return this.localStore.indexManager.updateCollectionGroup(transaction, collectionGroup, newOffset);\n      }).next(() => docs.size);\n    }));\n  }\n  /** Returns the next offset based on the provided documents. */\n\n\n  getNewOffset(existingOffset, lookupResult) {\n    let maxOffset = existingOffset;\n    lookupResult.changes.forEach((key, document) => {\n      const newOffset = newIndexOffsetFromDocument(document);\n\n      if (indexOffsetComparator(newOffset, maxOffset) > 0) {\n        maxOffset = newOffset;\n      }\n    });\n    return new IndexOffset(maxOffset.readTime, maxOffset.documentKey, Math.max(lookupResult.batchId, existingOffset.largestBatchId));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * `ListenSequence` is a monotonic sequence. It is initialized with a minimum value to\r\n * exceed. All subsequent calls to next will return increasing values. If provided with a\r\n * `SequenceNumberSyncer`, it will additionally bump its next value when told of a new value, as\r\n * well as write out sequence numbers that it produces via `next()`.\r\n */\n\n\nclass ListenSequence {\n  constructor(previousValue, sequenceNumberSyncer) {\n    this.previousValue = previousValue;\n\n    if (sequenceNumberSyncer) {\n      sequenceNumberSyncer.sequenceNumberHandler = sequenceNumber => this.setPreviousValue(sequenceNumber);\n\n      this.writeNewSequenceNumber = sequenceNumber => sequenceNumberSyncer.writeSequenceNumber(sequenceNumber);\n    }\n  }\n\n  setPreviousValue(externalPreviousValue) {\n    this.previousValue = Math.max(externalPreviousValue, this.previousValue);\n    return this.previousValue;\n  }\n\n  next() {\n    const nextValue = ++this.previousValue;\n\n    if (this.writeNewSequenceNumber) {\n      this.writeNewSequenceNumber(nextValue);\n    }\n\n    return nextValue;\n  }\n\n}\n\nListenSequence.INVALID = -1;\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\nconst escapeChar = '\\u0001';\nconst encodedSeparatorChar = '\\u0001';\nconst encodedNul = '\\u0010';\nconst encodedEscape = '\\u0011';\n/**\r\n * Encodes a resource path into a IndexedDb-compatible string form.\r\n */\n\nfunction encodeResourcePath(path) {\n  let result = '';\n\n  for (let i = 0; i < path.length; i++) {\n    if (result.length > 0) {\n      result = encodeSeparator(result);\n    }\n\n    result = encodeSegment(path.get(i), result);\n  }\n\n  return encodeSeparator(result);\n}\n/** Encodes a single segment of a resource path into the given result */\n\n\nfunction encodeSegment(segment, resultBuf) {\n  let result = resultBuf;\n  const length = segment.length;\n\n  for (let i = 0; i < length; i++) {\n    const c = segment.charAt(i);\n\n    switch (c) {\n      case '\\0':\n        result += escapeChar + encodedNul;\n        break;\n\n      case escapeChar:\n        result += escapeChar + encodedEscape;\n        break;\n\n      default:\n        result += c;\n    }\n  }\n\n  return result;\n}\n/** Encodes a path separator into the given result */\n\n\nfunction encodeSeparator(result) {\n  return result + escapeChar + encodedSeparatorChar;\n}\n/**\r\n * Decodes the given IndexedDb-compatible string form of a resource path into\r\n * a ResourcePath instance. Note that this method is not suitable for use with\r\n * decoding resource names from the server; those are One Platform format\r\n * strings.\r\n */\n\n\nfunction decodeResourcePath(path) {\n  // Event the empty path must encode as a path of at least length 2. A path\n  // with exactly 2 must be the empty path.\n  const length = path.length;\n  hardAssert(length >= 2);\n\n  if (length === 2) {\n    hardAssert(path.charAt(0) === escapeChar && path.charAt(1) === encodedSeparatorChar);\n    return ResourcePath.emptyPath();\n  } // Escape characters cannot exist past the second-to-last position in the\n  // source value.\n\n\n  const lastReasonableEscapeIndex = length - 2;\n  const segments = [];\n  let segmentBuilder = '';\n\n  for (let start = 0; start < length;) {\n    // The last two characters of a valid encoded path must be a separator, so\n    // there must be an end to this segment.\n    const end = path.indexOf(escapeChar, start);\n\n    if (end < 0 || end > lastReasonableEscapeIndex) {\n      fail();\n    }\n\n    const next = path.charAt(end + 1);\n\n    switch (next) {\n      case encodedSeparatorChar:\n        const currentPiece = path.substring(start, end);\n        let segment;\n\n        if (segmentBuilder.length === 0) {\n          // Avoid copying for the common case of a segment that excludes \\0\n          // and \\001\n          segment = currentPiece;\n        } else {\n          segmentBuilder += currentPiece;\n          segment = segmentBuilder;\n          segmentBuilder = '';\n        }\n\n        segments.push(segment);\n        break;\n\n      case encodedNul:\n        segmentBuilder += path.substring(start, end);\n        segmentBuilder += '\\0';\n        break;\n\n      case encodedEscape:\n        // The escape character can be used in the output to encode itself.\n        segmentBuilder += path.substring(start, end + 1);\n        break;\n\n      default:\n        fail();\n    }\n\n    start = end + 2;\n  }\n\n  return new ResourcePath(segments);\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst DbRemoteDocumentStore$1 = 'remoteDocuments';\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Name of the IndexedDb object store.\r\n *\r\n * Note that the name 'owner' is chosen to ensure backwards compatibility with\r\n * older clients that only supported single locked access to the persistence\r\n * layer.\r\n */\n\nconst DbPrimaryClientStore = 'owner';\n/**\r\n * The key string used for the single object that exists in the\r\n * DbPrimaryClient store.\r\n */\n\nconst DbPrimaryClientKey = 'owner';\n/** Name of the IndexedDb object store.  */\n\nconst DbMutationQueueStore = 'mutationQueues';\n/** Keys are automatically assigned via the userId property. */\n\nconst DbMutationQueueKeyPath = 'userId';\n/** Name of the IndexedDb object store.  */\n\nconst DbMutationBatchStore = 'mutations';\n/** Keys are automatically assigned via the userId, batchId properties. */\n\nconst DbMutationBatchKeyPath = 'batchId';\n/** The index name for lookup of mutations by user. */\n\nconst DbMutationBatchUserMutationsIndex = 'userMutationsIndex';\n/** The user mutations index is keyed by [userId, batchId] pairs. */\n\nconst DbMutationBatchUserMutationsKeyPath = ['userId', 'batchId'];\n/**\r\n * Creates a [userId] key for use in the DbDocumentMutations index to iterate\r\n * over all of a user's document mutations.\r\n */\n\nfunction newDbDocumentMutationPrefixForUser(userId) {\n  return [userId];\n}\n/**\r\n * Creates a [userId, encodedPath] key for use in the DbDocumentMutations\r\n * index to iterate over all at document mutations for a given path or lower.\r\n */\n\n\nfunction newDbDocumentMutationPrefixForPath(userId, path) {\n  return [userId, encodeResourcePath(path)];\n}\n/**\r\n * Creates a full index key of [userId, encodedPath, batchId] for inserting\r\n * and deleting into the DbDocumentMutations index.\r\n */\n\n\nfunction newDbDocumentMutationKey(userId, path, batchId) {\n  return [userId, encodeResourcePath(path), batchId];\n}\n/**\r\n * Because we store all the useful information for this store in the key,\r\n * there is no useful information to store as the value. The raw (unencoded)\r\n * path cannot be stored because IndexedDb doesn't store prototype\r\n * information.\r\n */\n\n\nconst DbDocumentMutationPlaceholder = {};\nconst DbDocumentMutationStore = 'documentMutations';\nconst DbRemoteDocumentStore = 'remoteDocumentsV14';\n/**\r\n * The primary key of the remote documents store, which allows for efficient\r\n * access by collection path and read time.\r\n */\n\nconst DbRemoteDocumentKeyPath = ['prefixPath', 'collectionGroup', 'readTime', 'documentId'];\n/** An index that provides access to documents by key. */\n\nconst DbRemoteDocumentDocumentKeyIndex = 'documentKeyIndex';\nconst DbRemoteDocumentDocumentKeyIndexPath = ['prefixPath', 'collectionGroup', 'documentId'];\n/**\r\n * An index that provides access to documents by collection group and read\r\n * time.\r\n *\r\n * This index is used by the index backfiller.\r\n */\n\nconst DbRemoteDocumentCollectionGroupIndex = 'collectionGroupIndex';\nconst DbRemoteDocumentCollectionGroupIndexPath = ['collectionGroup', 'readTime', 'prefixPath', 'documentId'];\nconst DbRemoteDocumentGlobalStore = 'remoteDocumentGlobal';\nconst DbRemoteDocumentGlobalKey = 'remoteDocumentGlobalKey';\nconst DbTargetStore = 'targets';\n/** Keys are automatically assigned via the targetId property. */\n\nconst DbTargetKeyPath = 'targetId';\n/** The name of the queryTargets index. */\n\nconst DbTargetQueryTargetsIndexName = 'queryTargetsIndex';\n/**\r\n * The index of all canonicalIds to the targets that they match. This is not\r\n * a unique mapping because canonicalId does not promise a unique name for all\r\n * possible queries, so we append the targetId to make the mapping unique.\r\n */\n\nconst DbTargetQueryTargetsKeyPath = ['canonicalId', 'targetId'];\n/** Name of the IndexedDb object store.  */\n\nconst DbTargetDocumentStore = 'targetDocuments';\n/** Keys are automatically assigned via the targetId, path properties. */\n\nconst DbTargetDocumentKeyPath = ['targetId', 'path'];\n/** The index name for the reverse index. */\n\nconst DbTargetDocumentDocumentTargetsIndex = 'documentTargetsIndex';\n/** We also need to create the reverse index for these properties. */\n\nconst DbTargetDocumentDocumentTargetsKeyPath = ['path', 'targetId'];\n/**\r\n * The key string used for the single object that exists in the\r\n * DbTargetGlobal store.\r\n */\n\nconst DbTargetGlobalKey = 'targetGlobalKey';\nconst DbTargetGlobalStore = 'targetGlobal';\n/** Name of the IndexedDb object store. */\n\nconst DbCollectionParentStore = 'collectionParents';\n/** Keys are automatically assigned via the collectionId, parent properties. */\n\nconst DbCollectionParentKeyPath = ['collectionId', 'parent'];\n/** Name of the IndexedDb object store. */\n\nconst DbClientMetadataStore = 'clientMetadata';\n/** Keys are automatically assigned via the clientId properties. */\n\nconst DbClientMetadataKeyPath = 'clientId';\n/** Name of the IndexedDb object store. */\n\nconst DbBundleStore = 'bundles';\nconst DbBundleKeyPath = 'bundleId';\n/** Name of the IndexedDb object store. */\n\nconst DbNamedQueryStore = 'namedQueries';\nconst DbNamedQueryKeyPath = 'name';\n/** Name of the IndexedDb object store. */\n\nconst DbIndexConfigurationStore = 'indexConfiguration';\nconst DbIndexConfigurationKeyPath = 'indexId';\n/**\r\n * An index that provides access to the index configurations by collection\r\n * group.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\n\nconst DbIndexConfigurationCollectionGroupIndex = 'collectionGroupIndex';\nconst DbIndexConfigurationCollectionGroupIndexPath = 'collectionGroup';\n/** Name of the IndexedDb object store. */\n\nconst DbIndexStateStore = 'indexState';\nconst DbIndexStateKeyPath = ['indexId', 'uid'];\n/**\r\n * An index that provides access to documents in a collection sorted by last\r\n * update time. Used by the backfiller.\r\n *\r\n * PORTING NOTE: iOS and Android maintain this index in-memory, but this is\r\n * not possible here as the Web client supports concurrent access to\r\n * persistence via multi-tab.\r\n */\n\nconst DbIndexStateSequenceNumberIndex = 'sequenceNumberIndex';\nconst DbIndexStateSequenceNumberIndexPath = ['uid', 'sequenceNumber'];\n/** Name of the IndexedDb object store. */\n\nconst DbIndexEntryStore = 'indexEntries';\nconst DbIndexEntryKeyPath = ['indexId', 'uid', 'arrayValue', 'directionalValue', 'orderedDocumentKey', 'documentKey'];\nconst DbIndexEntryDocumentKeyIndex = 'documentKeyIndex';\nconst DbIndexEntryDocumentKeyIndexPath = ['indexId', 'uid', 'orderedDocumentKey'];\n/** Name of the IndexedDb object store. */\n\nconst DbDocumentOverlayStore = 'documentOverlays';\nconst DbDocumentOverlayKeyPath = ['userId', 'collectionPath', 'documentId'];\nconst DbDocumentOverlayCollectionPathOverlayIndex = 'collectionPathOverlayIndex';\nconst DbDocumentOverlayCollectionPathOverlayIndexPath = ['userId', 'collectionPath', 'largestBatchId'];\nconst DbDocumentOverlayCollectionGroupOverlayIndex = 'collectionGroupOverlayIndex';\nconst DbDocumentOverlayCollectionGroupOverlayIndexPath = ['userId', 'collectionGroup', 'largestBatchId']; // Visible for testing\n\nconst V1_STORES = [DbMutationQueueStore, DbMutationBatchStore, DbDocumentMutationStore, DbRemoteDocumentStore$1, DbTargetStore, DbPrimaryClientStore, DbTargetGlobalStore, DbTargetDocumentStore]; // Visible for testing\n\nconst V3_STORES = V1_STORES; // Note: DbRemoteDocumentChanges is no longer used and dropped with v9.\n\nconst V4_STORES = [...V3_STORES, DbClientMetadataStore];\nconst V6_STORES = [...V4_STORES, DbRemoteDocumentGlobalStore];\nconst V8_STORES = [...V6_STORES, DbCollectionParentStore];\nconst V11_STORES = [...V8_STORES, DbBundleStore, DbNamedQueryStore];\nconst V12_STORES = [...V11_STORES, DbDocumentOverlayStore];\nconst V13_STORES = [DbMutationQueueStore, DbMutationBatchStore, DbDocumentMutationStore, DbRemoteDocumentStore, DbTargetStore, DbPrimaryClientStore, DbTargetGlobalStore, DbTargetDocumentStore, DbClientMetadataStore, DbRemoteDocumentGlobalStore, DbCollectionParentStore, DbBundleStore, DbNamedQueryStore, DbDocumentOverlayStore];\nconst V14_STORES = V13_STORES;\nconst V15_STORES = [...V14_STORES, DbIndexConfigurationStore, DbIndexStateStore, DbIndexEntryStore];\n/** Returns the object stores for the provided schema. */\n\nfunction getObjectStores(schemaVersion) {\n  if (schemaVersion === 15) {\n    return V15_STORES;\n  } else if (schemaVersion === 14) {\n    return V14_STORES;\n  } else if (schemaVersion === 13) {\n    return V13_STORES;\n  } else if (schemaVersion === 12) {\n    return V12_STORES;\n  } else if (schemaVersion === 11) {\n    return V11_STORES;\n  } else {\n    fail();\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass IndexedDbTransaction extends PersistenceTransaction {\n  constructor(simpleDbTransaction, currentSequenceNumber) {\n    super();\n    this.simpleDbTransaction = simpleDbTransaction;\n    this.currentSequenceNumber = currentSequenceNumber;\n  }\n\n}\n\nfunction getStore(txn, store) {\n  const indexedDbTransaction = debugCast(txn);\n  return SimpleDb.getStore(indexedDbTransaction.simpleDbTransaction, store);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction objectSize(obj) {\n  let count = 0;\n\n  for (const key in obj) {\n    if (Object.prototype.hasOwnProperty.call(obj, key)) {\n      count++;\n    }\n  }\n\n  return count;\n}\n\nfunction forEach(obj, fn) {\n  for (const key in obj) {\n    if (Object.prototype.hasOwnProperty.call(obj, key)) {\n      fn(key, obj[key]);\n    }\n  }\n}\n\nfunction mapToArray(obj, fn) {\n  const result = [];\n\n  for (const key in obj) {\n    if (Object.prototype.hasOwnProperty.call(obj, key)) {\n      result.push(fn(obj[key], key, obj));\n    }\n  }\n\n  return result;\n}\n\nfunction isEmpty(obj) {\n  for (const key in obj) {\n    if (Object.prototype.hasOwnProperty.call(obj, key)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// An immutable sorted map implementation, based on a Left-leaning Red-Black\n// tree.\n\n\nclass SortedMap {\n  constructor(comparator, root) {\n    this.comparator = comparator;\n    this.root = root ? root : LLRBNode.EMPTY;\n  } // Returns a copy of the map, with the specified key/value added or replaced.\n\n\n  insert(key, value) {\n    return new SortedMap(this.comparator, this.root.insert(key, value, this.comparator).copy(null, null, LLRBNode.BLACK, null, null));\n  } // Returns a copy of the map, with the specified key removed.\n\n\n  remove(key) {\n    return new SortedMap(this.comparator, this.root.remove(key, this.comparator).copy(null, null, LLRBNode.BLACK, null, null));\n  } // Returns the value of the node with the given key, or null.\n\n\n  get(key) {\n    let node = this.root;\n\n    while (!node.isEmpty()) {\n      const cmp = this.comparator(key, node.key);\n\n      if (cmp === 0) {\n        return node.value;\n      } else if (cmp < 0) {\n        node = node.left;\n      } else if (cmp > 0) {\n        node = node.right;\n      }\n    }\n\n    return null;\n  } // Returns the index of the element in this sorted map, or -1 if it doesn't\n  // exist.\n\n\n  indexOf(key) {\n    // Number of nodes that were pruned when descending right\n    let prunedNodes = 0;\n    let node = this.root;\n\n    while (!node.isEmpty()) {\n      const cmp = this.comparator(key, node.key);\n\n      if (cmp === 0) {\n        return prunedNodes + node.left.size;\n      } else if (cmp < 0) {\n        node = node.left;\n      } else {\n        // Count all nodes left of the node plus the node itself\n        prunedNodes += node.left.size + 1;\n        node = node.right;\n      }\n    } // Node not found\n\n\n    return -1;\n  }\n\n  isEmpty() {\n    return this.root.isEmpty();\n  } // Returns the total number of nodes in the map.\n\n\n  get size() {\n    return this.root.size;\n  } // Returns the minimum key in the map.\n\n\n  minKey() {\n    return this.root.minKey();\n  } // Returns the maximum key in the map.\n\n\n  maxKey() {\n    return this.root.maxKey();\n  } // Traverses the map in key order and calls the specified action function\n  // for each key/value pair. If action returns true, traversal is aborted.\n  // Returns the first truthy value returned by action, or the last falsey\n  // value returned by action.\n\n\n  inorderTraversal(action) {\n    return this.root.inorderTraversal(action);\n  }\n\n  forEach(fn) {\n    this.inorderTraversal((k, v) => {\n      fn(k, v);\n      return false;\n    });\n  }\n\n  toString() {\n    const descriptions = [];\n    this.inorderTraversal((k, v) => {\n      descriptions.push(`${k}:${v}`);\n      return false;\n    });\n    return `{${descriptions.join(', ')}}`;\n  } // Traverses the map in reverse key order and calls the specified action\n  // function for each key/value pair. If action returns true, traversal is\n  // aborted.\n  // Returns the first truthy value returned by action, or the last falsey\n  // value returned by action.\n\n\n  reverseTraversal(action) {\n    return this.root.reverseTraversal(action);\n  } // Returns an iterator over the SortedMap.\n\n\n  getIterator() {\n    return new SortedMapIterator(this.root, null, this.comparator, false);\n  }\n\n  getIteratorFrom(key) {\n    return new SortedMapIterator(this.root, key, this.comparator, false);\n  }\n\n  getReverseIterator() {\n    return new SortedMapIterator(this.root, null, this.comparator, true);\n  }\n\n  getReverseIteratorFrom(key) {\n    return new SortedMapIterator(this.root, key, this.comparator, true);\n  }\n\n} // end SortedMap\n// An iterator over an LLRBNode.\n\n\nclass SortedMapIterator {\n  constructor(node, startKey, comparator, isReverse) {\n    this.isReverse = isReverse;\n    this.nodeStack = [];\n    let cmp = 1;\n\n    while (!node.isEmpty()) {\n      cmp = startKey ? comparator(node.key, startKey) : 1; // flip the comparison if we're going in reverse\n\n      if (startKey && isReverse) {\n        cmp *= -1;\n      }\n\n      if (cmp < 0) {\n        // This node is less than our start key. ignore it\n        if (this.isReverse) {\n          node = node.left;\n        } else {\n          node = node.right;\n        }\n      } else if (cmp === 0) {\n        // This node is exactly equal to our start key. Push it on the stack,\n        // but stop iterating;\n        this.nodeStack.push(node);\n        break;\n      } else {\n        // This node is greater than our start key, add it to the stack and move\n        // to the next one\n        this.nodeStack.push(node);\n\n        if (this.isReverse) {\n          node = node.right;\n        } else {\n          node = node.left;\n        }\n      }\n    }\n  }\n\n  getNext() {\n    let node = this.nodeStack.pop();\n    const result = {\n      key: node.key,\n      value: node.value\n    };\n\n    if (this.isReverse) {\n      node = node.left;\n\n      while (!node.isEmpty()) {\n        this.nodeStack.push(node);\n        node = node.right;\n      }\n    } else {\n      node = node.right;\n\n      while (!node.isEmpty()) {\n        this.nodeStack.push(node);\n        node = node.left;\n      }\n    }\n\n    return result;\n  }\n\n  hasNext() {\n    return this.nodeStack.length > 0;\n  }\n\n  peek() {\n    if (this.nodeStack.length === 0) {\n      return null;\n    }\n\n    const node = this.nodeStack[this.nodeStack.length - 1];\n    return {\n      key: node.key,\n      value: node.value\n    };\n  }\n\n} // end SortedMapIterator\n// Represents a node in a Left-leaning Red-Black tree.\n\n\nclass LLRBNode {\n  constructor(key, value, color, left, right) {\n    this.key = key;\n    this.value = value;\n    this.color = color != null ? color : LLRBNode.RED;\n    this.left = left != null ? left : LLRBNode.EMPTY;\n    this.right = right != null ? right : LLRBNode.EMPTY;\n    this.size = this.left.size + 1 + this.right.size;\n  } // Returns a copy of the current node, optionally replacing pieces of it.\n\n\n  copy(key, value, color, left, right) {\n    return new LLRBNode(key != null ? key : this.key, value != null ? value : this.value, color != null ? color : this.color, left != null ? left : this.left, right != null ? right : this.right);\n  }\n\n  isEmpty() {\n    return false;\n  } // Traverses the tree in key order and calls the specified action function\n  // for each node. If action returns true, traversal is aborted.\n  // Returns the first truthy value returned by action, or the last falsey\n  // value returned by action.\n\n\n  inorderTraversal(action) {\n    return this.left.inorderTraversal(action) || action(this.key, this.value) || this.right.inorderTraversal(action);\n  } // Traverses the tree in reverse key order and calls the specified action\n  // function for each node. If action returns true, traversal is aborted.\n  // Returns the first truthy value returned by action, or the last falsey\n  // value returned by action.\n\n\n  reverseTraversal(action) {\n    return this.right.reverseTraversal(action) || action(this.key, this.value) || this.left.reverseTraversal(action);\n  } // Returns the minimum node in the tree.\n\n\n  min() {\n    if (this.left.isEmpty()) {\n      return this;\n    } else {\n      return this.left.min();\n    }\n  } // Returns the maximum key in the tree.\n\n\n  minKey() {\n    return this.min().key;\n  } // Returns the maximum key in the tree.\n\n\n  maxKey() {\n    if (this.right.isEmpty()) {\n      return this.key;\n    } else {\n      return this.right.maxKey();\n    }\n  } // Returns new tree, with the key/value added.\n\n\n  insert(key, value, comparator) {\n    let n = this;\n    const cmp = comparator(key, n.key);\n\n    if (cmp < 0) {\n      n = n.copy(null, null, null, n.left.insert(key, value, comparator), null);\n    } else if (cmp === 0) {\n      n = n.copy(null, value, null, null, null);\n    } else {\n      n = n.copy(null, null, null, null, n.right.insert(key, value, comparator));\n    }\n\n    return n.fixUp();\n  }\n\n  removeMin() {\n    if (this.left.isEmpty()) {\n      return LLRBNode.EMPTY;\n    }\n\n    let n = this;\n\n    if (!n.left.isRed() && !n.left.left.isRed()) {\n      n = n.moveRedLeft();\n    }\n\n    n = n.copy(null, null, null, n.left.removeMin(), null);\n    return n.fixUp();\n  } // Returns new tree, with the specified item removed.\n\n\n  remove(key, comparator) {\n    let smallest;\n    let n = this;\n\n    if (comparator(key, n.key) < 0) {\n      if (!n.left.isEmpty() && !n.left.isRed() && !n.left.left.isRed()) {\n        n = n.moveRedLeft();\n      }\n\n      n = n.copy(null, null, null, n.left.remove(key, comparator), null);\n    } else {\n      if (n.left.isRed()) {\n        n = n.rotateRight();\n      }\n\n      if (!n.right.isEmpty() && !n.right.isRed() && !n.right.left.isRed()) {\n        n = n.moveRedRight();\n      }\n\n      if (comparator(key, n.key) === 0) {\n        if (n.right.isEmpty()) {\n          return LLRBNode.EMPTY;\n        } else {\n          smallest = n.right.min();\n          n = n.copy(smallest.key, smallest.value, null, null, n.right.removeMin());\n        }\n      }\n\n      n = n.copy(null, null, null, null, n.right.remove(key, comparator));\n    }\n\n    return n.fixUp();\n  }\n\n  isRed() {\n    return this.color;\n  } // Returns new tree after performing any needed rotations.\n\n\n  fixUp() {\n    let n = this;\n\n    if (n.right.isRed() && !n.left.isRed()) {\n      n = n.rotateLeft();\n    }\n\n    if (n.left.isRed() && n.left.left.isRed()) {\n      n = n.rotateRight();\n    }\n\n    if (n.left.isRed() && n.right.isRed()) {\n      n = n.colorFlip();\n    }\n\n    return n;\n  }\n\n  moveRedLeft() {\n    let n = this.colorFlip();\n\n    if (n.right.left.isRed()) {\n      n = n.copy(null, null, null, null, n.right.rotateRight());\n      n = n.rotateLeft();\n      n = n.colorFlip();\n    }\n\n    return n;\n  }\n\n  moveRedRight() {\n    let n = this.colorFlip();\n\n    if (n.left.left.isRed()) {\n      n = n.rotateRight();\n      n = n.colorFlip();\n    }\n\n    return n;\n  }\n\n  rotateLeft() {\n    const nl = this.copy(null, null, LLRBNode.RED, null, this.right.left);\n    return this.right.copy(null, null, this.color, nl, null);\n  }\n\n  rotateRight() {\n    const nr = this.copy(null, null, LLRBNode.RED, this.left.right, null);\n    return this.left.copy(null, null, this.color, null, nr);\n  }\n\n  colorFlip() {\n    const left = this.left.copy(null, null, !this.left.color, null, null);\n    const right = this.right.copy(null, null, !this.right.color, null, null);\n    return this.copy(null, null, !this.color, left, right);\n  } // For testing.\n\n\n  checkMaxDepth() {\n    const blackDepth = this.check();\n\n    if (Math.pow(2.0, blackDepth) <= this.size + 1) {\n      return true;\n    } else {\n      return false;\n    }\n  } // In a balanced RB tree, the black-depth (number of black nodes) from root to\n  // leaves is equal on both sides.  This function verifies that or asserts.\n\n\n  check() {\n    if (this.isRed() && this.left.isRed()) {\n      throw fail();\n    }\n\n    if (this.right.isRed()) {\n      throw fail();\n    }\n\n    const blackDepth = this.left.check();\n\n    if (blackDepth !== this.right.check()) {\n      throw fail();\n    } else {\n      return blackDepth + (this.isRed() ? 0 : 1);\n    }\n  }\n\n} // end LLRBNode\n// Empty node is shared between all LLRB trees.\n// eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n\nLLRBNode.EMPTY = null;\nLLRBNode.RED = true;\nLLRBNode.BLACK = false; // Represents an empty node (a leaf node in the Red-Black Tree).\n\nclass LLRBEmptyNode {\n  constructor() {\n    this.size = 0;\n  }\n\n  get key() {\n    throw fail();\n  }\n\n  get value() {\n    throw fail();\n  }\n\n  get color() {\n    throw fail();\n  }\n\n  get left() {\n    throw fail();\n  }\n\n  get right() {\n    throw fail();\n  } // Returns a copy of the current node.\n\n\n  copy(key, value, color, left, right) {\n    return this;\n  } // Returns a copy of the tree, with the specified key/value added.\n\n\n  insert(key, value, comparator) {\n    return new LLRBNode(key, value);\n  } // Returns a copy of the tree, with the specified key removed.\n\n\n  remove(key, comparator) {\n    return this;\n  }\n\n  isEmpty() {\n    return true;\n  }\n\n  inorderTraversal(action) {\n    return false;\n  }\n\n  reverseTraversal(action) {\n    return false;\n  }\n\n  minKey() {\n    return null;\n  }\n\n  maxKey() {\n    return null;\n  }\n\n  isRed() {\n    return false;\n  } // For testing.\n\n\n  checkMaxDepth() {\n    return true;\n  }\n\n  check() {\n    return 0;\n  }\n\n} // end LLRBEmptyNode\n\n\nLLRBNode.EMPTY = new LLRBEmptyNode();\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * SortedSet is an immutable (copy-on-write) collection that holds elements\r\n * in order specified by the provided comparator.\r\n *\r\n * NOTE: if provided comparator returns 0 for two elements, we consider them to\r\n * be equal!\r\n */\n\nclass SortedSet {\n  constructor(comparator) {\n    this.comparator = comparator;\n    this.data = new SortedMap(this.comparator);\n  }\n\n  has(elem) {\n    return this.data.get(elem) !== null;\n  }\n\n  first() {\n    return this.data.minKey();\n  }\n\n  last() {\n    return this.data.maxKey();\n  }\n\n  get size() {\n    return this.data.size;\n  }\n\n  indexOf(elem) {\n    return this.data.indexOf(elem);\n  }\n  /** Iterates elements in order defined by \"comparator\" */\n\n\n  forEach(cb) {\n    this.data.inorderTraversal((k, v) => {\n      cb(k);\n      return false;\n    });\n  }\n  /** Iterates over `elem`s such that: range[0] &lt;= elem &lt; range[1]. */\n\n\n  forEachInRange(range, cb) {\n    const iter = this.data.getIteratorFrom(range[0]);\n\n    while (iter.hasNext()) {\n      const elem = iter.getNext();\n\n      if (this.comparator(elem.key, range[1]) >= 0) {\n        return;\n      }\n\n      cb(elem.key);\n    }\n  }\n  /**\r\n   * Iterates over `elem`s such that: start &lt;= elem until false is returned.\r\n   */\n\n\n  forEachWhile(cb, start) {\n    let iter;\n\n    if (start !== undefined) {\n      iter = this.data.getIteratorFrom(start);\n    } else {\n      iter = this.data.getIterator();\n    }\n\n    while (iter.hasNext()) {\n      const elem = iter.getNext();\n      const result = cb(elem.key);\n\n      if (!result) {\n        return;\n      }\n    }\n  }\n  /** Finds the least element greater than or equal to `elem`. */\n\n\n  firstAfterOrEqual(elem) {\n    const iter = this.data.getIteratorFrom(elem);\n    return iter.hasNext() ? iter.getNext().key : null;\n  }\n\n  getIterator() {\n    return new SortedSetIterator(this.data.getIterator());\n  }\n\n  getIteratorFrom(key) {\n    return new SortedSetIterator(this.data.getIteratorFrom(key));\n  }\n  /** Inserts or updates an element */\n\n\n  add(elem) {\n    return this.copy(this.data.remove(elem).insert(elem, true));\n  }\n  /** Deletes an element */\n\n\n  delete(elem) {\n    if (!this.has(elem)) {\n      return this;\n    }\n\n    return this.copy(this.data.remove(elem));\n  }\n\n  isEmpty() {\n    return this.data.isEmpty();\n  }\n\n  unionWith(other) {\n    let result = this; // Make sure `result` always refers to the larger one of the two sets.\n\n    if (result.size < other.size) {\n      result = other;\n      other = this;\n    }\n\n    other.forEach(elem => {\n      result = result.add(elem);\n    });\n    return result;\n  }\n\n  isEqual(other) {\n    if (!(other instanceof SortedSet)) {\n      return false;\n    }\n\n    if (this.size !== other.size) {\n      return false;\n    }\n\n    const thisIt = this.data.getIterator();\n    const otherIt = other.data.getIterator();\n\n    while (thisIt.hasNext()) {\n      const thisElem = thisIt.getNext().key;\n      const otherElem = otherIt.getNext().key;\n\n      if (this.comparator(thisElem, otherElem) !== 0) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  toArray() {\n    const res = [];\n    this.forEach(targetId => {\n      res.push(targetId);\n    });\n    return res;\n  }\n\n  toString() {\n    const result = [];\n    this.forEach(elem => result.push(elem));\n    return 'SortedSet(' + result.toString() + ')';\n  }\n\n  copy(data) {\n    const result = new SortedSet(this.comparator);\n    result.data = data;\n    return result;\n  }\n\n}\n\nclass SortedSetIterator {\n  constructor(iter) {\n    this.iter = iter;\n  }\n\n  getNext() {\n    return this.iter.getNext().key;\n  }\n\n  hasNext() {\n    return this.iter.hasNext();\n  }\n\n}\n/**\r\n * Compares two sorted sets for equality using their natural ordering. The\r\n * method computes the intersection and invokes `onAdd` for every element that\r\n * is in `after` but not `before`. `onRemove` is invoked for every element in\r\n * `before` but missing from `after`.\r\n *\r\n * The method creates a copy of both `before` and `after` and runs in O(n log\r\n * n), where n is the size of the two lists.\r\n *\r\n * @param before - The elements that exist in the original set.\r\n * @param after - The elements to diff against the original set.\r\n * @param comparator - The comparator for the elements in before and after.\r\n * @param onAdd - A function to invoke for every element that is part of `\r\n * after` but not `before`.\r\n * @param onRemove - A function to invoke for every element that is part of\r\n * `before` but not `after`.\r\n */\n\n\nfunction diffSortedSets(before, after, comparator, onAdd, onRemove) {\n  const beforeIt = before.getIterator();\n  const afterIt = after.getIterator();\n  let beforeValue = advanceIterator(beforeIt);\n  let afterValue = advanceIterator(afterIt); // Walk through the two sets at the same time, using the ordering defined by\n  // `comparator`.\n\n  while (beforeValue || afterValue) {\n    let added = false;\n    let removed = false;\n\n    if (beforeValue && afterValue) {\n      const cmp = comparator(beforeValue, afterValue);\n\n      if (cmp < 0) {\n        // The element was removed if the next element in our ordered\n        // walkthrough is only in `before`.\n        removed = true;\n      } else if (cmp > 0) {\n        // The element was added if the next element in our ordered walkthrough\n        // is only in `after`.\n        added = true;\n      }\n    } else if (beforeValue != null) {\n      removed = true;\n    } else {\n      added = true;\n    }\n\n    if (added) {\n      onAdd(afterValue);\n      afterValue = advanceIterator(afterIt);\n    } else if (removed) {\n      onRemove(beforeValue);\n      beforeValue = advanceIterator(beforeIt);\n    } else {\n      beforeValue = advanceIterator(beforeIt);\n      afterValue = advanceIterator(afterIt);\n    }\n  }\n}\n/**\r\n * Returns the next element from the iterator or `undefined` if none available.\r\n */\n\n\nfunction advanceIterator(it) {\n  return it.hasNext() ? it.getNext() : undefined;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Provides a set of fields that can be used to partially patch a document.\r\n * FieldMask is used in conjunction with ObjectValue.\r\n * Examples:\r\n *   foo - Overwrites foo entirely with the provided value. If foo is not\r\n *         present in the companion ObjectValue, the field is deleted.\r\n *   foo.bar - Overwrites only the field bar of the object foo.\r\n *             If foo is not an object, foo is replaced with an object\r\n *             containing foo\r\n */\n\n\nclass FieldMask {\n  constructor(fields) {\n    this.fields = fields; // TODO(dimond): validation of FieldMask\n    // Sort the field mask to support `FieldMask.isEqual()` and assert below.\n\n    fields.sort(FieldPath$1.comparator);\n  }\n\n  static empty() {\n    return new FieldMask([]);\n  }\n  /**\r\n   * Returns a new FieldMask object that is the result of adding all the given\r\n   * fields paths to this field mask.\r\n   */\n\n\n  unionWith(extraFields) {\n    let mergedMaskSet = new SortedSet(FieldPath$1.comparator);\n\n    for (const fieldPath of this.fields) {\n      mergedMaskSet = mergedMaskSet.add(fieldPath);\n    }\n\n    for (const fieldPath of extraFields) {\n      mergedMaskSet = mergedMaskSet.add(fieldPath);\n    }\n\n    return new FieldMask(mergedMaskSet.toArray());\n  }\n  /**\r\n   * Verifies that `fieldPath` is included by at least one field in this field\r\n   * mask.\r\n   *\r\n   * This is an O(n) operation, where `n` is the size of the field mask.\r\n   */\n\n\n  covers(fieldPath) {\n    for (const fieldMaskPath of this.fields) {\n      if (fieldMaskPath.isPrefixOf(fieldPath)) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  isEqual(other) {\n    return arrayEquals(this.fields, other.fields, (l, r) => l.isEqual(r));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Converts a Base64 encoded string to a binary string. */\n\n\nfunction decodeBase64(encoded) {\n  // Note: We used to validate the base64 string here via a regular expression.\n  // This was removed to improve the performance of indexing.\n  return Buffer.from(encoded, 'base64').toString('binary');\n}\n/** Converts a binary string to a Base64 encoded string. */\n\n\nfunction encodeBase64(raw) {\n  return Buffer.from(raw, 'binary').toString('base64');\n}\n/** True if and only if the Base64 conversion functions are available. */\n\n\nfunction isBase64Available() {\n  return true;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Immutable class that represents a \"proto\" byte string.\r\n *\r\n * Proto byte strings can either be Base64-encoded strings or Uint8Arrays when\r\n * sent on the wire. This class abstracts away this differentiation by holding\r\n * the proto byte string in a common class that must be converted into a string\r\n * before being sent as a proto.\r\n * @internal\r\n */\n\n\nclass ByteString {\n  constructor(binaryString) {\n    this.binaryString = binaryString;\n  }\n\n  static fromBase64String(base64) {\n    const binaryString = decodeBase64(base64);\n    return new ByteString(binaryString);\n  }\n\n  static fromUint8Array(array) {\n    // TODO(indexing); Remove the copy of the byte string here as this method\n    // is frequently called during indexing.\n    const binaryString = binaryStringFromUint8Array(array);\n    return new ByteString(binaryString);\n  }\n\n  [Symbol.iterator]() {\n    let i = 0;\n    return {\n      next: () => {\n        if (i < this.binaryString.length) {\n          return {\n            value: this.binaryString.charCodeAt(i++),\n            done: false\n          };\n        } else {\n          return {\n            value: undefined,\n            done: true\n          };\n        }\n      }\n    };\n  }\n\n  toBase64() {\n    return encodeBase64(this.binaryString);\n  }\n\n  toUint8Array() {\n    return uint8ArrayFromBinaryString(this.binaryString);\n  }\n\n  approximateByteSize() {\n    return this.binaryString.length * 2;\n  }\n\n  compareTo(other) {\n    return primitiveComparator(this.binaryString, other.binaryString);\n  }\n\n  isEqual(other) {\n    return this.binaryString === other.binaryString;\n  }\n\n}\n\nByteString.EMPTY_BYTE_STRING = new ByteString('');\n/**\r\n * Helper function to convert an Uint8array to a binary string.\r\n */\n\nfunction binaryStringFromUint8Array(array) {\n  let binaryString = '';\n\n  for (let i = 0; i < array.length; ++i) {\n    binaryString += String.fromCharCode(array[i]);\n  }\n\n  return binaryString;\n}\n/**\r\n * Helper function to convert a binary string to an Uint8Array.\r\n */\n\n\nfunction uint8ArrayFromBinaryString(binaryString) {\n  const buffer = new Uint8Array(binaryString.length);\n\n  for (let i = 0; i < binaryString.length; i++) {\n    buffer[i] = binaryString.charCodeAt(i);\n  }\n\n  return buffer;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// A RegExp matching ISO 8601 UTC timestamps with optional fraction.\n\n\nconst ISO_TIMESTAMP_REG_EXP = new RegExp(/^\\d{4}-\\d\\d-\\d\\dT\\d\\d:\\d\\d:\\d\\d(?:\\.(\\d+))?Z$/);\n/**\r\n * Converts the possible Proto values for a timestamp value into a \"seconds and\r\n * nanos\" representation.\r\n */\n\nfunction normalizeTimestamp(date) {\n  hardAssert(!!date); // The json interface (for the browser) will return an iso timestamp string,\n  // while the proto js library (for node) will return a\n  // google.protobuf.Timestamp instance.\n\n  if (typeof date === 'string') {\n    // The date string can have higher precision (nanos) than the Date class\n    // (millis), so we do some custom parsing here.\n    // Parse the nanos right out of the string.\n    let nanos = 0;\n    const fraction = ISO_TIMESTAMP_REG_EXP.exec(date);\n    hardAssert(!!fraction);\n\n    if (fraction[1]) {\n      // Pad the fraction out to 9 digits (nanos).\n      let nanoStr = fraction[1];\n      nanoStr = (nanoStr + '000000000').substr(0, 9);\n      nanos = Number(nanoStr);\n    } // Parse the date to get the seconds.\n\n\n    const parsedDate = new Date(date);\n    const seconds = Math.floor(parsedDate.getTime() / 1000);\n    return {\n      seconds,\n      nanos\n    };\n  } else {\n    // TODO(b/37282237): Use strings for Proto3 timestamps\n    // assert(!this.options.useProto3Json,\n    //   'The timestamp instance format requires Proto JS.');\n    const seconds = normalizeNumber(date.seconds);\n    const nanos = normalizeNumber(date.nanos);\n    return {\n      seconds,\n      nanos\n    };\n  }\n}\n/**\r\n * Converts the possible Proto types for numbers into a JavaScript number.\r\n * Returns 0 if the value is not numeric.\r\n */\n\n\nfunction normalizeNumber(value) {\n  // TODO(bjornick): Handle int64 greater than 53 bits.\n  if (typeof value === 'number') {\n    return value;\n  } else if (typeof value === 'string') {\n    return Number(value);\n  } else {\n    return 0;\n  }\n}\n/** Converts the possible Proto types for Blobs into a ByteString. */\n\n\nfunction normalizeByteString(blob) {\n  if (typeof blob === 'string') {\n    return ByteString.fromBase64String(blob);\n  } else {\n    return ByteString.fromUint8Array(blob);\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents a locally-applied ServerTimestamp.\r\n *\r\n * Server Timestamps are backed by MapValues that contain an internal field\r\n * `__type__` with a value of `server_timestamp`. The previous value and local\r\n * write time are stored in its `__previous_value__` and `__local_write_time__`\r\n * fields respectively.\r\n *\r\n * Notes:\r\n * - ServerTimestampValue instances are created as the result of applying a\r\n *   transform. They can only exist in the local view of a document. Therefore\r\n *   they do not need to be parsed or serialized.\r\n * - When evaluated locally (e.g. for snapshot.data()), they by default\r\n *   evaluate to `null`. This behavior can be configured by passing custom\r\n *   FieldValueOptions to value().\r\n * - With respect to other ServerTimestampValues, they sort by their\r\n *   localWriteTime.\r\n */\n\n\nconst SERVER_TIMESTAMP_SENTINEL = 'server_timestamp';\nconst TYPE_KEY = '__type__';\nconst PREVIOUS_VALUE_KEY = '__previous_value__';\nconst LOCAL_WRITE_TIME_KEY = '__local_write_time__';\n\nfunction isServerTimestamp(value) {\n  var _a, _b;\n\n  const type = (_b = (((_a = value === null || value === void 0 ? void 0 : value.mapValue) === null || _a === void 0 ? void 0 : _a.fields) || {})[TYPE_KEY]) === null || _b === void 0 ? void 0 : _b.stringValue;\n  return type === SERVER_TIMESTAMP_SENTINEL;\n}\n/**\r\n * Creates a new ServerTimestamp proto value (using the internal format).\r\n */\n\n\nfunction serverTimestamp$1(localWriteTime, previousValue) {\n  const mapValue = {\n    fields: {\n      [TYPE_KEY]: {\n        stringValue: SERVER_TIMESTAMP_SENTINEL\n      },\n      [LOCAL_WRITE_TIME_KEY]: {\n        timestampValue: {\n          seconds: localWriteTime.seconds,\n          nanos: localWriteTime.nanoseconds\n        }\n      }\n    }\n  }; // We should avoid storing deeply nested server timestamp map values\n  // because we never use the intermediate \"previous values\".\n  // For example:\n  // previous: 42L, add: t1, result: t1 -> 42L\n  // previous: t1,  add: t2, result: t2 -> 42L (NOT t2 -> t1 -> 42L)\n  // previous: t2,  add: t3, result: t3 -> 42L (NOT t3 -> t2 -> t1 -> 42L)\n  // `getPreviousValue` recursively traverses server timestamps to find the\n  // least recent Value.\n\n  if (previousValue && isServerTimestamp(previousValue)) {\n    previousValue = getPreviousValue(previousValue);\n  }\n\n  if (previousValue) {\n    mapValue.fields[PREVIOUS_VALUE_KEY] = previousValue;\n  }\n\n  return {\n    mapValue\n  };\n}\n/**\r\n * Returns the value of the field before this ServerTimestamp was set.\r\n *\r\n * Preserving the previous values allows the user to display the last resoled\r\n * value until the backend responds with the timestamp.\r\n */\n\n\nfunction getPreviousValue(value) {\n  const previousValue = value.mapValue.fields[PREVIOUS_VALUE_KEY];\n\n  if (isServerTimestamp(previousValue)) {\n    return getPreviousValue(previousValue);\n  }\n\n  return previousValue;\n}\n/**\r\n * Returns the local time at which this timestamp was first set.\r\n */\n\n\nfunction getLocalWriteTime(value) {\n  const localWriteTime = normalizeTimestamp(value.mapValue.fields[LOCAL_WRITE_TIME_KEY].timestampValue);\n  return new Timestamp(localWriteTime.seconds, localWriteTime.nanos);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass DatabaseInfo {\n  /**\r\n   * Constructs a DatabaseInfo using the provided host, databaseId and\r\n   * persistenceKey.\r\n   *\r\n   * @param databaseId - The database to use.\r\n   * @param appId - The Firebase App Id.\r\n   * @param persistenceKey - A unique identifier for this Firestore's local\r\n   * storage (used in conjunction with the databaseId).\r\n   * @param host - The Firestore backend host to connect to.\r\n   * @param ssl - Whether to use SSL when connecting.\r\n   * @param forceLongPolling - Whether to use the forceLongPolling option\r\n   * when using WebChannel as the network transport.\r\n   * @param autoDetectLongPolling - Whether to use the detectBufferingProxy\r\n   * option when using WebChannel as the network transport.\r\n   * @param longPollingOptions Options that configure long-polling.\r\n   * @param useFetchStreams Whether to use the Fetch API instead of\r\n   * XMLHTTPRequest\r\n   */\n  constructor(databaseId, appId, persistenceKey, host, ssl, forceLongPolling, autoDetectLongPolling, longPollingOptions, useFetchStreams) {\n    this.databaseId = databaseId;\n    this.appId = appId;\n    this.persistenceKey = persistenceKey;\n    this.host = host;\n    this.ssl = ssl;\n    this.forceLongPolling = forceLongPolling;\n    this.autoDetectLongPolling = autoDetectLongPolling;\n    this.longPollingOptions = longPollingOptions;\n    this.useFetchStreams = useFetchStreams;\n  }\n\n}\n/** The default database name for a project. */\n\n\nconst DEFAULT_DATABASE_NAME = '(default)';\n/**\r\n * Represents the database ID a Firestore client is associated with.\r\n * @internal\r\n */\n\nclass DatabaseId {\n  constructor(projectId, database) {\n    this.projectId = projectId;\n    this.database = database ? database : DEFAULT_DATABASE_NAME;\n  }\n\n  static empty() {\n    return new DatabaseId('', '');\n  }\n\n  get isDefaultDatabase() {\n    return this.database === DEFAULT_DATABASE_NAME;\n  }\n\n  isEqual(other) {\n    return other instanceof DatabaseId && other.projectId === this.projectId && other.database === this.database;\n  }\n\n}\n\nfunction databaseIdFromApp(app, database) {\n  if (!Object.prototype.hasOwnProperty.apply(app.options, ['projectId'])) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, '\"projectId\" not provided in firebase.initializeApp.');\n  }\n\n  return new DatabaseId(app.options.projectId, database);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Sentinel value that sorts before any Mutation Batch ID. */\n\n\nconst BATCHID_UNKNOWN = -1;\n/**\r\n * Returns whether a variable is either undefined or null.\r\n */\n\nfunction isNullOrUndefined(value) {\n  return value === null || value === undefined;\n}\n/** Returns whether the value represents -0. */\n\n\nfunction isNegativeZero(value) {\n  // Detect if the value is -0.0. Based on polyfill from\n  // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/is\n  return value === 0 && 1 / value === 1 / -0;\n}\n/**\r\n * Returns whether a value is an integer and in the safe integer range\r\n * @param value - The value to test for being an integer and in the safe range\r\n */\n\n\nfunction isSafeInteger(value) {\n  return typeof value === 'number' && Number.isInteger(value) && !isNegativeZero(value) && value <= Number.MAX_SAFE_INTEGER && value >= Number.MIN_SAFE_INTEGER;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst MAX_VALUE_TYPE = '__max__';\nconst MAX_VALUE = {\n  mapValue: {\n    fields: {\n      '__type__': {\n        stringValue: MAX_VALUE_TYPE\n      }\n    }\n  }\n};\nconst MIN_VALUE = {\n  nullValue: 'NULL_VALUE'\n};\n/** Extracts the backend's type order for the provided value. */\n\nfunction typeOrder(value) {\n  if ('nullValue' in value) {\n    return 0\n    /* TypeOrder.NullValue */\n    ;\n  } else if ('booleanValue' in value) {\n    return 1\n    /* TypeOrder.BooleanValue */\n    ;\n  } else if ('integerValue' in value || 'doubleValue' in value) {\n    return 2\n    /* TypeOrder.NumberValue */\n    ;\n  } else if ('timestampValue' in value) {\n    return 3\n    /* TypeOrder.TimestampValue */\n    ;\n  } else if ('stringValue' in value) {\n    return 5\n    /* TypeOrder.StringValue */\n    ;\n  } else if ('bytesValue' in value) {\n    return 6\n    /* TypeOrder.BlobValue */\n    ;\n  } else if ('referenceValue' in value) {\n    return 7\n    /* TypeOrder.RefValue */\n    ;\n  } else if ('geoPointValue' in value) {\n    return 8\n    /* TypeOrder.GeoPointValue */\n    ;\n  } else if ('arrayValue' in value) {\n    return 9\n    /* TypeOrder.ArrayValue */\n    ;\n  } else if ('mapValue' in value) {\n    if (isServerTimestamp(value)) {\n      return 4\n      /* TypeOrder.ServerTimestampValue */\n      ;\n    } else if (isMaxValue(value)) {\n      return 9007199254740991\n      /* TypeOrder.MaxValue */\n      ;\n    }\n\n    return 10\n    /* TypeOrder.ObjectValue */\n    ;\n  } else {\n    return fail();\n  }\n}\n/** Tests `left` and `right` for equality based on the backend semantics. */\n\n\nfunction valueEquals(left, right) {\n  if (left === right) {\n    return true;\n  }\n\n  const leftType = typeOrder(left);\n  const rightType = typeOrder(right);\n\n  if (leftType !== rightType) {\n    return false;\n  }\n\n  switch (leftType) {\n    case 0\n    /* TypeOrder.NullValue */\n    :\n      return true;\n\n    case 1\n    /* TypeOrder.BooleanValue */\n    :\n      return left.booleanValue === right.booleanValue;\n\n    case 4\n    /* TypeOrder.ServerTimestampValue */\n    :\n      return getLocalWriteTime(left).isEqual(getLocalWriteTime(right));\n\n    case 3\n    /* TypeOrder.TimestampValue */\n    :\n      return timestampEquals(left, right);\n\n    case 5\n    /* TypeOrder.StringValue */\n    :\n      return left.stringValue === right.stringValue;\n\n    case 6\n    /* TypeOrder.BlobValue */\n    :\n      return blobEquals(left, right);\n\n    case 7\n    /* TypeOrder.RefValue */\n    :\n      return left.referenceValue === right.referenceValue;\n\n    case 8\n    /* TypeOrder.GeoPointValue */\n    :\n      return geoPointEquals(left, right);\n\n    case 2\n    /* TypeOrder.NumberValue */\n    :\n      return numberEquals(left, right);\n\n    case 9\n    /* TypeOrder.ArrayValue */\n    :\n      return arrayEquals(left.arrayValue.values || [], right.arrayValue.values || [], valueEquals);\n\n    case 10\n    /* TypeOrder.ObjectValue */\n    :\n      return objectEquals(left, right);\n\n    case 9007199254740991\n    /* TypeOrder.MaxValue */\n    :\n      return true;\n\n    default:\n      return fail();\n  }\n}\n\nfunction timestampEquals(left, right) {\n  if (typeof left.timestampValue === 'string' && typeof right.timestampValue === 'string' && left.timestampValue.length === right.timestampValue.length) {\n    // Use string equality for ISO 8601 timestamps\n    return left.timestampValue === right.timestampValue;\n  }\n\n  const leftTimestamp = normalizeTimestamp(left.timestampValue);\n  const rightTimestamp = normalizeTimestamp(right.timestampValue);\n  return leftTimestamp.seconds === rightTimestamp.seconds && leftTimestamp.nanos === rightTimestamp.nanos;\n}\n\nfunction geoPointEquals(left, right) {\n  return normalizeNumber(left.geoPointValue.latitude) === normalizeNumber(right.geoPointValue.latitude) && normalizeNumber(left.geoPointValue.longitude) === normalizeNumber(right.geoPointValue.longitude);\n}\n\nfunction blobEquals(left, right) {\n  return normalizeByteString(left.bytesValue).isEqual(normalizeByteString(right.bytesValue));\n}\n\nfunction numberEquals(left, right) {\n  if ('integerValue' in left && 'integerValue' in right) {\n    return normalizeNumber(left.integerValue) === normalizeNumber(right.integerValue);\n  } else if ('doubleValue' in left && 'doubleValue' in right) {\n    const n1 = normalizeNumber(left.doubleValue);\n    const n2 = normalizeNumber(right.doubleValue);\n\n    if (n1 === n2) {\n      return isNegativeZero(n1) === isNegativeZero(n2);\n    } else {\n      return isNaN(n1) && isNaN(n2);\n    }\n  }\n\n  return false;\n}\n\nfunction objectEquals(left, right) {\n  const leftMap = left.mapValue.fields || {};\n  const rightMap = right.mapValue.fields || {};\n\n  if (objectSize(leftMap) !== objectSize(rightMap)) {\n    return false;\n  }\n\n  for (const key in leftMap) {\n    if (leftMap.hasOwnProperty(key)) {\n      if (rightMap[key] === undefined || !valueEquals(leftMap[key], rightMap[key])) {\n        return false;\n      }\n    }\n  }\n\n  return true;\n}\n/** Returns true if the ArrayValue contains the specified element. */\n\n\nfunction arrayValueContains(haystack, needle) {\n  return (haystack.values || []).find(v => valueEquals(v, needle)) !== undefined;\n}\n\nfunction valueCompare(left, right) {\n  if (left === right) {\n    return 0;\n  }\n\n  const leftType = typeOrder(left);\n  const rightType = typeOrder(right);\n\n  if (leftType !== rightType) {\n    return primitiveComparator(leftType, rightType);\n  }\n\n  switch (leftType) {\n    case 0\n    /* TypeOrder.NullValue */\n    :\n    case 9007199254740991\n    /* TypeOrder.MaxValue */\n    :\n      return 0;\n\n    case 1\n    /* TypeOrder.BooleanValue */\n    :\n      return primitiveComparator(left.booleanValue, right.booleanValue);\n\n    case 2\n    /* TypeOrder.NumberValue */\n    :\n      return compareNumbers(left, right);\n\n    case 3\n    /* TypeOrder.TimestampValue */\n    :\n      return compareTimestamps(left.timestampValue, right.timestampValue);\n\n    case 4\n    /* TypeOrder.ServerTimestampValue */\n    :\n      return compareTimestamps(getLocalWriteTime(left), getLocalWriteTime(right));\n\n    case 5\n    /* TypeOrder.StringValue */\n    :\n      return primitiveComparator(left.stringValue, right.stringValue);\n\n    case 6\n    /* TypeOrder.BlobValue */\n    :\n      return compareBlobs(left.bytesValue, right.bytesValue);\n\n    case 7\n    /* TypeOrder.RefValue */\n    :\n      return compareReferences(left.referenceValue, right.referenceValue);\n\n    case 8\n    /* TypeOrder.GeoPointValue */\n    :\n      return compareGeoPoints(left.geoPointValue, right.geoPointValue);\n\n    case 9\n    /* TypeOrder.ArrayValue */\n    :\n      return compareArrays(left.arrayValue, right.arrayValue);\n\n    case 10\n    /* TypeOrder.ObjectValue */\n    :\n      return compareMaps(left.mapValue, right.mapValue);\n\n    default:\n      throw fail();\n  }\n}\n\nfunction compareNumbers(left, right) {\n  const leftNumber = normalizeNumber(left.integerValue || left.doubleValue);\n  const rightNumber = normalizeNumber(right.integerValue || right.doubleValue);\n\n  if (leftNumber < rightNumber) {\n    return -1;\n  } else if (leftNumber > rightNumber) {\n    return 1;\n  } else if (leftNumber === rightNumber) {\n    return 0;\n  } else {\n    // one or both are NaN.\n    if (isNaN(leftNumber)) {\n      return isNaN(rightNumber) ? 0 : -1;\n    } else {\n      return 1;\n    }\n  }\n}\n\nfunction compareTimestamps(left, right) {\n  if (typeof left === 'string' && typeof right === 'string' && left.length === right.length) {\n    return primitiveComparator(left, right);\n  }\n\n  const leftTimestamp = normalizeTimestamp(left);\n  const rightTimestamp = normalizeTimestamp(right);\n  const comparison = primitiveComparator(leftTimestamp.seconds, rightTimestamp.seconds);\n\n  if (comparison !== 0) {\n    return comparison;\n  }\n\n  return primitiveComparator(leftTimestamp.nanos, rightTimestamp.nanos);\n}\n\nfunction compareReferences(leftPath, rightPath) {\n  const leftSegments = leftPath.split('/');\n  const rightSegments = rightPath.split('/');\n\n  for (let i = 0; i < leftSegments.length && i < rightSegments.length; i++) {\n    const comparison = primitiveComparator(leftSegments[i], rightSegments[i]);\n\n    if (comparison !== 0) {\n      return comparison;\n    }\n  }\n\n  return primitiveComparator(leftSegments.length, rightSegments.length);\n}\n\nfunction compareGeoPoints(left, right) {\n  const comparison = primitiveComparator(normalizeNumber(left.latitude), normalizeNumber(right.latitude));\n\n  if (comparison !== 0) {\n    return comparison;\n  }\n\n  return primitiveComparator(normalizeNumber(left.longitude), normalizeNumber(right.longitude));\n}\n\nfunction compareBlobs(left, right) {\n  const leftBytes = normalizeByteString(left);\n  const rightBytes = normalizeByteString(right);\n  return leftBytes.compareTo(rightBytes);\n}\n\nfunction compareArrays(left, right) {\n  const leftArray = left.values || [];\n  const rightArray = right.values || [];\n\n  for (let i = 0; i < leftArray.length && i < rightArray.length; ++i) {\n    const compare = valueCompare(leftArray[i], rightArray[i]);\n\n    if (compare) {\n      return compare;\n    }\n  }\n\n  return primitiveComparator(leftArray.length, rightArray.length);\n}\n\nfunction compareMaps(left, right) {\n  if (left === MAX_VALUE.mapValue && right === MAX_VALUE.mapValue) {\n    return 0;\n  } else if (left === MAX_VALUE.mapValue) {\n    return 1;\n  } else if (right === MAX_VALUE.mapValue) {\n    return -1;\n  }\n\n  const leftMap = left.fields || {};\n  const leftKeys = Object.keys(leftMap);\n  const rightMap = right.fields || {};\n  const rightKeys = Object.keys(rightMap); // Even though MapValues are likely sorted correctly based on their insertion\n  // order (e.g. when received from the backend), local modifications can bring\n  // elements out of order. We need to re-sort the elements to ensure that\n  // canonical IDs are independent of insertion order.\n\n  leftKeys.sort();\n  rightKeys.sort();\n\n  for (let i = 0; i < leftKeys.length && i < rightKeys.length; ++i) {\n    const keyCompare = primitiveComparator(leftKeys[i], rightKeys[i]);\n\n    if (keyCompare !== 0) {\n      return keyCompare;\n    }\n\n    const compare = valueCompare(leftMap[leftKeys[i]], rightMap[rightKeys[i]]);\n\n    if (compare !== 0) {\n      return compare;\n    }\n  }\n\n  return primitiveComparator(leftKeys.length, rightKeys.length);\n}\n/**\r\n * Generates the canonical ID for the provided field value (as used in Target\r\n * serialization).\r\n */\n\n\nfunction canonicalId(value) {\n  return canonifyValue(value);\n}\n\nfunction canonifyValue(value) {\n  if ('nullValue' in value) {\n    return 'null';\n  } else if ('booleanValue' in value) {\n    return '' + value.booleanValue;\n  } else if ('integerValue' in value) {\n    return '' + value.integerValue;\n  } else if ('doubleValue' in value) {\n    return '' + value.doubleValue;\n  } else if ('timestampValue' in value) {\n    return canonifyTimestamp(value.timestampValue);\n  } else if ('stringValue' in value) {\n    return value.stringValue;\n  } else if ('bytesValue' in value) {\n    return canonifyByteString(value.bytesValue);\n  } else if ('referenceValue' in value) {\n    return canonifyReference(value.referenceValue);\n  } else if ('geoPointValue' in value) {\n    return canonifyGeoPoint(value.geoPointValue);\n  } else if ('arrayValue' in value) {\n    return canonifyArray(value.arrayValue);\n  } else if ('mapValue' in value) {\n    return canonifyMap(value.mapValue);\n  } else {\n    return fail();\n  }\n}\n\nfunction canonifyByteString(byteString) {\n  return normalizeByteString(byteString).toBase64();\n}\n\nfunction canonifyTimestamp(timestamp) {\n  const normalizedTimestamp = normalizeTimestamp(timestamp);\n  return `time(${normalizedTimestamp.seconds},${normalizedTimestamp.nanos})`;\n}\n\nfunction canonifyGeoPoint(geoPoint) {\n  return `geo(${geoPoint.latitude},${geoPoint.longitude})`;\n}\n\nfunction canonifyReference(referenceValue) {\n  return DocumentKey.fromName(referenceValue).toString();\n}\n\nfunction canonifyMap(mapValue) {\n  // Iteration order in JavaScript is not guaranteed. To ensure that we generate\n  // matching canonical IDs for identical maps, we need to sort the keys.\n  const sortedKeys = Object.keys(mapValue.fields || {}).sort();\n  let result = '{';\n  let first = true;\n\n  for (const key of sortedKeys) {\n    if (!first) {\n      result += ',';\n    } else {\n      first = false;\n    }\n\n    result += `${key}:${canonifyValue(mapValue.fields[key])}`;\n  }\n\n  return result + '}';\n}\n\nfunction canonifyArray(arrayValue) {\n  let result = '[';\n  let first = true;\n\n  for (const value of arrayValue.values || []) {\n    if (!first) {\n      result += ',';\n    } else {\n      first = false;\n    }\n\n    result += canonifyValue(value);\n  }\n\n  return result + ']';\n}\n/**\r\n * Returns an approximate (and wildly inaccurate) in-memory size for the field\r\n * value.\r\n *\r\n * The memory size takes into account only the actual user data as it resides\r\n * in memory and ignores object overhead.\r\n */\n\n\nfunction estimateByteSize(value) {\n  switch (typeOrder(value)) {\n    case 0\n    /* TypeOrder.NullValue */\n    :\n      return 4;\n\n    case 1\n    /* TypeOrder.BooleanValue */\n    :\n      return 4;\n\n    case 2\n    /* TypeOrder.NumberValue */\n    :\n      return 8;\n\n    case 3\n    /* TypeOrder.TimestampValue */\n    :\n      // Timestamps are made up of two distinct numbers (seconds + nanoseconds)\n      return 16;\n\n    case 4\n    /* TypeOrder.ServerTimestampValue */\n    :\n      const previousValue = getPreviousValue(value);\n      return previousValue ? 16 + estimateByteSize(previousValue) : 16;\n\n    case 5\n    /* TypeOrder.StringValue */\n    :\n      // See https://developer.mozilla.org/en-US/docs/Web/JavaScript/Data_structures:\n      // \"JavaScript's String type is [...] a set of elements of 16-bit unsigned\n      // integer values\"\n      return value.stringValue.length * 2;\n\n    case 6\n    /* TypeOrder.BlobValue */\n    :\n      return normalizeByteString(value.bytesValue).approximateByteSize();\n\n    case 7\n    /* TypeOrder.RefValue */\n    :\n      return value.referenceValue.length;\n\n    case 8\n    /* TypeOrder.GeoPointValue */\n    :\n      // GeoPoints are made up of two distinct numbers (latitude + longitude)\n      return 16;\n\n    case 9\n    /* TypeOrder.ArrayValue */\n    :\n      return estimateArrayByteSize(value.arrayValue);\n\n    case 10\n    /* TypeOrder.ObjectValue */\n    :\n      return estimateMapByteSize(value.mapValue);\n\n    default:\n      throw fail();\n  }\n}\n\nfunction estimateMapByteSize(mapValue) {\n  let size = 0;\n  forEach(mapValue.fields, (key, val) => {\n    size += key.length + estimateByteSize(val);\n  });\n  return size;\n}\n\nfunction estimateArrayByteSize(arrayValue) {\n  return (arrayValue.values || []).reduce((previousSize, value) => previousSize + estimateByteSize(value), 0);\n}\n/** Returns a reference value for the provided database and key. */\n\n\nfunction refValue(databaseId, key) {\n  return {\n    referenceValue: `projects/${databaseId.projectId}/databases/${databaseId.database}/documents/${key.path.canonicalString()}`\n  };\n}\n/** Returns true if `value` is an IntegerValue . */\n\n\nfunction isInteger(value) {\n  return !!value && 'integerValue' in value;\n}\n/** Returns true if `value` is a DoubleValue. */\n\n\nfunction isDouble(value) {\n  return !!value && 'doubleValue' in value;\n}\n/** Returns true if `value` is either an IntegerValue or a DoubleValue. */\n\n\nfunction isNumber(value) {\n  return isInteger(value) || isDouble(value);\n}\n/** Returns true if `value` is an ArrayValue. */\n\n\nfunction isArray(value) {\n  return !!value && 'arrayValue' in value;\n}\n/** Returns true if `value` is a NullValue. */\n\n\nfunction isNullValue(value) {\n  return !!value && 'nullValue' in value;\n}\n/** Returns true if `value` is NaN. */\n\n\nfunction isNanValue(value) {\n  return !!value && 'doubleValue' in value && isNaN(Number(value.doubleValue));\n}\n/** Returns true if `value` is a MapValue. */\n\n\nfunction isMapValue(value) {\n  return !!value && 'mapValue' in value;\n}\n/** Creates a deep copy of `source`. */\n\n\nfunction deepClone(source) {\n  if (source.geoPointValue) {\n    return {\n      geoPointValue: Object.assign({}, source.geoPointValue)\n    };\n  } else if (source.timestampValue && typeof source.timestampValue === 'object') {\n    return {\n      timestampValue: Object.assign({}, source.timestampValue)\n    };\n  } else if (source.mapValue) {\n    const target = {\n      mapValue: {\n        fields: {}\n      }\n    };\n    forEach(source.mapValue.fields, (key, val) => target.mapValue.fields[key] = deepClone(val));\n    return target;\n  } else if (source.arrayValue) {\n    const target = {\n      arrayValue: {\n        values: []\n      }\n    };\n\n    for (let i = 0; i < (source.arrayValue.values || []).length; ++i) {\n      target.arrayValue.values[i] = deepClone(source.arrayValue.values[i]);\n    }\n\n    return target;\n  } else {\n    return Object.assign({}, source);\n  }\n}\n/** Returns true if the Value represents the canonical {@link #MAX_VALUE} . */\n\n\nfunction isMaxValue(value) {\n  return (((value.mapValue || {}).fields || {})['__type__'] || {}).stringValue === MAX_VALUE_TYPE;\n}\n/** Returns the lowest value for the given value type (inclusive). */\n\n\nfunction valuesGetLowerBound(value) {\n  if ('nullValue' in value) {\n    return MIN_VALUE;\n  } else if ('booleanValue' in value) {\n    return {\n      booleanValue: false\n    };\n  } else if ('integerValue' in value || 'doubleValue' in value) {\n    return {\n      doubleValue: NaN\n    };\n  } else if ('timestampValue' in value) {\n    return {\n      timestampValue: {\n        seconds: Number.MIN_SAFE_INTEGER\n      }\n    };\n  } else if ('stringValue' in value) {\n    return {\n      stringValue: ''\n    };\n  } else if ('bytesValue' in value) {\n    return {\n      bytesValue: ''\n    };\n  } else if ('referenceValue' in value) {\n    return refValue(DatabaseId.empty(), DocumentKey.empty());\n  } else if ('geoPointValue' in value) {\n    return {\n      geoPointValue: {\n        latitude: -90,\n        longitude: -180\n      }\n    };\n  } else if ('arrayValue' in value) {\n    return {\n      arrayValue: {}\n    };\n  } else if ('mapValue' in value) {\n    return {\n      mapValue: {}\n    };\n  } else {\n    return fail();\n  }\n}\n/** Returns the largest value for the given value type (exclusive). */\n\n\nfunction valuesGetUpperBound(value) {\n  if ('nullValue' in value) {\n    return {\n      booleanValue: false\n    };\n  } else if ('booleanValue' in value) {\n    return {\n      doubleValue: NaN\n    };\n  } else if ('integerValue' in value || 'doubleValue' in value) {\n    return {\n      timestampValue: {\n        seconds: Number.MIN_SAFE_INTEGER\n      }\n    };\n  } else if ('timestampValue' in value) {\n    return {\n      stringValue: ''\n    };\n  } else if ('stringValue' in value) {\n    return {\n      bytesValue: ''\n    };\n  } else if ('bytesValue' in value) {\n    return refValue(DatabaseId.empty(), DocumentKey.empty());\n  } else if ('referenceValue' in value) {\n    return {\n      geoPointValue: {\n        latitude: -90,\n        longitude: -180\n      }\n    };\n  } else if ('geoPointValue' in value) {\n    return {\n      arrayValue: {}\n    };\n  } else if ('arrayValue' in value) {\n    return {\n      mapValue: {}\n    };\n  } else if ('mapValue' in value) {\n    return MAX_VALUE;\n  } else {\n    return fail();\n  }\n}\n\nfunction lowerBoundCompare(left, right) {\n  const cmp = valueCompare(left.value, right.value);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  if (left.inclusive && !right.inclusive) {\n    return -1;\n  } else if (!left.inclusive && right.inclusive) {\n    return 1;\n  }\n\n  return 0;\n}\n\nfunction upperBoundCompare(left, right) {\n  const cmp = valueCompare(left.value, right.value);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  if (left.inclusive && !right.inclusive) {\n    return 1;\n  } else if (!left.inclusive && right.inclusive) {\n    return -1;\n  }\n\n  return 0;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An ObjectValue represents a MapValue in the Firestore Proto and offers the\r\n * ability to add and remove fields (via the ObjectValueBuilder).\r\n */\n\n\nclass ObjectValue {\n  constructor(value) {\n    this.value = value;\n  }\n\n  static empty() {\n    return new ObjectValue({\n      mapValue: {}\n    });\n  }\n  /**\r\n   * Returns the value at the given path or null.\r\n   *\r\n   * @param path - the path to search\r\n   * @returns The value at the path or null if the path is not set.\r\n   */\n\n\n  field(path) {\n    if (path.isEmpty()) {\n      return this.value;\n    } else {\n      let currentLevel = this.value;\n\n      for (let i = 0; i < path.length - 1; ++i) {\n        currentLevel = (currentLevel.mapValue.fields || {})[path.get(i)];\n\n        if (!isMapValue(currentLevel)) {\n          return null;\n        }\n      }\n\n      currentLevel = (currentLevel.mapValue.fields || {})[path.lastSegment()];\n      return currentLevel || null;\n    }\n  }\n  /**\r\n   * Sets the field to the provided value.\r\n   *\r\n   * @param path - The field path to set.\r\n   * @param value - The value to set.\r\n   */\n\n\n  set(path, value) {\n    const fieldsMap = this.getFieldsMap(path.popLast());\n    fieldsMap[path.lastSegment()] = deepClone(value);\n  }\n  /**\r\n   * Sets the provided fields to the provided values.\r\n   *\r\n   * @param data - A map of fields to values (or null for deletes).\r\n   */\n\n\n  setAll(data) {\n    let parent = FieldPath$1.emptyPath();\n    let upserts = {};\n    let deletes = [];\n    data.forEach((value, path) => {\n      if (!parent.isImmediateParentOf(path)) {\n        // Insert the accumulated changes at this parent location\n        const fieldsMap = this.getFieldsMap(parent);\n        this.applyChanges(fieldsMap, upserts, deletes);\n        upserts = {};\n        deletes = [];\n        parent = path.popLast();\n      }\n\n      if (value) {\n        upserts[path.lastSegment()] = deepClone(value);\n      } else {\n        deletes.push(path.lastSegment());\n      }\n    });\n    const fieldsMap = this.getFieldsMap(parent);\n    this.applyChanges(fieldsMap, upserts, deletes);\n  }\n  /**\r\n   * Removes the field at the specified path. If there is no field at the\r\n   * specified path, nothing is changed.\r\n   *\r\n   * @param path - The field path to remove.\r\n   */\n\n\n  delete(path) {\n    const nestedValue = this.field(path.popLast());\n\n    if (isMapValue(nestedValue) && nestedValue.mapValue.fields) {\n      delete nestedValue.mapValue.fields[path.lastSegment()];\n    }\n  }\n\n  isEqual(other) {\n    return valueEquals(this.value, other.value);\n  }\n  /**\r\n   * Returns the map that contains the leaf element of `path`. If the parent\r\n   * entry does not yet exist, or if it is not a map, a new map will be created.\r\n   */\n\n\n  getFieldsMap(path) {\n    let current = this.value;\n\n    if (!current.mapValue.fields) {\n      current.mapValue = {\n        fields: {}\n      };\n    }\n\n    for (let i = 0; i < path.length; ++i) {\n      let next = current.mapValue.fields[path.get(i)];\n\n      if (!isMapValue(next) || !next.mapValue.fields) {\n        next = {\n          mapValue: {\n            fields: {}\n          }\n        };\n        current.mapValue.fields[path.get(i)] = next;\n      }\n\n      current = next;\n    }\n\n    return current.mapValue.fields;\n  }\n  /**\r\n   * Modifies `fieldsMap` by adding, replacing or deleting the specified\r\n   * entries.\r\n   */\n\n\n  applyChanges(fieldsMap, inserts, deletes) {\n    forEach(inserts, (key, val) => fieldsMap[key] = val);\n\n    for (const field of deletes) {\n      delete fieldsMap[field];\n    }\n  }\n\n  clone() {\n    return new ObjectValue(deepClone(this.value));\n  }\n\n}\n/**\r\n * Returns a FieldMask built from all fields in a MapValue.\r\n */\n\n\nfunction extractFieldMask(value) {\n  const fields = [];\n  forEach(value.fields, (key, value) => {\n    const currentPath = new FieldPath$1([key]);\n\n    if (isMapValue(value)) {\n      const nestedMask = extractFieldMask(value.mapValue);\n      const nestedFields = nestedMask.fields;\n\n      if (nestedFields.length === 0) {\n        // Preserve the empty map by adding it to the FieldMask.\n        fields.push(currentPath);\n      } else {\n        // For nested and non-empty ObjectValues, add the FieldPath of the\n        // leaf nodes.\n        for (const nestedPath of nestedFields) {\n          fields.push(currentPath.child(nestedPath));\n        }\n      }\n    } else {\n      // For nested and non-empty ObjectValues, add the FieldPath of the leaf\n      // nodes.\n      fields.push(currentPath);\n    }\n  });\n  return new FieldMask(fields);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents a document in Firestore with a key, version, data and whether it\r\n * has local mutations applied to it.\r\n *\r\n * Documents can transition between states via `convertToFoundDocument()`,\r\n * `convertToNoDocument()` and `convertToUnknownDocument()`. If a document does\r\n * not transition to one of these states even after all mutations have been\r\n * applied, `isValidDocument()` returns false and the document should be removed\r\n * from all views.\r\n */\n\n\nclass MutableDocument {\n  constructor(key, documentType, version, readTime, createTime, data, documentState) {\n    this.key = key;\n    this.documentType = documentType;\n    this.version = version;\n    this.readTime = readTime;\n    this.createTime = createTime;\n    this.data = data;\n    this.documentState = documentState;\n  }\n  /**\r\n   * Creates a document with no known version or data, but which can serve as\r\n   * base document for mutations.\r\n   */\n\n\n  static newInvalidDocument(documentKey) {\n    return new MutableDocument(documentKey, 0\n    /* DocumentType.INVALID */\n    ,\n    /* version */\n    SnapshotVersion.min(),\n    /* readTime */\n    SnapshotVersion.min(),\n    /* createTime */\n    SnapshotVersion.min(), ObjectValue.empty(), 0\n    /* DocumentState.SYNCED */\n    );\n  }\n  /**\r\n   * Creates a new document that is known to exist with the given data at the\r\n   * given version.\r\n   */\n\n\n  static newFoundDocument(documentKey, version, createTime, value) {\n    return new MutableDocument(documentKey, 1\n    /* DocumentType.FOUND_DOCUMENT */\n    ,\n    /* version */\n    version,\n    /* readTime */\n    SnapshotVersion.min(),\n    /* createTime */\n    createTime, value, 0\n    /* DocumentState.SYNCED */\n    );\n  }\n  /** Creates a new document that is known to not exist at the given version. */\n\n\n  static newNoDocument(documentKey, version) {\n    return new MutableDocument(documentKey, 2\n    /* DocumentType.NO_DOCUMENT */\n    ,\n    /* version */\n    version,\n    /* readTime */\n    SnapshotVersion.min(),\n    /* createTime */\n    SnapshotVersion.min(), ObjectValue.empty(), 0\n    /* DocumentState.SYNCED */\n    );\n  }\n  /**\r\n   * Creates a new document that is known to exist at the given version but\r\n   * whose data is not known (e.g. a document that was updated without a known\r\n   * base document).\r\n   */\n\n\n  static newUnknownDocument(documentKey, version) {\n    return new MutableDocument(documentKey, 3\n    /* DocumentType.UNKNOWN_DOCUMENT */\n    ,\n    /* version */\n    version,\n    /* readTime */\n    SnapshotVersion.min(),\n    /* createTime */\n    SnapshotVersion.min(), ObjectValue.empty(), 2\n    /* DocumentState.HAS_COMMITTED_MUTATIONS */\n    );\n  }\n  /**\r\n   * Changes the document type to indicate that it exists and that its version\r\n   * and data are known.\r\n   */\n\n\n  convertToFoundDocument(version, value) {\n    // If a document is switching state from being an invalid or deleted\n    // document to a valid (FOUND_DOCUMENT) document, either due to receiving an\n    // update from Watch or due to applying a local set mutation on top\n    // of a deleted document, our best guess about its createTime would be the\n    // version at which the document transitioned to a FOUND_DOCUMENT.\n    if (this.createTime.isEqual(SnapshotVersion.min()) && (this.documentType === 2\n    /* DocumentType.NO_DOCUMENT */\n    || this.documentType === 0\n    /* DocumentType.INVALID */\n    )) {\n      this.createTime = version;\n    }\n\n    this.version = version;\n    this.documentType = 1\n    /* DocumentType.FOUND_DOCUMENT */\n    ;\n    this.data = value;\n    this.documentState = 0\n    /* DocumentState.SYNCED */\n    ;\n    return this;\n  }\n  /**\r\n   * Changes the document type to indicate that it doesn't exist at the given\r\n   * version.\r\n   */\n\n\n  convertToNoDocument(version) {\n    this.version = version;\n    this.documentType = 2\n    /* DocumentType.NO_DOCUMENT */\n    ;\n    this.data = ObjectValue.empty();\n    this.documentState = 0\n    /* DocumentState.SYNCED */\n    ;\n    return this;\n  }\n  /**\r\n   * Changes the document type to indicate that it exists at a given version but\r\n   * that its data is not known (e.g. a document that was updated without a known\r\n   * base document).\r\n   */\n\n\n  convertToUnknownDocument(version) {\n    this.version = version;\n    this.documentType = 3\n    /* DocumentType.UNKNOWN_DOCUMENT */\n    ;\n    this.data = ObjectValue.empty();\n    this.documentState = 2\n    /* DocumentState.HAS_COMMITTED_MUTATIONS */\n    ;\n    return this;\n  }\n\n  setHasCommittedMutations() {\n    this.documentState = 2\n    /* DocumentState.HAS_COMMITTED_MUTATIONS */\n    ;\n    return this;\n  }\n\n  setHasLocalMutations() {\n    this.documentState = 1\n    /* DocumentState.HAS_LOCAL_MUTATIONS */\n    ;\n    this.version = SnapshotVersion.min();\n    return this;\n  }\n\n  setReadTime(readTime) {\n    this.readTime = readTime;\n    return this;\n  }\n\n  get hasLocalMutations() {\n    return this.documentState === 1\n    /* DocumentState.HAS_LOCAL_MUTATIONS */\n    ;\n  }\n\n  get hasCommittedMutations() {\n    return this.documentState === 2\n    /* DocumentState.HAS_COMMITTED_MUTATIONS */\n    ;\n  }\n\n  get hasPendingWrites() {\n    return this.hasLocalMutations || this.hasCommittedMutations;\n  }\n\n  isValidDocument() {\n    return this.documentType !== 0\n    /* DocumentType.INVALID */\n    ;\n  }\n\n  isFoundDocument() {\n    return this.documentType === 1\n    /* DocumentType.FOUND_DOCUMENT */\n    ;\n  }\n\n  isNoDocument() {\n    return this.documentType === 2\n    /* DocumentType.NO_DOCUMENT */\n    ;\n  }\n\n  isUnknownDocument() {\n    return this.documentType === 3\n    /* DocumentType.UNKNOWN_DOCUMENT */\n    ;\n  }\n\n  isEqual(other) {\n    return other instanceof MutableDocument && this.key.isEqual(other.key) && this.version.isEqual(other.version) && this.documentType === other.documentType && this.documentState === other.documentState && this.data.isEqual(other.data);\n  }\n\n  mutableCopy() {\n    return new MutableDocument(this.key, this.documentType, this.version, this.readTime, this.createTime, this.data.clone(), this.documentState);\n  }\n\n  toString() {\n    return `Document(${this.key}, ${this.version}, ${JSON.stringify(this.data.value)}, ` + `{createTime: ${this.createTime}}), ` + `{documentType: ${this.documentType}}), ` + `{documentState: ${this.documentState}})`;\n  }\n\n}\n/**\r\n * Compares the value for field `field` in the provided documents. Throws if\r\n * the field does not exist in both documents.\r\n */\n\n\nfunction compareDocumentsByField(field, d1, d2) {\n  const v1 = d1.data.field(field);\n  const v2 = d2.data.field(field);\n\n  if (v1 !== null && v2 !== null) {\n    return valueCompare(v1, v2);\n  } else {\n    return fail();\n  }\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents a bound of a query.\r\n *\r\n * The bound is specified with the given components representing a position and\r\n * whether it's just before or just after the position (relative to whatever the\r\n * query order is).\r\n *\r\n * The position represents a logical index position for a query. It's a prefix\r\n * of values for the (potentially implicit) order by clauses of a query.\r\n *\r\n * Bound provides a function to determine whether a document comes before or\r\n * after a bound. This is influenced by whether the position is just before or\r\n * just after the provided values.\r\n */\n\n\nclass Bound {\n  constructor(position, inclusive) {\n    this.position = position;\n    this.inclusive = inclusive;\n  }\n\n}\n\nfunction boundCompareToDocument(bound, orderBy, doc) {\n  let comparison = 0;\n\n  for (let i = 0; i < bound.position.length; i++) {\n    const orderByComponent = orderBy[i];\n    const component = bound.position[i];\n\n    if (orderByComponent.field.isKeyField()) {\n      comparison = DocumentKey.comparator(DocumentKey.fromName(component.referenceValue), doc.key);\n    } else {\n      const docValue = doc.data.field(orderByComponent.field);\n      comparison = valueCompare(component, docValue);\n    }\n\n    if (orderByComponent.dir === \"desc\"\n    /* Direction.DESCENDING */\n    ) {\n      comparison = comparison * -1;\n    }\n\n    if (comparison !== 0) {\n      break;\n    }\n  }\n\n  return comparison;\n}\n/**\r\n * Returns true if a document sorts after a bound using the provided sort\r\n * order.\r\n */\n\n\nfunction boundSortsAfterDocument(bound, orderBy, doc) {\n  const comparison = boundCompareToDocument(bound, orderBy, doc);\n  return bound.inclusive ? comparison >= 0 : comparison > 0;\n}\n/**\r\n * Returns true if a document sorts before a bound using the provided sort\r\n * order.\r\n */\n\n\nfunction boundSortsBeforeDocument(bound, orderBy, doc) {\n  const comparison = boundCompareToDocument(bound, orderBy, doc);\n  return bound.inclusive ? comparison <= 0 : comparison < 0;\n}\n\nfunction boundEquals(left, right) {\n  if (left === null) {\n    return right === null;\n  } else if (right === null) {\n    return false;\n  }\n\n  if (left.inclusive !== right.inclusive || left.position.length !== right.position.length) {\n    return false;\n  }\n\n  for (let i = 0; i < left.position.length; i++) {\n    const leftPosition = left.position[i];\n    const rightPosition = right.position[i];\n\n    if (!valueEquals(leftPosition, rightPosition)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An ordering on a field, in some Direction. Direction defaults to ASCENDING.\r\n */\n\n\nclass OrderBy {\n  constructor(field, dir = \"asc\"\n  /* Direction.ASCENDING */\n  ) {\n    this.field = field;\n    this.dir = dir;\n  }\n\n}\n\nfunction canonifyOrderBy(orderBy) {\n  // TODO(b/29183165): Make this collision robust.\n  return orderBy.field.canonicalString() + orderBy.dir;\n}\n\nfunction stringifyOrderBy(orderBy) {\n  return `${orderBy.field.canonicalString()} (${orderBy.dir})`;\n}\n\nfunction orderByEquals(left, right) {\n  return left.dir === right.dir && left.field.isEqual(right.field);\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass Filter {}\n\nclass FieldFilter extends Filter {\n  constructor(field, op, value) {\n    super();\n    this.field = field;\n    this.op = op;\n    this.value = value;\n  }\n  /**\r\n   * Creates a filter based on the provided arguments.\r\n   */\n\n\n  static create(field, op, value) {\n    if (field.isKeyField()) {\n      if (op === \"in\"\n      /* Operator.IN */\n      || op === \"not-in\"\n      /* Operator.NOT_IN */\n      ) {\n        return this.createKeyFieldInFilter(field, op, value);\n      } else {\n        return new KeyFieldFilter(field, op, value);\n      }\n    } else if (op === \"array-contains\"\n    /* Operator.ARRAY_CONTAINS */\n    ) {\n      return new ArrayContainsFilter(field, value);\n    } else if (op === \"in\"\n    /* Operator.IN */\n    ) {\n      return new InFilter(field, value);\n    } else if (op === \"not-in\"\n    /* Operator.NOT_IN */\n    ) {\n      return new NotInFilter(field, value);\n    } else if (op === \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    ) {\n      return new ArrayContainsAnyFilter(field, value);\n    } else {\n      return new FieldFilter(field, op, value);\n    }\n  }\n\n  static createKeyFieldInFilter(field, op, value) {\n    return op === \"in\"\n    /* Operator.IN */\n    ? new KeyFieldInFilter(field, value) : new KeyFieldNotInFilter(field, value);\n  }\n\n  matches(doc) {\n    const other = doc.data.field(this.field); // Types do not have to match in NOT_EQUAL filters.\n\n    if (this.op === \"!=\"\n    /* Operator.NOT_EQUAL */\n    ) {\n      return other !== null && this.matchesComparison(valueCompare(other, this.value));\n    } // Only compare types with matching backend order (such as double and int).\n\n\n    return other !== null && typeOrder(this.value) === typeOrder(other) && this.matchesComparison(valueCompare(other, this.value));\n  }\n\n  matchesComparison(comparison) {\n    switch (this.op) {\n      case \"<\"\n      /* Operator.LESS_THAN */\n      :\n        return comparison < 0;\n\n      case \"<=\"\n      /* Operator.LESS_THAN_OR_EQUAL */\n      :\n        return comparison <= 0;\n\n      case \"==\"\n      /* Operator.EQUAL */\n      :\n        return comparison === 0;\n\n      case \"!=\"\n      /* Operator.NOT_EQUAL */\n      :\n        return comparison !== 0;\n\n      case \">\"\n      /* Operator.GREATER_THAN */\n      :\n        return comparison > 0;\n\n      case \">=\"\n      /* Operator.GREATER_THAN_OR_EQUAL */\n      :\n        return comparison >= 0;\n\n      default:\n        return fail();\n    }\n  }\n\n  isInequality() {\n    return [\"<\"\n    /* Operator.LESS_THAN */\n    , \"<=\"\n    /* Operator.LESS_THAN_OR_EQUAL */\n    , \">\"\n    /* Operator.GREATER_THAN */\n    , \">=\"\n    /* Operator.GREATER_THAN_OR_EQUAL */\n    , \"!=\"\n    /* Operator.NOT_EQUAL */\n    , \"not-in\"\n    /* Operator.NOT_IN */\n    ].indexOf(this.op) >= 0;\n  }\n\n  getFlattenedFilters() {\n    return [this];\n  }\n\n  getFilters() {\n    return [this];\n  }\n\n  getFirstInequalityField() {\n    if (this.isInequality()) {\n      return this.field;\n    }\n\n    return null;\n  }\n\n}\n\nclass CompositeFilter extends Filter {\n  constructor(filters, op) {\n    super();\n    this.filters = filters;\n    this.op = op;\n    this.memoizedFlattenedFilters = null;\n  }\n  /**\r\n   * Creates a filter based on the provided arguments.\r\n   */\n\n\n  static create(filters, op) {\n    return new CompositeFilter(filters, op);\n  }\n\n  matches(doc) {\n    if (compositeFilterIsConjunction(this)) {\n      // For conjunctions, all filters must match, so return false if any filter doesn't match.\n      return this.filters.find(filter => !filter.matches(doc)) === undefined;\n    } else {\n      // For disjunctions, at least one filter should match.\n      return this.filters.find(filter => filter.matches(doc)) !== undefined;\n    }\n  }\n\n  getFlattenedFilters() {\n    if (this.memoizedFlattenedFilters !== null) {\n      return this.memoizedFlattenedFilters;\n    }\n\n    this.memoizedFlattenedFilters = this.filters.reduce((result, subfilter) => {\n      return result.concat(subfilter.getFlattenedFilters());\n    }, []);\n    return this.memoizedFlattenedFilters;\n  } // Returns a mutable copy of `this.filters`\n\n\n  getFilters() {\n    return Object.assign([], this.filters);\n  }\n\n  getFirstInequalityField() {\n    const found = this.findFirstMatchingFilter(filter => filter.isInequality());\n\n    if (found !== null) {\n      return found.field;\n    }\n\n    return null;\n  } // Performs a depth-first search to find and return the first FieldFilter in the composite filter\n  // that satisfies the predicate. Returns `null` if none of the FieldFilters satisfy the\n  // predicate.\n\n\n  findFirstMatchingFilter(predicate) {\n    for (const fieldFilter of this.getFlattenedFilters()) {\n      if (predicate(fieldFilter)) {\n        return fieldFilter;\n      }\n    }\n\n    return null;\n  }\n\n}\n\nfunction compositeFilterIsConjunction(compositeFilter) {\n  return compositeFilter.op === \"and\"\n  /* CompositeOperator.AND */\n  ;\n}\n\nfunction compositeFilterIsDisjunction(compositeFilter) {\n  return compositeFilter.op === \"or\"\n  /* CompositeOperator.OR */\n  ;\n}\n/**\r\n * Returns true if this filter is a conjunction of field filters only. Returns false otherwise.\r\n */\n\n\nfunction compositeFilterIsFlatConjunction(compositeFilter) {\n  return compositeFilterIsFlat(compositeFilter) && compositeFilterIsConjunction(compositeFilter);\n}\n/**\r\n * Returns true if this filter does not contain any composite filters. Returns false otherwise.\r\n */\n\n\nfunction compositeFilterIsFlat(compositeFilter) {\n  for (const filter of compositeFilter.filters) {\n    if (filter instanceof CompositeFilter) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\nfunction canonifyFilter(filter) {\n  if (filter instanceof FieldFilter) {\n    // TODO(b/29183165): Technically, this won't be unique if two values have\n    // the same description, such as the int 3 and the string \"3\". So we should\n    // add the types in here somehow, too.\n    return filter.field.canonicalString() + filter.op.toString() + canonicalId(filter.value);\n  } else if (compositeFilterIsFlatConjunction(filter)) {\n    // Older SDK versions use an implicit AND operation between their filters.\n    // In the new SDK versions, the developer may use an explicit AND filter.\n    // To stay consistent with the old usages, we add a special case to ensure\n    // the canonical ID for these two are the same. For example:\n    // `col.whereEquals(\"a\", 1).whereEquals(\"b\", 2)` should have the same\n    // canonical ID as `col.where(and(equals(\"a\",1), equals(\"b\",2)))`.\n    return filter.filters.map(filter => canonifyFilter(filter)).join(',');\n  } else {\n    // filter instanceof CompositeFilter\n    const canonicalIdsString = filter.filters.map(filter => canonifyFilter(filter)).join(',');\n    return `${filter.op}(${canonicalIdsString})`;\n  }\n}\n\nfunction filterEquals(f1, f2) {\n  if (f1 instanceof FieldFilter) {\n    return fieldFilterEquals(f1, f2);\n  } else if (f1 instanceof CompositeFilter) {\n    return compositeFilterEquals(f1, f2);\n  } else {\n    fail();\n  }\n}\n\nfunction fieldFilterEquals(f1, f2) {\n  return f2 instanceof FieldFilter && f1.op === f2.op && f1.field.isEqual(f2.field) && valueEquals(f1.value, f2.value);\n}\n\nfunction compositeFilterEquals(f1, f2) {\n  if (f2 instanceof CompositeFilter && f1.op === f2.op && f1.filters.length === f2.filters.length) {\n    const subFiltersMatch = f1.filters.reduce((result, f1Filter, index) => result && filterEquals(f1Filter, f2.filters[index]), true);\n    return subFiltersMatch;\n  }\n\n  return false;\n}\n/**\r\n * Returns a new composite filter that contains all filter from\r\n * `compositeFilter` plus all the given filters in `otherFilters`.\r\n */\n\n\nfunction compositeFilterWithAddedFilters(compositeFilter, otherFilters) {\n  const mergedFilters = compositeFilter.filters.concat(otherFilters);\n  return CompositeFilter.create(mergedFilters, compositeFilter.op);\n}\n/** Returns a debug description for `filter`. */\n\n\nfunction stringifyFilter(filter) {\n  if (filter instanceof FieldFilter) {\n    return stringifyFieldFilter(filter);\n  } else if (filter instanceof CompositeFilter) {\n    return stringifyCompositeFilter(filter);\n  } else {\n    return 'Filter';\n  }\n}\n\nfunction stringifyCompositeFilter(filter) {\n  return filter.op.toString() + ` {` + filter.getFilters().map(stringifyFilter).join(' ,') + '}';\n}\n\nfunction stringifyFieldFilter(filter) {\n  return `${filter.field.canonicalString()} ${filter.op} ${canonicalId(filter.value)}`;\n}\n/** Filter that matches on key fields (i.e. '__name__'). */\n\n\nclass KeyFieldFilter extends FieldFilter {\n  constructor(field, op, value) {\n    super(field, op, value);\n    this.key = DocumentKey.fromName(value.referenceValue);\n  }\n\n  matches(doc) {\n    const comparison = DocumentKey.comparator(doc.key, this.key);\n    return this.matchesComparison(comparison);\n  }\n\n}\n/** Filter that matches on key fields within an array. */\n\n\nclass KeyFieldInFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"in\"\n    /* Operator.IN */\n    , value);\n    this.keys = extractDocumentKeysFromArrayValue(\"in\"\n    /* Operator.IN */\n    , value);\n  }\n\n  matches(doc) {\n    return this.keys.some(key => key.isEqual(doc.key));\n  }\n\n}\n/** Filter that matches on key fields not present within an array. */\n\n\nclass KeyFieldNotInFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"not-in\"\n    /* Operator.NOT_IN */\n    , value);\n    this.keys = extractDocumentKeysFromArrayValue(\"not-in\"\n    /* Operator.NOT_IN */\n    , value);\n  }\n\n  matches(doc) {\n    return !this.keys.some(key => key.isEqual(doc.key));\n  }\n\n}\n\nfunction extractDocumentKeysFromArrayValue(op, value) {\n  var _a;\n\n  return (((_a = value.arrayValue) === null || _a === void 0 ? void 0 : _a.values) || []).map(v => {\n    return DocumentKey.fromName(v.referenceValue);\n  });\n}\n/** A Filter that implements the array-contains operator. */\n\n\nclass ArrayContainsFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"array-contains\"\n    /* Operator.ARRAY_CONTAINS */\n    , value);\n  }\n\n  matches(doc) {\n    const other = doc.data.field(this.field);\n    return isArray(other) && arrayValueContains(other.arrayValue, this.value);\n  }\n\n}\n/** A Filter that implements the IN operator. */\n\n\nclass InFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"in\"\n    /* Operator.IN */\n    , value);\n  }\n\n  matches(doc) {\n    const other = doc.data.field(this.field);\n    return other !== null && arrayValueContains(this.value.arrayValue, other);\n  }\n\n}\n/** A Filter that implements the not-in operator. */\n\n\nclass NotInFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"not-in\"\n    /* Operator.NOT_IN */\n    , value);\n  }\n\n  matches(doc) {\n    if (arrayValueContains(this.value.arrayValue, {\n      nullValue: 'NULL_VALUE'\n    })) {\n      return false;\n    }\n\n    const other = doc.data.field(this.field);\n    return other !== null && !arrayValueContains(this.value.arrayValue, other);\n  }\n\n}\n/** A Filter that implements the array-contains-any operator. */\n\n\nclass ArrayContainsAnyFilter extends FieldFilter {\n  constructor(field, value) {\n    super(field, \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    , value);\n  }\n\n  matches(doc) {\n    const other = doc.data.field(this.field);\n\n    if (!isArray(other) || !other.arrayValue.values) {\n      return false;\n    }\n\n    return other.arrayValue.values.some(val => arrayValueContains(this.value.arrayValue, val));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// Visible for testing\n\n\nclass TargetImpl {\n  constructor(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\n    this.path = path;\n    this.collectionGroup = collectionGroup;\n    this.orderBy = orderBy;\n    this.filters = filters;\n    this.limit = limit;\n    this.startAt = startAt;\n    this.endAt = endAt;\n    this.memoizedCanonicalId = null;\n  }\n\n}\n/**\r\n * Initializes a Target with a path and optional additional query constraints.\r\n * Path must currently be empty if this is a collection group query.\r\n *\r\n * NOTE: you should always construct `Target` from `Query.toTarget` instead of\r\n * using this factory method, because `Query` provides an implicit `orderBy`\r\n * property.\r\n */\n\n\nfunction newTarget(path, collectionGroup = null, orderBy = [], filters = [], limit = null, startAt = null, endAt = null) {\n  return new TargetImpl(path, collectionGroup, orderBy, filters, limit, startAt, endAt);\n}\n\nfunction canonifyTarget(target) {\n  const targetImpl = debugCast(target);\n\n  if (targetImpl.memoizedCanonicalId === null) {\n    let str = targetImpl.path.canonicalString();\n\n    if (targetImpl.collectionGroup !== null) {\n      str += '|cg:' + targetImpl.collectionGroup;\n    }\n\n    str += '|f:';\n    str += targetImpl.filters.map(f => canonifyFilter(f)).join(',');\n    str += '|ob:';\n    str += targetImpl.orderBy.map(o => canonifyOrderBy(o)).join(',');\n\n    if (!isNullOrUndefined(targetImpl.limit)) {\n      str += '|l:';\n      str += targetImpl.limit;\n    }\n\n    if (targetImpl.startAt) {\n      str += '|lb:';\n      str += targetImpl.startAt.inclusive ? 'b:' : 'a:';\n      str += targetImpl.startAt.position.map(p => canonicalId(p)).join(',');\n    }\n\n    if (targetImpl.endAt) {\n      str += '|ub:';\n      str += targetImpl.endAt.inclusive ? 'a:' : 'b:';\n      str += targetImpl.endAt.position.map(p => canonicalId(p)).join(',');\n    }\n\n    targetImpl.memoizedCanonicalId = str;\n  }\n\n  return targetImpl.memoizedCanonicalId;\n}\n\nfunction stringifyTarget(target) {\n  let str = target.path.canonicalString();\n\n  if (target.collectionGroup !== null) {\n    str += ' collectionGroup=' + target.collectionGroup;\n  }\n\n  if (target.filters.length > 0) {\n    str += `, filters: [${target.filters.map(f => stringifyFilter(f)).join(', ')}]`;\n  }\n\n  if (!isNullOrUndefined(target.limit)) {\n    str += ', limit: ' + target.limit;\n  }\n\n  if (target.orderBy.length > 0) {\n    str += `, orderBy: [${target.orderBy.map(o => stringifyOrderBy(o)).join(', ')}]`;\n  }\n\n  if (target.startAt) {\n    str += ', startAt: ';\n    str += target.startAt.inclusive ? 'b:' : 'a:';\n    str += target.startAt.position.map(p => canonicalId(p)).join(',');\n  }\n\n  if (target.endAt) {\n    str += ', endAt: ';\n    str += target.endAt.inclusive ? 'a:' : 'b:';\n    str += target.endAt.position.map(p => canonicalId(p)).join(',');\n  }\n\n  return `Target(${str})`;\n}\n\nfunction targetEquals(left, right) {\n  if (left.limit !== right.limit) {\n    return false;\n  }\n\n  if (left.orderBy.length !== right.orderBy.length) {\n    return false;\n  }\n\n  for (let i = 0; i < left.orderBy.length; i++) {\n    if (!orderByEquals(left.orderBy[i], right.orderBy[i])) {\n      return false;\n    }\n  }\n\n  if (left.filters.length !== right.filters.length) {\n    return false;\n  }\n\n  for (let i = 0; i < left.filters.length; i++) {\n    if (!filterEquals(left.filters[i], right.filters[i])) {\n      return false;\n    }\n  }\n\n  if (left.collectionGroup !== right.collectionGroup) {\n    return false;\n  }\n\n  if (!left.path.isEqual(right.path)) {\n    return false;\n  }\n\n  if (!boundEquals(left.startAt, right.startAt)) {\n    return false;\n  }\n\n  return boundEquals(left.endAt, right.endAt);\n}\n\nfunction targetIsDocumentTarget(target) {\n  return DocumentKey.isDocumentKey(target.path) && target.collectionGroup === null && target.filters.length === 0;\n}\n/** Returns the field filters that target the given field path. */\n\n\nfunction targetGetFieldFiltersForPath(target, path) {\n  return target.filters.filter(f => f instanceof FieldFilter && f.field.isEqual(path));\n}\n/**\r\n * Returns the values that are used in ARRAY_CONTAINS or ARRAY_CONTAINS_ANY\r\n * filters. Returns `null` if there are no such filters.\r\n */\n\n\nfunction targetGetArrayValues(target, fieldIndex) {\n  const segment = fieldIndexGetArraySegment(fieldIndex);\n\n  if (segment === undefined) {\n    return null;\n  }\n\n  for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\n    switch (fieldFilter.op) {\n      case \"array-contains-any\"\n      /* Operator.ARRAY_CONTAINS_ANY */\n      :\n        return fieldFilter.value.arrayValue.values || [];\n\n      case \"array-contains\"\n      /* Operator.ARRAY_CONTAINS */\n      :\n        return [fieldFilter.value];\n      // Remaining filters are not array filters.\n    }\n  }\n\n  return null;\n}\n/**\r\n * Returns the list of values that are used in != or NOT_IN filters. Returns\r\n * `null` if there are no such filters.\r\n */\n\n\nfunction targetGetNotInValues(target, fieldIndex) {\n  const values = new Map();\n\n  for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\n    for (const fieldFilter of targetGetFieldFiltersForPath(target, segment.fieldPath)) {\n      switch (fieldFilter.op) {\n        case \"==\"\n        /* Operator.EQUAL */\n        :\n        case \"in\"\n        /* Operator.IN */\n        :\n          // Encode equality prefix, which is encoded in the index value before\n          // the inequality (e.g. `a == 'a' && b != 'b'` is encoded to\n          // `value != 'ab'`).\n          values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\n          break;\n\n        case \"not-in\"\n        /* Operator.NOT_IN */\n        :\n        case \"!=\"\n        /* Operator.NOT_EQUAL */\n        :\n          // NotIn/NotEqual is always a suffix. There cannot be any remaining\n          // segments and hence we can return early here.\n          values.set(segment.fieldPath.canonicalString(), fieldFilter.value);\n          return Array.from(values.values());\n        // Remaining filters cannot be used as notIn bounds.\n      }\n    }\n  }\n\n  return null;\n}\n/**\r\n * Returns a lower bound of field values that can be used as a starting point to\r\n * scan the index defined by `fieldIndex`. Returns `MIN_VALUE` if no lower bound\r\n * exists.\r\n */\n\n\nfunction targetGetLowerBound(target, fieldIndex) {\n  const values = [];\n  let inclusive = true; // For each segment, retrieve a lower bound if there is a suitable filter or\n  // startAt.\n\n  for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\n    const segmentBound = segment.kind === 0\n    /* IndexKind.ASCENDING */\n    ? targetGetAscendingBound(target, segment.fieldPath, target.startAt) : targetGetDescendingBound(target, segment.fieldPath, target.startAt);\n    values.push(segmentBound.value);\n    inclusive && (inclusive = segmentBound.inclusive);\n  }\n\n  return new Bound(values, inclusive);\n}\n/**\r\n * Returns an upper bound of field values that can be used as an ending point\r\n * when scanning the index defined by `fieldIndex`. Returns `MAX_VALUE` if no\r\n * upper bound exists.\r\n */\n\n\nfunction targetGetUpperBound(target, fieldIndex) {\n  const values = [];\n  let inclusive = true; // For each segment, retrieve an upper bound if there is a suitable filter or\n  // endAt.\n\n  for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\n    const segmentBound = segment.kind === 0\n    /* IndexKind.ASCENDING */\n    ? targetGetDescendingBound(target, segment.fieldPath, target.endAt) : targetGetAscendingBound(target, segment.fieldPath, target.endAt);\n    values.push(segmentBound.value);\n    inclusive && (inclusive = segmentBound.inclusive);\n  }\n\n  return new Bound(values, inclusive);\n}\n/**\r\n * Returns the value to use as the lower bound for ascending index segment at\r\n * the provided `fieldPath` (or the upper bound for an descending segment).\r\n */\n\n\nfunction targetGetAscendingBound(target, fieldPath, bound) {\n  let value = MIN_VALUE;\n  let inclusive = true; // Process all filters to find a value for the current field segment\n\n  for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\n    let filterValue = MIN_VALUE;\n    let filterInclusive = true;\n\n    switch (fieldFilter.op) {\n      case \"<\"\n      /* Operator.LESS_THAN */\n      :\n      case \"<=\"\n      /* Operator.LESS_THAN_OR_EQUAL */\n      :\n        filterValue = valuesGetLowerBound(fieldFilter.value);\n        break;\n\n      case \"==\"\n      /* Operator.EQUAL */\n      :\n      case \"in\"\n      /* Operator.IN */\n      :\n      case \">=\"\n      /* Operator.GREATER_THAN_OR_EQUAL */\n      :\n        filterValue = fieldFilter.value;\n        break;\n\n      case \">\"\n      /* Operator.GREATER_THAN */\n      :\n        filterValue = fieldFilter.value;\n        filterInclusive = false;\n        break;\n\n      case \"!=\"\n      /* Operator.NOT_EQUAL */\n      :\n      case \"not-in\"\n      /* Operator.NOT_IN */\n      :\n        filterValue = MIN_VALUE;\n        break;\n      // Remaining filters cannot be used as lower bounds.\n    }\n\n    if (lowerBoundCompare({\n      value,\n      inclusive\n    }, {\n      value: filterValue,\n      inclusive: filterInclusive\n    }) < 0) {\n      value = filterValue;\n      inclusive = filterInclusive;\n    }\n  } // If there is an additional bound, compare the values against the existing\n  // range to see if we can narrow the scope.\n\n\n  if (bound !== null) {\n    for (let i = 0; i < target.orderBy.length; ++i) {\n      const orderBy = target.orderBy[i];\n\n      if (orderBy.field.isEqual(fieldPath)) {\n        const cursorValue = bound.position[i];\n\n        if (lowerBoundCompare({\n          value,\n          inclusive\n        }, {\n          value: cursorValue,\n          inclusive: bound.inclusive\n        }) < 0) {\n          value = cursorValue;\n          inclusive = bound.inclusive;\n        }\n\n        break;\n      }\n    }\n  }\n\n  return {\n    value,\n    inclusive\n  };\n}\n/**\r\n * Returns the value to use as the upper bound for ascending index segment at\r\n * the provided `fieldPath` (or the lower bound for a descending segment).\r\n */\n\n\nfunction targetGetDescendingBound(target, fieldPath, bound) {\n  let value = MAX_VALUE;\n  let inclusive = true; // Process all filters to find a value for the current field segment\n\n  for (const fieldFilter of targetGetFieldFiltersForPath(target, fieldPath)) {\n    let filterValue = MAX_VALUE;\n    let filterInclusive = true;\n\n    switch (fieldFilter.op) {\n      case \">=\"\n      /* Operator.GREATER_THAN_OR_EQUAL */\n      :\n      case \">\"\n      /* Operator.GREATER_THAN */\n      :\n        filterValue = valuesGetUpperBound(fieldFilter.value);\n        filterInclusive = false;\n        break;\n\n      case \"==\"\n      /* Operator.EQUAL */\n      :\n      case \"in\"\n      /* Operator.IN */\n      :\n      case \"<=\"\n      /* Operator.LESS_THAN_OR_EQUAL */\n      :\n        filterValue = fieldFilter.value;\n        break;\n\n      case \"<\"\n      /* Operator.LESS_THAN */\n      :\n        filterValue = fieldFilter.value;\n        filterInclusive = false;\n        break;\n\n      case \"!=\"\n      /* Operator.NOT_EQUAL */\n      :\n      case \"not-in\"\n      /* Operator.NOT_IN */\n      :\n        filterValue = MAX_VALUE;\n        break;\n      // Remaining filters cannot be used as upper bounds.\n    }\n\n    if (upperBoundCompare({\n      value,\n      inclusive\n    }, {\n      value: filterValue,\n      inclusive: filterInclusive\n    }) > 0) {\n      value = filterValue;\n      inclusive = filterInclusive;\n    }\n  } // If there is an additional bound, compare the values against the existing\n  // range to see if we can narrow the scope.\n\n\n  if (bound !== null) {\n    for (let i = 0; i < target.orderBy.length; ++i) {\n      const orderBy = target.orderBy[i];\n\n      if (orderBy.field.isEqual(fieldPath)) {\n        const cursorValue = bound.position[i];\n\n        if (upperBoundCompare({\n          value,\n          inclusive\n        }, {\n          value: cursorValue,\n          inclusive: bound.inclusive\n        }) > 0) {\n          value = cursorValue;\n          inclusive = bound.inclusive;\n        }\n\n        break;\n      }\n    }\n  }\n\n  return {\n    value,\n    inclusive\n  };\n}\n/** Returns the number of segments of a perfect index for this target. */\n\n\nfunction targetGetSegmentCount(target) {\n  let fields = new SortedSet(FieldPath$1.comparator);\n  let hasArraySegment = false;\n\n  for (const filter of target.filters) {\n    for (const subFilter of filter.getFlattenedFilters()) {\n      // __name__ is not an explicit segment of any index, so we don't need to\n      // count it.\n      if (subFilter.field.isKeyField()) {\n        continue;\n      } // ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filters must be counted separately.\n      // For instance, it is possible to have an index for \"a ARRAY a ASC\". Even\n      // though these are on the same field, they should be counted as two\n      // separate segments in an index.\n\n\n      if (subFilter.op === \"array-contains\"\n      /* Operator.ARRAY_CONTAINS */\n      || subFilter.op === \"array-contains-any\"\n      /* Operator.ARRAY_CONTAINS_ANY */\n      ) {\n        hasArraySegment = true;\n      } else {\n        fields = fields.add(subFilter.field);\n      }\n    }\n  }\n\n  for (const orderBy of target.orderBy) {\n    // __name__ is not an explicit segment of any index, so we don't need to\n    // count it.\n    if (!orderBy.field.isKeyField()) {\n      fields = fields.add(orderBy.field);\n    }\n  }\n\n  return fields.size + (hasArraySegment ? 1 : 0);\n}\n\nfunction targetHasLimit(target) {\n  return target.limit !== null;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Query encapsulates all the query attributes we support in the SDK. It can\r\n * be run against the LocalStore, as well as be converted to a `Target` to\r\n * query the RemoteStore results.\r\n *\r\n * Visible for testing.\r\n */\n\n\nclass QueryImpl {\n  /**\r\n   * Initializes a Query with a path and optional additional query constraints.\r\n   * Path must currently be empty if this is a collection group query.\r\n   */\n  constructor(path, collectionGroup = null, explicitOrderBy = [], filters = [], limit = null, limitType = \"F\"\n  /* LimitType.First */\n  , startAt = null, endAt = null) {\n    this.path = path;\n    this.collectionGroup = collectionGroup;\n    this.explicitOrderBy = explicitOrderBy;\n    this.filters = filters;\n    this.limit = limit;\n    this.limitType = limitType;\n    this.startAt = startAt;\n    this.endAt = endAt;\n    this.memoizedOrderBy = null; // The corresponding `Target` of this `Query` instance.\n\n    this.memoizedTarget = null;\n    if (this.startAt) ;\n    if (this.endAt) ;\n  }\n\n}\n/** Creates a new Query instance with the options provided. */\n\n\nfunction newQuery(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt) {\n  return new QueryImpl(path, collectionGroup, explicitOrderBy, filters, limit, limitType, startAt, endAt);\n}\n/** Creates a new Query for a query that matches all documents at `path` */\n\n\nfunction newQueryForPath(path) {\n  return new QueryImpl(path);\n}\n/**\r\n * Helper to convert a collection group query into a collection query at a\r\n * specific path. This is used when executing collection group queries, since\r\n * we have to split the query into a set of collection queries at multiple\r\n * paths.\r\n */\n\n\nfunction asCollectionQueryAtPath(query, path) {\n  return new QueryImpl(path,\n  /*collectionGroup=*/\n  null, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\n}\n/**\r\n * Returns true if this query does not specify any query constraints that\r\n * could remove results.\r\n */\n\n\nfunction queryMatchesAllDocuments(query) {\n  return query.filters.length === 0 && query.limit === null && query.startAt == null && query.endAt == null && (query.explicitOrderBy.length === 0 || query.explicitOrderBy.length === 1 && query.explicitOrderBy[0].field.isKeyField());\n}\n\nfunction getFirstOrderByField(query) {\n  return query.explicitOrderBy.length > 0 ? query.explicitOrderBy[0].field : null;\n}\n\nfunction getInequalityFilterField(query) {\n  for (const filter of query.filters) {\n    const result = filter.getFirstInequalityField();\n\n    if (result !== null) {\n      return result;\n    }\n  }\n\n  return null;\n}\n/**\r\n * Creates a new Query for a collection group query that matches all documents\r\n * within the provided collection group.\r\n */\n\n\nfunction newQueryForCollectionGroup(collectionId) {\n  return new QueryImpl(ResourcePath.emptyPath(), collectionId);\n}\n/**\r\n * Returns whether the query matches a single document by path (rather than a\r\n * collection).\r\n */\n\n\nfunction isDocumentQuery$1(query) {\n  return DocumentKey.isDocumentKey(query.path) && query.collectionGroup === null && query.filters.length === 0;\n}\n/**\r\n * Returns whether the query matches a collection group rather than a specific\r\n * collection.\r\n */\n\n\nfunction isCollectionGroupQuery(query) {\n  return query.collectionGroup !== null;\n}\n/**\r\n * Returns the implicit order by constraint that is used to execute the Query,\r\n * which can be different from the order by constraints the user provided (e.g.\r\n * the SDK and backend always orders by `__name__`).\r\n */\n\n\nfunction queryOrderBy(query) {\n  const queryImpl = debugCast(query);\n\n  if (queryImpl.memoizedOrderBy === null) {\n    queryImpl.memoizedOrderBy = [];\n    const inequalityField = getInequalityFilterField(queryImpl);\n    const firstOrderByField = getFirstOrderByField(queryImpl);\n\n    if (inequalityField !== null && firstOrderByField === null) {\n      // In order to implicitly add key ordering, we must also add the\n      // inequality filter field for it to be a valid query.\n      // Note that the default inequality field and key ordering is ascending.\n      if (!inequalityField.isKeyField()) {\n        queryImpl.memoizedOrderBy.push(new OrderBy(inequalityField));\n      }\n\n      queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), \"asc\"\n      /* Direction.ASCENDING */\n      ));\n    } else {\n      let foundKeyOrdering = false;\n\n      for (const orderBy of queryImpl.explicitOrderBy) {\n        queryImpl.memoizedOrderBy.push(orderBy);\n\n        if (orderBy.field.isKeyField()) {\n          foundKeyOrdering = true;\n        }\n      }\n\n      if (!foundKeyOrdering) {\n        // The order of the implicit key ordering always matches the last\n        // explicit order by\n        const lastDirection = queryImpl.explicitOrderBy.length > 0 ? queryImpl.explicitOrderBy[queryImpl.explicitOrderBy.length - 1].dir : \"asc\"\n        /* Direction.ASCENDING */\n        ;\n        queryImpl.memoizedOrderBy.push(new OrderBy(FieldPath$1.keyField(), lastDirection));\n      }\n    }\n  }\n\n  return queryImpl.memoizedOrderBy;\n}\n/**\r\n * Converts this `Query` instance to it's corresponding `Target` representation.\r\n */\n\n\nfunction queryToTarget(query) {\n  const queryImpl = debugCast(query);\n\n  if (!queryImpl.memoizedTarget) {\n    if (queryImpl.limitType === \"F\"\n    /* LimitType.First */\n    ) {\n      queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, queryOrderBy(queryImpl), queryImpl.filters, queryImpl.limit, queryImpl.startAt, queryImpl.endAt);\n    } else {\n      // Flip the orderBy directions since we want the last results\n      const orderBys = [];\n\n      for (const orderBy of queryOrderBy(queryImpl)) {\n        const dir = orderBy.dir === \"desc\"\n        /* Direction.DESCENDING */\n        ? \"asc\"\n        /* Direction.ASCENDING */\n        : \"desc\"\n        /* Direction.DESCENDING */\n        ;\n        orderBys.push(new OrderBy(orderBy.field, dir));\n      } // We need to swap the cursors to match the now-flipped query ordering.\n\n\n      const startAt = queryImpl.endAt ? new Bound(queryImpl.endAt.position, queryImpl.endAt.inclusive) : null;\n      const endAt = queryImpl.startAt ? new Bound(queryImpl.startAt.position, queryImpl.startAt.inclusive) : null; // Now return as a LimitType.First query.\n\n      queryImpl.memoizedTarget = newTarget(queryImpl.path, queryImpl.collectionGroup, orderBys, queryImpl.filters, queryImpl.limit, startAt, endAt);\n    }\n  }\n\n  return queryImpl.memoizedTarget;\n}\n\nfunction queryWithAddedFilter(query, filter) {\n  filter.getFirstInequalityField();\n  getInequalityFilterField(query);\n  const newFilters = query.filters.concat([filter]);\n  return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), newFilters, query.limit, query.limitType, query.startAt, query.endAt);\n}\n\nfunction queryWithAddedOrderBy(query, orderBy) {\n  // TODO(dimond): validate that orderBy does not list the same key twice.\n  const newOrderBy = query.explicitOrderBy.concat([orderBy]);\n  return new QueryImpl(query.path, query.collectionGroup, newOrderBy, query.filters.slice(), query.limit, query.limitType, query.startAt, query.endAt);\n}\n\nfunction queryWithLimit(query, limit, limitType) {\n  return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), limit, limitType, query.startAt, query.endAt);\n}\n\nfunction queryWithStartAt(query, bound) {\n  return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, bound, query.endAt);\n}\n\nfunction queryWithEndAt(query, bound) {\n  return new QueryImpl(query.path, query.collectionGroup, query.explicitOrderBy.slice(), query.filters.slice(), query.limit, query.limitType, query.startAt, bound);\n}\n\nfunction queryEquals(left, right) {\n  return targetEquals(queryToTarget(left), queryToTarget(right)) && left.limitType === right.limitType;\n} // TODO(b/29183165): This is used to get a unique string from a query to, for\n// example, use as a dictionary key, but the implementation is subject to\n// collisions. Make it collision-free.\n\n\nfunction canonifyQuery(query) {\n  return `${canonifyTarget(queryToTarget(query))}|lt:${query.limitType}`;\n}\n\nfunction stringifyQuery(query) {\n  return `Query(target=${stringifyTarget(queryToTarget(query))}; limitType=${query.limitType})`;\n}\n/** Returns whether `doc` matches the constraints of `query`. */\n\n\nfunction queryMatches(query, doc) {\n  return doc.isFoundDocument() && queryMatchesPathAndCollectionGroup(query, doc) && queryMatchesOrderBy(query, doc) && queryMatchesFilters(query, doc) && queryMatchesBounds(query, doc);\n}\n\nfunction queryMatchesPathAndCollectionGroup(query, doc) {\n  const docPath = doc.key.path;\n\n  if (query.collectionGroup !== null) {\n    // NOTE: this.path is currently always empty since we don't expose Collection\n    // Group queries rooted at a document path yet.\n    return doc.key.hasCollectionId(query.collectionGroup) && query.path.isPrefixOf(docPath);\n  } else if (DocumentKey.isDocumentKey(query.path)) {\n    // exact match for document queries\n    return query.path.isEqual(docPath);\n  } else {\n    // shallow ancestor queries by default\n    return query.path.isImmediateParentOf(docPath);\n  }\n}\n/**\r\n * A document must have a value for every ordering clause in order to show up\r\n * in the results.\r\n */\n\n\nfunction queryMatchesOrderBy(query, doc) {\n  // We must use `queryOrderBy()` to get the list of all orderBys (both implicit and explicit).\n  // Note that for OR queries, orderBy applies to all disjunction terms and implicit orderBys must\n  // be taken into account. For example, the query \"a > 1 || b==1\" has an implicit \"orderBy a\" due\n  // to the inequality, and is evaluated as \"a > 1 orderBy a || b==1 orderBy a\".\n  // A document with content of {b:1} matches the filters, but does not match the orderBy because\n  // it's missing the field 'a'.\n  for (const orderBy of queryOrderBy(query)) {\n    // order by key always matches\n    if (!orderBy.field.isKeyField() && doc.data.field(orderBy.field) === null) {\n      return false;\n    }\n  }\n\n  return true;\n}\n\nfunction queryMatchesFilters(query, doc) {\n  for (const filter of query.filters) {\n    if (!filter.matches(doc)) {\n      return false;\n    }\n  }\n\n  return true;\n}\n/** Makes sure a document is within the bounds, if provided. */\n\n\nfunction queryMatchesBounds(query, doc) {\n  if (query.startAt && !boundSortsBeforeDocument(query.startAt, queryOrderBy(query), doc)) {\n    return false;\n  }\n\n  if (query.endAt && !boundSortsAfterDocument(query.endAt, queryOrderBy(query), doc)) {\n    return false;\n  }\n\n  return true;\n}\n/**\r\n * Returns the collection group that this query targets.\r\n *\r\n * PORTING NOTE: This is only used in the Web SDK to facilitate multi-tab\r\n * synchronization for query results.\r\n */\n\n\nfunction queryCollectionGroup(query) {\n  return query.collectionGroup || (query.path.length % 2 === 1 ? query.path.lastSegment() : query.path.get(query.path.length - 2));\n}\n/**\r\n * Returns a new comparator function that can be used to compare two documents\r\n * based on the Query's ordering constraint.\r\n */\n\n\nfunction newQueryComparator(query) {\n  return (d1, d2) => {\n    let comparedOnKeyField = false;\n\n    for (const orderBy of queryOrderBy(query)) {\n      const comp = compareDocs(orderBy, d1, d2);\n\n      if (comp !== 0) {\n        return comp;\n      }\n\n      comparedOnKeyField = comparedOnKeyField || orderBy.field.isKeyField();\n    }\n\n    return 0;\n  };\n}\n\nfunction compareDocs(orderBy, d1, d2) {\n  const comparison = orderBy.field.isKeyField() ? DocumentKey.comparator(d1.key, d2.key) : compareDocumentsByField(orderBy.field, d1, d2);\n\n  switch (orderBy.dir) {\n    case \"asc\"\n    /* Direction.ASCENDING */\n    :\n      return comparison;\n\n    case \"desc\"\n    /* Direction.DESCENDING */\n    :\n      return -1 * comparison;\n\n    default:\n      return fail();\n  }\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A map implementation that uses objects as keys. Objects must have an\r\n * associated equals function and must be immutable. Entries in the map are\r\n * stored together with the key being produced from the mapKeyFn. This map\r\n * automatically handles collisions of keys.\r\n */\n\n\nclass ObjectMap {\n  constructor(mapKeyFn, equalsFn) {\n    this.mapKeyFn = mapKeyFn;\n    this.equalsFn = equalsFn;\n    /**\r\n     * The inner map for a key/value pair. Due to the possibility of collisions we\r\n     * keep a list of entries that we do a linear search through to find an actual\r\n     * match. Note that collisions should be rare, so we still expect near\r\n     * constant time lookups in practice.\r\n     */\n\n    this.inner = {};\n    /** The number of entries stored in the map */\n\n    this.innerSize = 0;\n  }\n  /** Get a value for this key, or undefined if it does not exist. */\n\n\n  get(key) {\n    const id = this.mapKeyFn(key);\n    const matches = this.inner[id];\n\n    if (matches === undefined) {\n      return undefined;\n    }\n\n    for (const [otherKey, value] of matches) {\n      if (this.equalsFn(otherKey, key)) {\n        return value;\n      }\n    }\n\n    return undefined;\n  }\n\n  has(key) {\n    return this.get(key) !== undefined;\n  }\n  /** Put this key and value in the map. */\n\n\n  set(key, value) {\n    const id = this.mapKeyFn(key);\n    const matches = this.inner[id];\n\n    if (matches === undefined) {\n      this.inner[id] = [[key, value]];\n      this.innerSize++;\n      return;\n    }\n\n    for (let i = 0; i < matches.length; i++) {\n      if (this.equalsFn(matches[i][0], key)) {\n        // This is updating an existing entry and does not increase `innerSize`.\n        matches[i] = [key, value];\n        return;\n      }\n    }\n\n    matches.push([key, value]);\n    this.innerSize++;\n  }\n  /**\r\n   * Remove this key from the map. Returns a boolean if anything was deleted.\r\n   */\n\n\n  delete(key) {\n    const id = this.mapKeyFn(key);\n    const matches = this.inner[id];\n\n    if (matches === undefined) {\n      return false;\n    }\n\n    for (let i = 0; i < matches.length; i++) {\n      if (this.equalsFn(matches[i][0], key)) {\n        if (matches.length === 1) {\n          delete this.inner[id];\n        } else {\n          matches.splice(i, 1);\n        }\n\n        this.innerSize--;\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  forEach(fn) {\n    forEach(this.inner, (_, entries) => {\n      for (const [k, v] of entries) {\n        fn(k, v);\n      }\n    });\n  }\n\n  isEmpty() {\n    return isEmpty(this.inner);\n  }\n\n  size() {\n    return this.innerSize;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst EMPTY_MUTABLE_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\n\nfunction mutableDocumentMap() {\n  return EMPTY_MUTABLE_DOCUMENT_MAP;\n}\n\nconst EMPTY_DOCUMENT_MAP = new SortedMap(DocumentKey.comparator);\n\nfunction documentMap(...docs) {\n  let map = EMPTY_DOCUMENT_MAP;\n\n  for (const doc of docs) {\n    map = map.insert(doc.key, doc);\n  }\n\n  return map;\n}\n\nfunction newOverlayedDocumentMap() {\n  return newDocumentKeyMap();\n}\n\nfunction convertOverlayedDocumentMapToDocumentMap(collection) {\n  let documents = EMPTY_DOCUMENT_MAP;\n  collection.forEach((k, v) => documents = documents.insert(k, v.overlayedDocument));\n  return documents;\n}\n\nfunction newOverlayMap() {\n  return newDocumentKeyMap();\n}\n\nfunction newMutationMap() {\n  return newDocumentKeyMap();\n}\n\nfunction newDocumentKeyMap() {\n  return new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\n}\n\nconst EMPTY_DOCUMENT_VERSION_MAP = new SortedMap(DocumentKey.comparator);\n\nfunction documentVersionMap() {\n  return EMPTY_DOCUMENT_VERSION_MAP;\n}\n\nconst EMPTY_DOCUMENT_KEY_SET = new SortedSet(DocumentKey.comparator);\n\nfunction documentKeySet(...keys) {\n  let set = EMPTY_DOCUMENT_KEY_SET;\n\n  for (const key of keys) {\n    set = set.add(key);\n  }\n\n  return set;\n}\n\nconst EMPTY_TARGET_ID_SET = new SortedSet(primitiveComparator);\n\nfunction targetIdSet() {\n  return EMPTY_TARGET_ID_SET;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Returns an DoubleValue for `value` that is encoded based the serializer's\r\n * `useProto3Json` setting.\r\n */\n\n\nfunction toDouble(serializer, value) {\n  if (serializer.useProto3Json) {\n    if (isNaN(value)) {\n      return {\n        doubleValue: 'NaN'\n      };\n    } else if (value === Infinity) {\n      return {\n        doubleValue: 'Infinity'\n      };\n    } else if (value === -Infinity) {\n      return {\n        doubleValue: '-Infinity'\n      };\n    }\n  }\n\n  return {\n    doubleValue: isNegativeZero(value) ? '-0' : value\n  };\n}\n/**\r\n * Returns an IntegerValue for `value`.\r\n */\n\n\nfunction toInteger(value) {\n  return {\n    integerValue: '' + value\n  };\n}\n/**\r\n * Returns a value for a number that's appropriate to put into a proto.\r\n * The return value is an IntegerValue if it can safely represent the value,\r\n * otherwise a DoubleValue is returned.\r\n */\n\n\nfunction toNumber(serializer, value) {\n  return isSafeInteger(value) ? toInteger(value) : toDouble(serializer, value);\n}\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Used to represent a field transform on a mutation. */\n\n\nclass TransformOperation {\n  constructor() {\n    // Make sure that the structural type of `TransformOperation` is unique.\n    // See https://github.com/microsoft/TypeScript/issues/5451\n    this._ = undefined;\n  }\n\n}\n/**\r\n * Computes the local transform result against the provided `previousValue`,\r\n * optionally using the provided localWriteTime.\r\n */\n\n\nfunction applyTransformOperationToLocalView(transform, previousValue, localWriteTime) {\n  if (transform instanceof ServerTimestampTransform) {\n    return serverTimestamp$1(localWriteTime, previousValue);\n  } else if (transform instanceof ArrayUnionTransformOperation) {\n    return applyArrayUnionTransformOperation(transform, previousValue);\n  } else if (transform instanceof ArrayRemoveTransformOperation) {\n    return applyArrayRemoveTransformOperation(transform, previousValue);\n  } else {\n    return applyNumericIncrementTransformOperationToLocalView(transform, previousValue);\n  }\n}\n/**\r\n * Computes a final transform result after the transform has been acknowledged\r\n * by the server, potentially using the server-provided transformResult.\r\n */\n\n\nfunction applyTransformOperationToRemoteDocument(transform, previousValue, transformResult) {\n  // The server just sends null as the transform result for array operations,\n  // so we have to calculate a result the same as we do for local\n  // applications.\n  if (transform instanceof ArrayUnionTransformOperation) {\n    return applyArrayUnionTransformOperation(transform, previousValue);\n  } else if (transform instanceof ArrayRemoveTransformOperation) {\n    return applyArrayRemoveTransformOperation(transform, previousValue);\n  }\n\n  return transformResult;\n}\n/**\r\n * If this transform operation is not idempotent, returns the base value to\r\n * persist for this transform. If a base value is returned, the transform\r\n * operation is always applied to this base value, even if document has\r\n * already been updated.\r\n *\r\n * Base values provide consistent behavior for non-idempotent transforms and\r\n * allow us to return the same latency-compensated value even if the backend\r\n * has already applied the transform operation. The base value is null for\r\n * idempotent transforms, as they can be re-played even if the backend has\r\n * already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent transforms.\r\n */\n\n\nfunction computeTransformOperationBaseValue(transform, previousValue) {\n  if (transform instanceof NumericIncrementTransformOperation) {\n    return isNumber(previousValue) ? previousValue : {\n      integerValue: 0\n    };\n  }\n\n  return null;\n}\n\nfunction transformOperationEquals(left, right) {\n  if (left instanceof ArrayUnionTransformOperation && right instanceof ArrayUnionTransformOperation) {\n    return arrayEquals(left.elements, right.elements, valueEquals);\n  } else if (left instanceof ArrayRemoveTransformOperation && right instanceof ArrayRemoveTransformOperation) {\n    return arrayEquals(left.elements, right.elements, valueEquals);\n  } else if (left instanceof NumericIncrementTransformOperation && right instanceof NumericIncrementTransformOperation) {\n    return valueEquals(left.operand, right.operand);\n  }\n\n  return left instanceof ServerTimestampTransform && right instanceof ServerTimestampTransform;\n}\n/** Transforms a value into a server-generated timestamp. */\n\n\nclass ServerTimestampTransform extends TransformOperation {}\n/** Transforms an array value via a union operation. */\n\n\nclass ArrayUnionTransformOperation extends TransformOperation {\n  constructor(elements) {\n    super();\n    this.elements = elements;\n  }\n\n}\n\nfunction applyArrayUnionTransformOperation(transform, previousValue) {\n  const values = coercedFieldValuesArray(previousValue);\n\n  for (const toUnion of transform.elements) {\n    if (!values.some(element => valueEquals(element, toUnion))) {\n      values.push(toUnion);\n    }\n  }\n\n  return {\n    arrayValue: {\n      values\n    }\n  };\n}\n/** Transforms an array value via a remove operation. */\n\n\nclass ArrayRemoveTransformOperation extends TransformOperation {\n  constructor(elements) {\n    super();\n    this.elements = elements;\n  }\n\n}\n\nfunction applyArrayRemoveTransformOperation(transform, previousValue) {\n  let values = coercedFieldValuesArray(previousValue);\n\n  for (const toRemove of transform.elements) {\n    values = values.filter(element => !valueEquals(element, toRemove));\n  }\n\n  return {\n    arrayValue: {\n      values\n    }\n  };\n}\n/**\r\n * Implements the backend semantics for locally computed NUMERIC_ADD (increment)\r\n * transforms. Converts all field values to integers or doubles, but unlike the\r\n * backend does not cap integer values at 2^63. Instead, JavaScript number\r\n * arithmetic is used and precision loss can occur for values greater than 2^53.\r\n */\n\n\nclass NumericIncrementTransformOperation extends TransformOperation {\n  constructor(serializer, operand) {\n    super();\n    this.serializer = serializer;\n    this.operand = operand;\n  }\n\n}\n\nfunction applyNumericIncrementTransformOperationToLocalView(transform, previousValue) {\n  // PORTING NOTE: Since JavaScript's integer arithmetic is limited to 53 bit\n  // precision and resolves overflows by reducing precision, we do not\n  // manually cap overflows at 2^63.\n  const baseValue = computeTransformOperationBaseValue(transform, previousValue);\n  const sum = asNumber(baseValue) + asNumber(transform.operand);\n\n  if (isInteger(baseValue) && isInteger(transform.operand)) {\n    return toInteger(sum);\n  } else {\n    return toDouble(transform.serializer, sum);\n  }\n}\n\nfunction asNumber(value) {\n  return normalizeNumber(value.integerValue || value.doubleValue);\n}\n\nfunction coercedFieldValuesArray(value) {\n  return isArray(value) && value.arrayValue.values ? value.arrayValue.values.slice() : [];\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** A field path and the TransformOperation to perform upon it. */\n\n\nclass FieldTransform {\n  constructor(field, transform) {\n    this.field = field;\n    this.transform = transform;\n  }\n\n}\n\nfunction fieldTransformEquals(left, right) {\n  return left.field.isEqual(right.field) && transformOperationEquals(left.transform, right.transform);\n}\n\nfunction fieldTransformsAreEqual(left, right) {\n  if (left === undefined && right === undefined) {\n    return true;\n  }\n\n  if (left && right) {\n    return arrayEquals(left, right, (l, r) => fieldTransformEquals(l, r));\n  }\n\n  return false;\n}\n/** The result of successfully applying a mutation to the backend. */\n\n\nclass MutationResult {\n  constructor(\n  /**\r\n   * The version at which the mutation was committed:\r\n   *\r\n   * - For most operations, this is the updateTime in the WriteResult.\r\n   * - For deletes, the commitTime of the WriteResponse (because deletes are\r\n   *   not stored and have no updateTime).\r\n   *\r\n   * Note that these versions can be different: No-op writes will not change\r\n   * the updateTime even though the commitTime advances.\r\n   */\n  version,\n  /**\r\n   * The resulting fields returned from the backend after a mutation\r\n   * containing field transforms has been committed. Contains one FieldValue\r\n   * for each FieldTransform that was in the mutation.\r\n   *\r\n   * Will be empty if the mutation did not contain any field transforms.\r\n   */\n  transformResults) {\n    this.version = version;\n    this.transformResults = transformResults;\n  }\n\n}\n/**\r\n * Encodes a precondition for a mutation. This follows the model that the\r\n * backend accepts with the special case of an explicit \"empty\" precondition\r\n * (meaning no precondition).\r\n */\n\n\nclass Precondition {\n  constructor(updateTime, exists) {\n    this.updateTime = updateTime;\n    this.exists = exists;\n  }\n  /** Creates a new empty Precondition. */\n\n\n  static none() {\n    return new Precondition();\n  }\n  /** Creates a new Precondition with an exists flag. */\n\n\n  static exists(exists) {\n    return new Precondition(undefined, exists);\n  }\n  /** Creates a new Precondition based on a version a document exists at. */\n\n\n  static updateTime(version) {\n    return new Precondition(version);\n  }\n  /** Returns whether this Precondition is empty. */\n\n\n  get isNone() {\n    return this.updateTime === undefined && this.exists === undefined;\n  }\n\n  isEqual(other) {\n    return this.exists === other.exists && (this.updateTime ? !!other.updateTime && this.updateTime.isEqual(other.updateTime) : !other.updateTime);\n  }\n\n}\n/** Returns true if the preconditions is valid for the given document. */\n\n\nfunction preconditionIsValidForDocument(precondition, document) {\n  if (precondition.updateTime !== undefined) {\n    return document.isFoundDocument() && document.version.isEqual(precondition.updateTime);\n  } else if (precondition.exists !== undefined) {\n    return precondition.exists === document.isFoundDocument();\n  } else {\n    return true;\n  }\n}\n/**\r\n * A mutation describes a self-contained change to a document. Mutations can\r\n * create, replace, delete, and update subsets of documents.\r\n *\r\n * Mutations not only act on the value of the document but also its version.\r\n *\r\n * For local mutations (mutations that haven't been committed yet), we preserve\r\n * the existing version for Set and Patch mutations. For Delete mutations, we\r\n * reset the version to 0.\r\n *\r\n * Here's the expected transition table.\r\n *\r\n * MUTATION           APPLIED TO            RESULTS IN\r\n *\r\n * SetMutation        Document(v3)          Document(v3)\r\n * SetMutation        NoDocument(v3)        Document(v0)\r\n * SetMutation        InvalidDocument(v0)   Document(v0)\r\n * PatchMutation      Document(v3)          Document(v3)\r\n * PatchMutation      NoDocument(v3)        NoDocument(v3)\r\n * PatchMutation      InvalidDocument(v0)   UnknownDocument(v3)\r\n * DeleteMutation     Document(v3)          NoDocument(v0)\r\n * DeleteMutation     NoDocument(v3)        NoDocument(v0)\r\n * DeleteMutation     InvalidDocument(v0)   NoDocument(v0)\r\n *\r\n * For acknowledged mutations, we use the updateTime of the WriteResponse as\r\n * the resulting version for Set and Patch mutations. As deletes have no\r\n * explicit update time, we use the commitTime of the WriteResponse for\r\n * Delete mutations.\r\n *\r\n * If a mutation is acknowledged by the backend but fails the precondition check\r\n * locally, we transition to an `UnknownDocument` and rely on Watch to send us\r\n * the updated version.\r\n *\r\n * Field transforms are used only with Patch and Set Mutations. We use the\r\n * `updateTransforms` message to store transforms, rather than the `transforms`s\r\n * messages.\r\n *\r\n * ## Subclassing Notes\r\n *\r\n * Every type of mutation needs to implement its own applyToRemoteDocument() and\r\n * applyToLocalView() to implement the actual behavior of applying the mutation\r\n * to some source document (see `setMutationApplyToRemoteDocument()` for an\r\n * example).\r\n */\n\n\nclass Mutation {}\n/**\r\n * A utility method to calculate a `Mutation` representing the overlay from the\r\n * final state of the document, and a `FieldMask` representing the fields that\r\n * are mutated by the local mutations.\r\n */\n\n\nfunction calculateOverlayMutation(doc, mask) {\n  if (!doc.hasLocalMutations || mask && mask.fields.length === 0) {\n    return null;\n  } // mask is null when sets or deletes are applied to the current document.\n\n\n  if (mask === null) {\n    if (doc.isNoDocument()) {\n      return new DeleteMutation(doc.key, Precondition.none());\n    } else {\n      return new SetMutation(doc.key, doc.data, Precondition.none());\n    }\n  } else {\n    const docValue = doc.data;\n    const patchValue = ObjectValue.empty();\n    let maskSet = new SortedSet(FieldPath$1.comparator);\n\n    for (let path of mask.fields) {\n      if (!maskSet.has(path)) {\n        let value = docValue.field(path); // If we are deleting a nested field, we take the immediate parent as\n        // the mask used to construct the resulting mutation.\n        // Justification: Nested fields can create parent fields implicitly. If\n        // only a leaf entry is deleted in later mutations, the parent field\n        // should still remain, but we may have lost this information.\n        // Consider mutation (foo.bar 1), then mutation (foo.bar delete()).\n        // This leaves the final result (foo, {}). Despite the fact that `doc`\n        // has the correct result, `foo` is not in `mask`, and the resulting\n        // mutation would miss `foo`.\n\n        if (value === null && path.length > 1) {\n          path = path.popLast();\n          value = docValue.field(path);\n        }\n\n        if (value === null) {\n          patchValue.delete(path);\n        } else {\n          patchValue.set(path, value);\n        }\n\n        maskSet = maskSet.add(path);\n      }\n    }\n\n    return new PatchMutation(doc.key, patchValue, new FieldMask(maskSet.toArray()), Precondition.none());\n  }\n}\n/**\r\n * Applies this mutation to the given document for the purposes of computing a\r\n * new remote document. If the input document doesn't match the expected state\r\n * (e.g. it is invalid or outdated), the document type may transition to\r\n * unknown.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param mutationResult - The result of applying the mutation from the backend.\r\n */\n\n\nfunction mutationApplyToRemoteDocument(mutation, document, mutationResult) {\n  if (mutation instanceof SetMutation) {\n    setMutationApplyToRemoteDocument(mutation, document, mutationResult);\n  } else if (mutation instanceof PatchMutation) {\n    patchMutationApplyToRemoteDocument(mutation, document, mutationResult);\n  } else {\n    deleteMutationApplyToRemoteDocument(mutation, document, mutationResult);\n  }\n}\n/**\r\n * Applies this mutation to the given document for the purposes of computing\r\n * the new local view of a document. If the input document doesn't match the\r\n * expected state, the document is not modified.\r\n *\r\n * @param mutation - The mutation to apply.\r\n * @param document - The document to mutate. The input document can be an\r\n *     invalid document if the client has no knowledge of the pre-mutation state\r\n *     of the document.\r\n * @param previousMask - The fields that have been updated before applying this mutation.\r\n * @param localWriteTime - A timestamp indicating the local write time of the\r\n *     batch this mutation is a part of.\r\n * @returns A `FieldMask` representing the fields that are changed by applying this mutation.\r\n */\n\n\nfunction mutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\n  if (mutation instanceof SetMutation) {\n    return setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\n  } else if (mutation instanceof PatchMutation) {\n    return patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime);\n  } else {\n    return deleteMutationApplyToLocalView(mutation, document, previousMask);\n  }\n}\n/**\r\n * If this mutation is not idempotent, returns the base value to persist with\r\n * this mutation. If a base value is returned, the mutation is always applied\r\n * to this base value, even if document has already been updated.\r\n *\r\n * The base value is a sparse object that consists of only the document\r\n * fields for which this mutation contains a non-idempotent transformation\r\n * (e.g. a numeric increment). The provided value guarantees consistent\r\n * behavior for non-idempotent transforms and allow us to return the same\r\n * latency-compensated value even if the backend has already applied the\r\n * mutation. The base value is null for idempotent mutations, as they can be\r\n * re-played even if the backend has already applied them.\r\n *\r\n * @returns a base value to store along with the mutation, or null for\r\n * idempotent mutations.\r\n */\n\n\nfunction mutationExtractBaseValue(mutation, document) {\n  let baseObject = null;\n\n  for (const fieldTransform of mutation.fieldTransforms) {\n    const existingValue = document.data.field(fieldTransform.field);\n    const coercedValue = computeTransformOperationBaseValue(fieldTransform.transform, existingValue || null);\n\n    if (coercedValue != null) {\n      if (baseObject === null) {\n        baseObject = ObjectValue.empty();\n      }\n\n      baseObject.set(fieldTransform.field, coercedValue);\n    }\n  }\n\n  return baseObject ? baseObject : null;\n}\n\nfunction mutationEquals(left, right) {\n  if (left.type !== right.type) {\n    return false;\n  }\n\n  if (!left.key.isEqual(right.key)) {\n    return false;\n  }\n\n  if (!left.precondition.isEqual(right.precondition)) {\n    return false;\n  }\n\n  if (!fieldTransformsAreEqual(left.fieldTransforms, right.fieldTransforms)) {\n    return false;\n  }\n\n  if (left.type === 0\n  /* MutationType.Set */\n  ) {\n    return left.value.isEqual(right.value);\n  }\n\n  if (left.type === 1\n  /* MutationType.Patch */\n  ) {\n    return left.data.isEqual(right.data) && left.fieldMask.isEqual(right.fieldMask);\n  }\n\n  return true;\n}\n/**\r\n * A mutation that creates or replaces the document at the given key with the\r\n * object value contents.\r\n */\n\n\nclass SetMutation extends Mutation {\n  constructor(key, value, precondition, fieldTransforms = []) {\n    super();\n    this.key = key;\n    this.value = value;\n    this.precondition = precondition;\n    this.fieldTransforms = fieldTransforms;\n    this.type = 0\n    /* MutationType.Set */\n    ;\n  }\n\n  getFieldMask() {\n    return null;\n  }\n\n}\n\nfunction setMutationApplyToRemoteDocument(mutation, document, mutationResult) {\n  // Unlike setMutationApplyToLocalView, if we're applying a mutation to a\n  // remote document the server has accepted the mutation so the precondition\n  // must have held.\n  const newData = mutation.value.clone();\n  const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\n  newData.setAll(transformResults);\n  document.convertToFoundDocument(mutationResult.version, newData).setHasCommittedMutations();\n}\n\nfunction setMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\n  if (!preconditionIsValidForDocument(mutation.precondition, document)) {\n    // The mutation failed to apply (e.g. a document ID created with add()\n    // caused a name collision).\n    return previousMask;\n  }\n\n  const newData = mutation.value.clone();\n  const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\n  newData.setAll(transformResults);\n  document.convertToFoundDocument(document.version, newData).setHasLocalMutations();\n  return null; // SetMutation overwrites all fields.\n}\n/**\r\n * A mutation that modifies fields of the document at the given key with the\r\n * given values. The values are applied through a field mask:\r\n *\r\n *  * When a field is in both the mask and the values, the corresponding field\r\n *    is updated.\r\n *  * When a field is in neither the mask nor the values, the corresponding\r\n *    field is unmodified.\r\n *  * When a field is in the mask but not in the values, the corresponding field\r\n *    is deleted.\r\n *  * When a field is not in the mask but is in the values, the values map is\r\n *    ignored.\r\n */\n\n\nclass PatchMutation extends Mutation {\n  constructor(key, data, fieldMask, precondition, fieldTransforms = []) {\n    super();\n    this.key = key;\n    this.data = data;\n    this.fieldMask = fieldMask;\n    this.precondition = precondition;\n    this.fieldTransforms = fieldTransforms;\n    this.type = 1\n    /* MutationType.Patch */\n    ;\n  }\n\n  getFieldMask() {\n    return this.fieldMask;\n  }\n\n}\n\nfunction patchMutationApplyToRemoteDocument(mutation, document, mutationResult) {\n  if (!preconditionIsValidForDocument(mutation.precondition, document)) {\n    // Since the mutation was not rejected, we know that the precondition\n    // matched on the backend. We therefore must not have the expected version\n    // of the document in our cache and convert to an UnknownDocument with a\n    // known updateTime.\n    document.convertToUnknownDocument(mutationResult.version);\n    return;\n  }\n\n  const transformResults = serverTransformResults(mutation.fieldTransforms, document, mutationResult.transformResults);\n  const newData = document.data;\n  newData.setAll(getPatch(mutation));\n  newData.setAll(transformResults);\n  document.convertToFoundDocument(mutationResult.version, newData).setHasCommittedMutations();\n}\n\nfunction patchMutationApplyToLocalView(mutation, document, previousMask, localWriteTime) {\n  if (!preconditionIsValidForDocument(mutation.precondition, document)) {\n    return previousMask;\n  }\n\n  const transformResults = localTransformResults(mutation.fieldTransforms, localWriteTime, document);\n  const newData = document.data;\n  newData.setAll(getPatch(mutation));\n  newData.setAll(transformResults);\n  document.convertToFoundDocument(document.version, newData).setHasLocalMutations();\n\n  if (previousMask === null) {\n    return null;\n  }\n\n  return previousMask.unionWith(mutation.fieldMask.fields).unionWith(mutation.fieldTransforms.map(transform => transform.field));\n}\n/**\r\n * Returns a FieldPath/Value map with the content of the PatchMutation.\r\n */\n\n\nfunction getPatch(mutation) {\n  const result = new Map();\n  mutation.fieldMask.fields.forEach(fieldPath => {\n    if (!fieldPath.isEmpty()) {\n      const newValue = mutation.data.field(fieldPath);\n      result.set(fieldPath, newValue);\n    }\n  });\n  return result;\n}\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use after a mutation\r\n * containing transforms has been acknowledged by the server.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param mutableDocument - The current state of the document after applying all\r\n * previous mutations.\r\n * @param serverTransformResults - The transform results received by the server.\r\n * @returns The transform results list.\r\n */\n\n\nfunction serverTransformResults(fieldTransforms, mutableDocument, serverTransformResults) {\n  const transformResults = new Map();\n  hardAssert(fieldTransforms.length === serverTransformResults.length);\n\n  for (let i = 0; i < serverTransformResults.length; i++) {\n    const fieldTransform = fieldTransforms[i];\n    const transform = fieldTransform.transform;\n    const previousValue = mutableDocument.data.field(fieldTransform.field);\n    transformResults.set(fieldTransform.field, applyTransformOperationToRemoteDocument(transform, previousValue, serverTransformResults[i]));\n  }\n\n  return transformResults;\n}\n/**\r\n * Creates a list of \"transform results\" (a transform result is a field value\r\n * representing the result of applying a transform) for use when applying a\r\n * transform locally.\r\n *\r\n * @param fieldTransforms - The field transforms to apply the result to.\r\n * @param localWriteTime - The local time of the mutation (used to\r\n *     generate ServerTimestampValues).\r\n * @param mutableDocument - The document to apply transforms on.\r\n * @returns The transform results list.\r\n */\n\n\nfunction localTransformResults(fieldTransforms, localWriteTime, mutableDocument) {\n  const transformResults = new Map();\n\n  for (const fieldTransform of fieldTransforms) {\n    const transform = fieldTransform.transform;\n    const previousValue = mutableDocument.data.field(fieldTransform.field);\n    transformResults.set(fieldTransform.field, applyTransformOperationToLocalView(transform, previousValue, localWriteTime));\n  }\n\n  return transformResults;\n}\n/** A mutation that deletes the document at the given key. */\n\n\nclass DeleteMutation extends Mutation {\n  constructor(key, precondition) {\n    super();\n    this.key = key;\n    this.precondition = precondition;\n    this.type = 2\n    /* MutationType.Delete */\n    ;\n    this.fieldTransforms = [];\n  }\n\n  getFieldMask() {\n    return null;\n  }\n\n}\n\nfunction deleteMutationApplyToRemoteDocument(mutation, document, mutationResult) {\n  // Unlike applyToLocalView, if we're applying a mutation to a remote\n  // document the server has accepted the mutation so the precondition must\n  // have held.\n  document.convertToNoDocument(mutationResult.version).setHasCommittedMutations();\n}\n\nfunction deleteMutationApplyToLocalView(mutation, document, previousMask) {\n  if (preconditionIsValidForDocument(mutation.precondition, document)) {\n    document.convertToNoDocument(document.version).setHasLocalMutations();\n    return null;\n  }\n\n  return previousMask;\n}\n/**\r\n * A mutation that verifies the existence of the document at the given key with\r\n * the provided precondition.\r\n *\r\n * The `verify` operation is only used in Transactions, and this class serves\r\n * primarily to facilitate serialization into protos.\r\n */\n\n\nclass VerifyMutation extends Mutation {\n  constructor(key, precondition) {\n    super();\n    this.key = key;\n    this.precondition = precondition;\n    this.type = 3\n    /* MutationType.Verify */\n    ;\n    this.fieldTransforms = [];\n  }\n\n  getFieldMask() {\n    return null;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A batch of mutations that will be sent as one unit to the backend.\r\n */\n\n\nclass MutationBatch {\n  /**\r\n   * @param batchId - The unique ID of this mutation batch.\r\n   * @param localWriteTime - The original write time of this mutation.\r\n   * @param baseMutations - Mutations that are used to populate the base\r\n   * values when this mutation is applied locally. This can be used to locally\r\n   * overwrite values that are persisted in the remote document cache. Base\r\n   * mutations are never sent to the backend.\r\n   * @param mutations - The user-provided mutations in this mutation batch.\r\n   * User-provided mutations are applied both locally and remotely on the\r\n   * backend.\r\n   */\n  constructor(batchId, localWriteTime, baseMutations, mutations) {\n    this.batchId = batchId;\n    this.localWriteTime = localWriteTime;\n    this.baseMutations = baseMutations;\n    this.mutations = mutations;\n  }\n  /**\r\n   * Applies all the mutations in this MutationBatch to the specified document\r\n   * to compute the state of the remote document\r\n   *\r\n   * @param document - The document to apply mutations to.\r\n   * @param batchResult - The result of applying the MutationBatch to the\r\n   * backend.\r\n   */\n\n\n  applyToRemoteDocument(document, batchResult) {\n    const mutationResults = batchResult.mutationResults;\n\n    for (let i = 0; i < this.mutations.length; i++) {\n      const mutation = this.mutations[i];\n\n      if (mutation.key.isEqual(document.key)) {\n        const mutationResult = mutationResults[i];\n        mutationApplyToRemoteDocument(mutation, document, mutationResult);\n      }\n    }\n  }\n  /**\r\n   * Computes the local view of a document given all the mutations in this\r\n   * batch.\r\n   *\r\n   * @param document - The document to apply mutations to.\r\n   * @param mutatedFields - Fields that have been updated before applying this mutation batch.\r\n   * @returns A `FieldMask` representing all the fields that are mutated.\r\n   */\n\n\n  applyToLocalView(document, mutatedFields) {\n    // First, apply the base state. This allows us to apply non-idempotent\n    // transform against a consistent set of values.\n    for (const mutation of this.baseMutations) {\n      if (mutation.key.isEqual(document.key)) {\n        mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\n      }\n    } // Second, apply all user-provided mutations.\n\n\n    for (const mutation of this.mutations) {\n      if (mutation.key.isEqual(document.key)) {\n        mutatedFields = mutationApplyToLocalView(mutation, document, mutatedFields, this.localWriteTime);\n      }\n    }\n\n    return mutatedFields;\n  }\n  /**\r\n   * Computes the local view for all provided documents given the mutations in\r\n   * this batch. Returns a `DocumentKey` to `Mutation` map which can be used to\r\n   * replace all the mutation applications.\r\n   */\n\n\n  applyToLocalDocumentSet(documentMap, documentsWithoutRemoteVersion) {\n    // TODO(mrschmidt): This implementation is O(n^2). If we apply the mutations\n    // directly (as done in `applyToLocalView()`), we can reduce the complexity\n    // to O(n).\n    const overlays = newMutationMap();\n    this.mutations.forEach(m => {\n      const overlayedDocument = documentMap.get(m.key); // TODO(mutabledocuments): This method should take a MutableDocumentMap\n      // and we should remove this cast.\n\n      const mutableDocument = overlayedDocument.overlayedDocument;\n      let mutatedFields = this.applyToLocalView(mutableDocument, overlayedDocument.mutatedFields); // Set mutatedFields to null if the document is only from local mutations.\n      // This creates a Set or Delete mutation, instead of trying to create a\n      // patch mutation as the overlay.\n\n      mutatedFields = documentsWithoutRemoteVersion.has(m.key) ? null : mutatedFields;\n      const overlay = calculateOverlayMutation(mutableDocument, mutatedFields);\n\n      if (overlay !== null) {\n        overlays.set(m.key, overlay);\n      }\n\n      if (!mutableDocument.isValidDocument()) {\n        mutableDocument.convertToNoDocument(SnapshotVersion.min());\n      }\n    });\n    return overlays;\n  }\n\n  keys() {\n    return this.mutations.reduce((keys, m) => keys.add(m.key), documentKeySet());\n  }\n\n  isEqual(other) {\n    return this.batchId === other.batchId && arrayEquals(this.mutations, other.mutations, (l, r) => mutationEquals(l, r)) && arrayEquals(this.baseMutations, other.baseMutations, (l, r) => mutationEquals(l, r));\n  }\n\n}\n/** The result of applying a mutation batch to the backend. */\n\n\nclass MutationBatchResult {\n  constructor(batch, commitVersion, mutationResults,\n  /**\r\n   * A pre-computed mapping from each mutated document to the resulting\r\n   * version.\r\n   */\n  docVersions) {\n    this.batch = batch;\n    this.commitVersion = commitVersion;\n    this.mutationResults = mutationResults;\n    this.docVersions = docVersions;\n  }\n  /**\r\n   * Creates a new MutationBatchResult for the given batch and results. There\r\n   * must be one result for each mutation in the batch. This static factory\r\n   * caches a document=&gt;version mapping (docVersions).\r\n   */\n\n\n  static from(batch, commitVersion, results) {\n    hardAssert(batch.mutations.length === results.length);\n    let versionMap = documentVersionMap();\n    const mutations = batch.mutations;\n\n    for (let i = 0; i < mutations.length; i++) {\n      versionMap = versionMap.insert(mutations[i].key, results[i].version);\n    }\n\n    return new MutationBatchResult(batch, commitVersion, results, versionMap);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Representation of an overlay computed by Firestore.\r\n *\r\n * Holds information about a mutation and the largest batch id in Firestore when\r\n * the mutation was created.\r\n */\n\n\nclass Overlay {\n  constructor(largestBatchId, mutation) {\n    this.largestBatchId = largestBatchId;\n    this.mutation = mutation;\n  }\n\n  getKey() {\n    return this.mutation.key;\n  }\n\n  isEqual(other) {\n    return other !== null && this.mutation === other.mutation;\n  }\n\n  toString() {\n    return `Overlay{\n      largestBatchId: ${this.largestBatchId},\n      mutation: ${this.mutation.toString()}\n    }`;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass ExistenceFilter {\n  constructor(count, unchangedNames) {\n    this.count = count;\n    this.unchangedNames = unchangedNames;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Error Codes describing the different ways GRPC can fail. These are copied\r\n * directly from GRPC's sources here:\r\n *\r\n * https://github.com/grpc/grpc/blob/bceec94ea4fc5f0085d81235d8e1c06798dc341a/include/grpc%2B%2B/impl/codegen/status_code_enum.h\r\n *\r\n * Important! The names of these identifiers matter because the string forms\r\n * are used for reverse lookups from the webchannel stream. Do NOT change the\r\n * names of these identifiers or change this into a const enum.\r\n */\n\n\nvar RpcCode;\n\n(function (RpcCode) {\n  RpcCode[RpcCode[\"OK\"] = 0] = \"OK\";\n  RpcCode[RpcCode[\"CANCELLED\"] = 1] = \"CANCELLED\";\n  RpcCode[RpcCode[\"UNKNOWN\"] = 2] = \"UNKNOWN\";\n  RpcCode[RpcCode[\"INVALID_ARGUMENT\"] = 3] = \"INVALID_ARGUMENT\";\n  RpcCode[RpcCode[\"DEADLINE_EXCEEDED\"] = 4] = \"DEADLINE_EXCEEDED\";\n  RpcCode[RpcCode[\"NOT_FOUND\"] = 5] = \"NOT_FOUND\";\n  RpcCode[RpcCode[\"ALREADY_EXISTS\"] = 6] = \"ALREADY_EXISTS\";\n  RpcCode[RpcCode[\"PERMISSION_DENIED\"] = 7] = \"PERMISSION_DENIED\";\n  RpcCode[RpcCode[\"UNAUTHENTICATED\"] = 16] = \"UNAUTHENTICATED\";\n  RpcCode[RpcCode[\"RESOURCE_EXHAUSTED\"] = 8] = \"RESOURCE_EXHAUSTED\";\n  RpcCode[RpcCode[\"FAILED_PRECONDITION\"] = 9] = \"FAILED_PRECONDITION\";\n  RpcCode[RpcCode[\"ABORTED\"] = 10] = \"ABORTED\";\n  RpcCode[RpcCode[\"OUT_OF_RANGE\"] = 11] = \"OUT_OF_RANGE\";\n  RpcCode[RpcCode[\"UNIMPLEMENTED\"] = 12] = \"UNIMPLEMENTED\";\n  RpcCode[RpcCode[\"INTERNAL\"] = 13] = \"INTERNAL\";\n  RpcCode[RpcCode[\"UNAVAILABLE\"] = 14] = \"UNAVAILABLE\";\n  RpcCode[RpcCode[\"DATA_LOSS\"] = 15] = \"DATA_LOSS\";\n})(RpcCode || (RpcCode = {}));\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a non-write operation.\r\n *\r\n * See isPermanentWriteError for classifying write errors.\r\n */\n\n\nfunction isPermanentError(code) {\n  switch (code) {\n    case Code.OK:\n      return fail();\n\n    case Code.CANCELLED:\n    case Code.UNKNOWN:\n    case Code.DEADLINE_EXCEEDED:\n    case Code.RESOURCE_EXHAUSTED:\n    case Code.INTERNAL:\n    case Code.UNAVAILABLE: // Unauthenticated means something went wrong with our token and we need\n    // to retry with new credentials which will happen automatically.\n\n    case Code.UNAUTHENTICATED:\n      return false;\n\n    case Code.INVALID_ARGUMENT:\n    case Code.NOT_FOUND:\n    case Code.ALREADY_EXISTS:\n    case Code.PERMISSION_DENIED:\n    case Code.FAILED_PRECONDITION: // Aborted might be retried in some scenarios, but that is dependant on\n    // the context and should handled individually by the calling code.\n    // See https://cloud.google.com/apis/design/errors.\n\n    case Code.ABORTED:\n    case Code.OUT_OF_RANGE:\n    case Code.UNIMPLEMENTED:\n    case Code.DATA_LOSS:\n      return true;\n\n    default:\n      return fail();\n  }\n}\n/**\r\n * Determines whether an error code represents a permanent error when received\r\n * in response to a write operation.\r\n *\r\n * Write operations must be handled specially because as of b/119437764, ABORTED\r\n * errors on the write stream should be retried too (even though ABORTED errors\r\n * are not generally retryable).\r\n *\r\n * Note that during the initial handshake on the write stream an ABORTED error\r\n * signals that we should discard our stream token (i.e. it is permanent). This\r\n * means a handshake error should be classified with isPermanentError, above.\r\n */\n\n\nfunction isPermanentWriteError(code) {\n  return isPermanentError(code) && code !== Code.ABORTED;\n}\n/**\r\n * Maps an error Code from GRPC status code number, like 0, 1, or 14. These\r\n * are not the same as HTTP status codes.\r\n *\r\n * @returns The Code equivalent to the given GRPC status code. Fails if there\r\n *     is no match.\r\n */\n\n\nfunction mapCodeFromRpcCode(code) {\n  if (code === undefined) {\n    // This shouldn't normally happen, but in certain error cases (like trying\n    // to send invalid proto messages) we may get an error with no GRPC code.\n    logError('GRPC error has no .code');\n    return Code.UNKNOWN;\n  }\n\n  switch (code) {\n    case RpcCode.OK:\n      return Code.OK;\n\n    case RpcCode.CANCELLED:\n      return Code.CANCELLED;\n\n    case RpcCode.UNKNOWN:\n      return Code.UNKNOWN;\n\n    case RpcCode.DEADLINE_EXCEEDED:\n      return Code.DEADLINE_EXCEEDED;\n\n    case RpcCode.RESOURCE_EXHAUSTED:\n      return Code.RESOURCE_EXHAUSTED;\n\n    case RpcCode.INTERNAL:\n      return Code.INTERNAL;\n\n    case RpcCode.UNAVAILABLE:\n      return Code.UNAVAILABLE;\n\n    case RpcCode.UNAUTHENTICATED:\n      return Code.UNAUTHENTICATED;\n\n    case RpcCode.INVALID_ARGUMENT:\n      return Code.INVALID_ARGUMENT;\n\n    case RpcCode.NOT_FOUND:\n      return Code.NOT_FOUND;\n\n    case RpcCode.ALREADY_EXISTS:\n      return Code.ALREADY_EXISTS;\n\n    case RpcCode.PERMISSION_DENIED:\n      return Code.PERMISSION_DENIED;\n\n    case RpcCode.FAILED_PRECONDITION:\n      return Code.FAILED_PRECONDITION;\n\n    case RpcCode.ABORTED:\n      return Code.ABORTED;\n\n    case RpcCode.OUT_OF_RANGE:\n      return Code.OUT_OF_RANGE;\n\n    case RpcCode.UNIMPLEMENTED:\n      return Code.UNIMPLEMENTED;\n\n    case RpcCode.DATA_LOSS:\n      return Code.DATA_LOSS;\n\n    default:\n      return fail();\n  }\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An error encountered while decoding base64 string.\r\n */\n\n\nclass Base64DecodeError extends Error {\n  constructor() {\n    super(...arguments);\n    this.name = 'Base64DecodeError';\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Manages \"testing hooks\", hooks into the internals of the SDK to verify\r\n * internal state and events during integration tests. Do not use this class\r\n * except for testing purposes.\r\n *\r\n * There are two ways to retrieve the global singleton instance of this class:\r\n * 1. The `instance` property, which returns null if the global singleton\r\n *      instance has not been created. Use this property if the caller should\r\n *      \"do nothing\" if there are no testing hooks registered, such as when\r\n *      delivering an event to notify registered callbacks.\r\n * 2. The `getOrCreateInstance()` method, which creates the global singleton\r\n *      instance if it has not been created. Use this method if the instance is\r\n *      needed to, for example, register a callback.\r\n *\r\n * @internal\r\n */\n\n\nclass TestingHooks {\n  constructor() {\n    this.onExistenceFilterMismatchCallbacks = new Map();\n  }\n  /**\r\n   * Returns the singleton instance of this class, or null if it has not been\r\n   * initialized.\r\n   */\n\n\n  static get instance() {\n    return gTestingHooksSingletonInstance;\n  }\n  /**\r\n   * Returns the singleton instance of this class, creating it if is has never\r\n   * been created before.\r\n   */\n\n\n  static getOrCreateInstance() {\n    if (gTestingHooksSingletonInstance === null) {\n      gTestingHooksSingletonInstance = new TestingHooks();\n    }\n\n    return gTestingHooksSingletonInstance;\n  }\n  /**\r\n   * Registers a callback to be notified when an existence filter mismatch\r\n   * occurs in the Watch listen stream.\r\n   *\r\n   * The relative order in which callbacks are notified is unspecified; do not\r\n   * rely on any particular ordering. If a given callback is registered multiple\r\n   * times then it will be notified multiple times, once per registration.\r\n   *\r\n   * @param callback the callback to invoke upon existence filter mismatch.\r\n   *\r\n   * @return a function that, when called, unregisters the given callback; only\r\n   * the first invocation of the returned function does anything; all subsequent\r\n   * invocations do nothing.\r\n   */\n\n\n  onExistenceFilterMismatch(callback) {\n    const key = Symbol();\n    this.onExistenceFilterMismatchCallbacks.set(key, callback);\n    return () => this.onExistenceFilterMismatchCallbacks.delete(key);\n  }\n  /**\r\n   * Invokes all currently-registered `onExistenceFilterMismatch` callbacks.\r\n   * @param info Information about the existence filter mismatch.\r\n   */\n\n\n  notifyOnExistenceFilterMismatch(info) {\n    this.onExistenceFilterMismatchCallbacks.forEach(callback => callback(info));\n  }\n\n}\n/** The global singleton instance of `TestingHooks`. */\n\n\nlet gTestingHooksSingletonInstance = null;\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An instance of the Platform's 'TextEncoder' implementation.\r\n */\n\nfunction newTextEncoder() {\n  return new TextEncoder();\n}\n/**\r\n * An instance of the Platform's 'TextDecoder' implementation.\r\n */\n\n\nfunction newTextDecoder() {\n  return new TextDecoder('utf-8');\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst MAX_64_BIT_UNSIGNED_INTEGER = new Integer([0xffffffff, 0xffffffff], 0); // Hash a string using md5 hashing algorithm.\n\nfunction getMd5HashValue(value) {\n  const encodedValue = newTextEncoder().encode(value);\n  const md5 = new Md5();\n  md5.update(encodedValue);\n  return new Uint8Array(md5.digest());\n} // Interpret the 16 bytes array as two 64-bit unsigned integers, encoded using\n// 2’s complement using little endian.\n\n\nfunction get64BitUints(Bytes) {\n  const dataView = new DataView(Bytes.buffer);\n  const chunk1 = dataView.getUint32(0,\n  /* littleEndian= */\n  true);\n  const chunk2 = dataView.getUint32(4,\n  /* littleEndian= */\n  true);\n  const chunk3 = dataView.getUint32(8,\n  /* littleEndian= */\n  true);\n  const chunk4 = dataView.getUint32(12,\n  /* littleEndian= */\n  true);\n  const integer1 = new Integer([chunk1, chunk2], 0);\n  const integer2 = new Integer([chunk3, chunk4], 0);\n  return [integer1, integer2];\n}\n\nclass BloomFilter {\n  constructor(bitmap, padding, hashCount) {\n    this.bitmap = bitmap;\n    this.padding = padding;\n    this.hashCount = hashCount;\n\n    if (padding < 0 || padding >= 8) {\n      throw new BloomFilterError(`Invalid padding: ${padding}`);\n    }\n\n    if (hashCount < 0) {\n      throw new BloomFilterError(`Invalid hash count: ${hashCount}`);\n    }\n\n    if (bitmap.length > 0 && this.hashCount === 0) {\n      // Only empty bloom filter can have 0 hash count.\n      throw new BloomFilterError(`Invalid hash count: ${hashCount}`);\n    }\n\n    if (bitmap.length === 0 && padding !== 0) {\n      // Empty bloom filter should have 0 padding.\n      throw new BloomFilterError(`Invalid padding when bitmap length is 0: ${padding}`);\n    }\n\n    this.bitCount = bitmap.length * 8 - padding; // Set the bit count in Integer to avoid repetition in mightContain().\n\n    this.bitCountInInteger = Integer.fromNumber(this.bitCount);\n  } // Calculate the ith hash value based on the hashed 64bit integers,\n  // and calculate its corresponding bit index in the bitmap to be checked.\n\n\n  getBitIndex(num1, num2, hashIndex) {\n    // Calculate hashed value h(i) = h1 + (i * h2).\n    let hashValue = num1.add(num2.multiply(Integer.fromNumber(hashIndex))); // Wrap if hash value overflow 64bit.\n\n    if (hashValue.compare(MAX_64_BIT_UNSIGNED_INTEGER) === 1) {\n      hashValue = new Integer([hashValue.getBits(0), hashValue.getBits(1)], 0);\n    }\n\n    return hashValue.modulo(this.bitCountInInteger).toNumber();\n  } // Return whether the bit on the given index in the bitmap is set to 1.\n\n\n  isBitSet(index) {\n    // To retrieve bit n, calculate: (bitmap[n / 8] & (0x01 << (n % 8))).\n    const byte = this.bitmap[Math.floor(index / 8)];\n    const offset = index % 8;\n    return (byte & 0x01 << offset) !== 0;\n  }\n\n  mightContain(value) {\n    // Empty bitmap should always return false on membership check.\n    if (this.bitCount === 0) {\n      return false;\n    }\n\n    const md5HashedValue = getMd5HashValue(value);\n    const [hash1, hash2] = get64BitUints(md5HashedValue);\n\n    for (let i = 0; i < this.hashCount; i++) {\n      const index = this.getBitIndex(hash1, hash2, i);\n\n      if (!this.isBitSet(index)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n  /** Create bloom filter for testing purposes only. */\n\n\n  static create(bitCount, hashCount, contains) {\n    const padding = bitCount % 8 === 0 ? 0 : 8 - bitCount % 8;\n    const bitmap = new Uint8Array(Math.ceil(bitCount / 8));\n    const bloomFilter = new BloomFilter(bitmap, padding, hashCount);\n    contains.forEach(item => bloomFilter.insert(item));\n    return bloomFilter;\n  }\n\n  insert(value) {\n    if (this.bitCount === 0) {\n      return;\n    }\n\n    const md5HashedValue = getMd5HashValue(value);\n    const [hash1, hash2] = get64BitUints(md5HashedValue);\n\n    for (let i = 0; i < this.hashCount; i++) {\n      const index = this.getBitIndex(hash1, hash2, i);\n      this.setBit(index);\n    }\n  }\n\n  setBit(index) {\n    const indexOfByte = Math.floor(index / 8);\n    const offset = index % 8;\n    this.bitmap[indexOfByte] |= 0x01 << offset;\n  }\n\n}\n\nclass BloomFilterError extends Error {\n  constructor() {\n    super(...arguments);\n    this.name = 'BloomFilterError';\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An event from the RemoteStore. It is split into targetChanges (changes to the\r\n * state or the set of documents in our watched targets) and documentUpdates\r\n * (changes to the actual documents).\r\n */\n\n\nclass RemoteEvent {\n  constructor(\n  /**\r\n   * The snapshot version this event brings us up to, or MIN if not set.\r\n   */\n  snapshotVersion,\n  /**\r\n   * A map from target to changes to the target. See TargetChange.\r\n   */\n  targetChanges,\n  /**\r\n   * A map of targets that is known to be inconsistent, and the purpose for\r\n   * re-listening. Listens for these targets should be re-established without\r\n   * resume tokens.\r\n   */\n  targetMismatches,\n  /**\r\n   * A set of which documents have changed or been deleted, along with the\r\n   * doc's new values (if not deleted).\r\n   */\n  documentUpdates,\n  /**\r\n   * A set of which document updates are due only to limbo resolution targets.\r\n   */\n  resolvedLimboDocuments) {\n    this.snapshotVersion = snapshotVersion;\n    this.targetChanges = targetChanges;\n    this.targetMismatches = targetMismatches;\n    this.documentUpdates = documentUpdates;\n    this.resolvedLimboDocuments = resolvedLimboDocuments;\n  }\n  /**\r\n   * HACK: Views require RemoteEvents in order to determine whether the view is\r\n   * CURRENT, but secondary tabs don't receive remote events. So this method is\r\n   * used to create a synthesized RemoteEvent that can be used to apply a\r\n   * CURRENT status change to a View, for queries executed in a different tab.\r\n   */\n  // PORTING NOTE: Multi-tab only\n\n\n  static createSynthesizedRemoteEventForCurrentChange(targetId, current, resumeToken) {\n    const targetChanges = new Map();\n    targetChanges.set(targetId, TargetChange.createSynthesizedTargetChangeForCurrentChange(targetId, current, resumeToken));\n    return new RemoteEvent(SnapshotVersion.min(), targetChanges, new SortedMap(primitiveComparator), mutableDocumentMap(), documentKeySet());\n  }\n\n}\n/**\r\n * A TargetChange specifies the set of changes for a specific target as part of\r\n * a RemoteEvent. These changes track which documents are added, modified or\r\n * removed, as well as the target's resume token and whether the target is\r\n * marked CURRENT.\r\n * The actual changes *to* documents are not part of the TargetChange since\r\n * documents may be part of multiple targets.\r\n */\n\n\nclass TargetChange {\n  constructor(\n  /**\r\n   * An opaque, server-assigned token that allows watching a query to be resumed\r\n   * after disconnecting without retransmitting all the data that matches the\r\n   * query. The resume token essentially identifies a point in time from which\r\n   * the server should resume sending results.\r\n   */\n  resumeToken,\n  /**\r\n   * The \"current\" (synced) status of this target. Note that \"current\"\r\n   * has special meaning in the RPC protocol that implies that a target is\r\n   * both up-to-date and consistent with the rest of the watch stream.\r\n   */\n  current,\n  /**\r\n   * The set of documents that were newly assigned to this target as part of\r\n   * this remote event.\r\n   */\n  addedDocuments,\n  /**\r\n   * The set of documents that were already assigned to this target but received\r\n   * an update during this remote event.\r\n   */\n  modifiedDocuments,\n  /**\r\n   * The set of documents that were removed from this target as part of this\r\n   * remote event.\r\n   */\n  removedDocuments) {\n    this.resumeToken = resumeToken;\n    this.current = current;\n    this.addedDocuments = addedDocuments;\n    this.modifiedDocuments = modifiedDocuments;\n    this.removedDocuments = removedDocuments;\n  }\n  /**\r\n   * This method is used to create a synthesized TargetChanges that can be used to\r\n   * apply a CURRENT status change to a View (for queries executed in a different\r\n   * tab) or for new queries (to raise snapshots with correct CURRENT status).\r\n   */\n\n\n  static createSynthesizedTargetChangeForCurrentChange(targetId, current, resumeToken) {\n    return new TargetChange(resumeToken, current, documentKeySet(), documentKeySet(), documentKeySet());\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents a changed document and a list of target ids to which this change\r\n * applies.\r\n *\r\n * If document has been deleted NoDocument will be provided.\r\n */\n\n\nclass DocumentWatchChange {\n  constructor(\n  /** The new document applies to all of these targets. */\n  updatedTargetIds,\n  /** The new document is removed from all of these targets. */\n  removedTargetIds,\n  /** The key of the document for this change. */\n  key,\n  /**\r\n   * The new document or NoDocument if it was deleted. Is null if the\r\n   * document went out of view without the server sending a new document.\r\n   */\n  newDoc) {\n    this.updatedTargetIds = updatedTargetIds;\n    this.removedTargetIds = removedTargetIds;\n    this.key = key;\n    this.newDoc = newDoc;\n  }\n\n}\n\nclass ExistenceFilterChange {\n  constructor(targetId, existenceFilter) {\n    this.targetId = targetId;\n    this.existenceFilter = existenceFilter;\n  }\n\n}\n\nclass WatchTargetChange {\n  constructor(\n  /** What kind of change occurred to the watch target. */\n  state,\n  /** The target IDs that were added/removed/set. */\n  targetIds,\n  /**\r\n   * An opaque, server-assigned token that allows watching a target to be\r\n   * resumed after disconnecting without retransmitting all the data that\r\n   * matches the target. The resume token essentially identifies a point in\r\n   * time from which the server should resume sending results.\r\n   */\n  resumeToken = ByteString.EMPTY_BYTE_STRING,\n  /** An RPC error indicating why the watch failed. */\n  cause = null) {\n    this.state = state;\n    this.targetIds = targetIds;\n    this.resumeToken = resumeToken;\n    this.cause = cause;\n  }\n\n}\n/** Tracks the internal state of a Watch target. */\n\n\nclass TargetState {\n  constructor() {\n    /**\r\n     * The number of pending responses (adds or removes) that we are waiting on.\r\n     * We only consider targets active that have no pending responses.\r\n     */\n    this.pendingResponses = 0;\n    /**\r\n     * Keeps track of the document changes since the last raised snapshot.\r\n     *\r\n     * These changes are continuously updated as we receive document updates and\r\n     * always reflect the current set of changes against the last issued snapshot.\r\n     */\n\n    this.documentChanges = snapshotChangesMap();\n    /** See public getters for explanations of these fields. */\n\n    this._resumeToken = ByteString.EMPTY_BYTE_STRING;\n    this._current = false;\n    /**\r\n     * Whether this target state should be included in the next snapshot. We\r\n     * initialize to true so that newly-added targets are included in the next\r\n     * RemoteEvent.\r\n     */\n\n    this._hasPendingChanges = true;\n  }\n  /**\r\n   * Whether this target has been marked 'current'.\r\n   *\r\n   * 'Current' has special meaning in the RPC protocol: It implies that the\r\n   * Watch backend has sent us all changes up to the point at which the target\r\n   * was added and that the target is consistent with the rest of the watch\r\n   * stream.\r\n   */\n\n\n  get current() {\n    return this._current;\n  }\n  /** The last resume token sent to us for this target. */\n\n\n  get resumeToken() {\n    return this._resumeToken;\n  }\n  /** Whether this target has pending target adds or target removes. */\n\n\n  get isPending() {\n    return this.pendingResponses !== 0;\n  }\n  /** Whether we have modified any state that should trigger a snapshot. */\n\n\n  get hasPendingChanges() {\n    return this._hasPendingChanges;\n  }\n  /**\r\n   * Applies the resume token to the TargetChange, but only when it has a new\r\n   * value. Empty resumeTokens are discarded.\r\n   */\n\n\n  updateResumeToken(resumeToken) {\n    if (resumeToken.approximateByteSize() > 0) {\n      this._hasPendingChanges = true;\n      this._resumeToken = resumeToken;\n    }\n  }\n  /**\r\n   * Creates a target change from the current set of changes.\r\n   *\r\n   * To reset the document changes after raising this snapshot, call\r\n   * `clearPendingChanges()`.\r\n   */\n\n\n  toTargetChange() {\n    let addedDocuments = documentKeySet();\n    let modifiedDocuments = documentKeySet();\n    let removedDocuments = documentKeySet();\n    this.documentChanges.forEach((key, changeType) => {\n      switch (changeType) {\n        case 0\n        /* ChangeType.Added */\n        :\n          addedDocuments = addedDocuments.add(key);\n          break;\n\n        case 2\n        /* ChangeType.Modified */\n        :\n          modifiedDocuments = modifiedDocuments.add(key);\n          break;\n\n        case 1\n        /* ChangeType.Removed */\n        :\n          removedDocuments = removedDocuments.add(key);\n          break;\n\n        default:\n          fail();\n      }\n    });\n    return new TargetChange(this._resumeToken, this._current, addedDocuments, modifiedDocuments, removedDocuments);\n  }\n  /**\r\n   * Resets the document changes and sets `hasPendingChanges` to false.\r\n   */\n\n\n  clearPendingChanges() {\n    this._hasPendingChanges = false;\n    this.documentChanges = snapshotChangesMap();\n  }\n\n  addDocumentChange(key, changeType) {\n    this._hasPendingChanges = true;\n    this.documentChanges = this.documentChanges.insert(key, changeType);\n  }\n\n  removeDocumentChange(key) {\n    this._hasPendingChanges = true;\n    this.documentChanges = this.documentChanges.remove(key);\n  }\n\n  recordPendingTargetRequest() {\n    this.pendingResponses += 1;\n  }\n\n  recordTargetResponse() {\n    this.pendingResponses -= 1;\n  }\n\n  markCurrent() {\n    this._hasPendingChanges = true;\n    this._current = true;\n  }\n\n}\n\nconst LOG_TAG$g = 'WatchChangeAggregator';\n/**\r\n * A helper class to accumulate watch changes into a RemoteEvent.\r\n */\n\nclass WatchChangeAggregator {\n  constructor(metadataProvider) {\n    this.metadataProvider = metadataProvider;\n    /** The internal state of all tracked targets. */\n\n    this.targetStates = new Map();\n    /** Keeps track of the documents to update since the last raised snapshot. */\n\n    this.pendingDocumentUpdates = mutableDocumentMap();\n    /** A mapping of document keys to their set of target IDs. */\n\n    this.pendingDocumentTargetMapping = documentTargetMap();\n    /**\r\n     * A map of targets with existence filter mismatches. These targets are\r\n     * known to be inconsistent and their listens needs to be re-established by\r\n     * RemoteStore.\r\n     */\n\n    this.pendingTargetResets = new SortedMap(primitiveComparator);\n  }\n  /**\r\n   * Processes and adds the DocumentWatchChange to the current set of changes.\r\n   */\n\n\n  handleDocumentChange(docChange) {\n    for (const targetId of docChange.updatedTargetIds) {\n      if (docChange.newDoc && docChange.newDoc.isFoundDocument()) {\n        this.addDocumentToTarget(targetId, docChange.newDoc);\n      } else {\n        this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\n      }\n    }\n\n    for (const targetId of docChange.removedTargetIds) {\n      this.removeDocumentFromTarget(targetId, docChange.key, docChange.newDoc);\n    }\n  }\n  /** Processes and adds the WatchTargetChange to the current set of changes. */\n\n\n  handleTargetChange(targetChange) {\n    this.forEachTarget(targetChange, targetId => {\n      const targetState = this.ensureTargetState(targetId);\n\n      switch (targetChange.state) {\n        case 0\n        /* WatchTargetChangeState.NoChange */\n        :\n          if (this.isActiveTarget(targetId)) {\n            targetState.updateResumeToken(targetChange.resumeToken);\n          }\n\n          break;\n\n        case 1\n        /* WatchTargetChangeState.Added */\n        :\n          // We need to decrement the number of pending acks needed from watch\n          // for this targetId.\n          targetState.recordTargetResponse();\n\n          if (!targetState.isPending) {\n            // We have a freshly added target, so we need to reset any state\n            // that we had previously. This can happen e.g. when remove and add\n            // back a target for existence filter mismatches.\n            targetState.clearPendingChanges();\n          }\n\n          targetState.updateResumeToken(targetChange.resumeToken);\n          break;\n\n        case 2\n        /* WatchTargetChangeState.Removed */\n        :\n          // We need to keep track of removed targets to we can post-filter and\n          // remove any target changes.\n          // We need to decrement the number of pending acks needed from watch\n          // for this targetId.\n          targetState.recordTargetResponse();\n\n          if (!targetState.isPending) {\n            this.removeTarget(targetId);\n          }\n\n          break;\n\n        case 3\n        /* WatchTargetChangeState.Current */\n        :\n          if (this.isActiveTarget(targetId)) {\n            targetState.markCurrent();\n            targetState.updateResumeToken(targetChange.resumeToken);\n          }\n\n          break;\n\n        case 4\n        /* WatchTargetChangeState.Reset */\n        :\n          if (this.isActiveTarget(targetId)) {\n            // Reset the target and synthesizes removes for all existing\n            // documents. The backend will re-add any documents that still\n            // match the target before it sends the next global snapshot.\n            this.resetTarget(targetId);\n            targetState.updateResumeToken(targetChange.resumeToken);\n          }\n\n          break;\n\n        default:\n          fail();\n      }\n    });\n  }\n  /**\r\n   * Iterates over all targetIds that the watch change applies to: either the\r\n   * targetIds explicitly listed in the change or the targetIds of all currently\r\n   * active targets.\r\n   */\n\n\n  forEachTarget(targetChange, fn) {\n    if (targetChange.targetIds.length > 0) {\n      targetChange.targetIds.forEach(fn);\n    } else {\n      this.targetStates.forEach((_, targetId) => {\n        if (this.isActiveTarget(targetId)) {\n          fn(targetId);\n        }\n      });\n    }\n  }\n  /**\r\n   * Handles existence filters and synthesizes deletes for filter mismatches.\r\n   * Targets that are invalidated by filter mismatches are added to\r\n   * `pendingTargetResets`.\r\n   */\n\n\n  handleExistenceFilter(watchChange) {\n    var _a;\n\n    const targetId = watchChange.targetId;\n    const expectedCount = watchChange.existenceFilter.count;\n    const targetData = this.targetDataForActiveTarget(targetId);\n\n    if (targetData) {\n      const target = targetData.target;\n\n      if (targetIsDocumentTarget(target)) {\n        if (expectedCount === 0) {\n          // The existence filter told us the document does not exist. We deduce\n          // that this document does not exist and apply a deleted document to\n          // our updates. Without applying this deleted document there might be\n          // another query that will raise this document as part of a snapshot\n          // until it is resolved, essentially exposing inconsistency between\n          // queries.\n          const key = new DocumentKey(target.path);\n          this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, SnapshotVersion.min()));\n        } else {\n          hardAssert(expectedCount === 1);\n        }\n      } else {\n        const currentSize = this.getCurrentDocumentCountForTarget(targetId); // Existence filter mismatch. Mark the documents as being in limbo, and\n        // raise a snapshot with `isFromCache:true`.\n\n        if (currentSize !== expectedCount) {\n          // Apply bloom filter to identify and mark removed documents.\n          const status = this.applyBloomFilter(watchChange, currentSize);\n\n          if (status !== 0\n          /* BloomFilterApplicationStatus.Success */\n          ) {\n            // If bloom filter application fails, we reset the mapping and\n            // trigger re-run of the query.\n            this.resetTarget(targetId);\n            const purpose = status === 2\n            /* BloomFilterApplicationStatus.FalsePositive */\n            ? \"TargetPurposeExistenceFilterMismatchBloom\"\n            /* TargetPurpose.ExistenceFilterMismatchBloom */\n            : \"TargetPurposeExistenceFilterMismatch\"\n            /* TargetPurpose.ExistenceFilterMismatch */\n            ;\n            this.pendingTargetResets = this.pendingTargetResets.insert(targetId, purpose);\n          }\n\n          (_a = TestingHooks.instance) === null || _a === void 0 ? void 0 : _a.notifyOnExistenceFilterMismatch(createExistenceFilterMismatchInfoForTestingHooks(status, currentSize, watchChange.existenceFilter));\n        }\n      }\n    }\n  }\n  /**\r\n   * Apply bloom filter to remove the deleted documents, and return the\r\n   * application status.\r\n   */\n\n\n  applyBloomFilter(watchChange, currentCount) {\n    const {\n      unchangedNames,\n      count: expectedCount\n    } = watchChange.existenceFilter;\n\n    if (!unchangedNames || !unchangedNames.bits) {\n      return 1\n      /* BloomFilterApplicationStatus.Skipped */\n      ;\n    }\n\n    const {\n      bits: {\n        bitmap = '',\n        padding = 0\n      },\n      hashCount = 0\n    } = unchangedNames;\n    let normalizedBitmap;\n\n    try {\n      normalizedBitmap = normalizeByteString(bitmap).toUint8Array();\n    } catch (err) {\n      if (err instanceof Base64DecodeError) {\n        logWarn('Decoding the base64 bloom filter in existence filter failed (' + err.message + '); ignoring the bloom filter and falling back to full re-query.');\n        return 1\n        /* BloomFilterApplicationStatus.Skipped */\n        ;\n      } else {\n        throw err;\n      }\n    }\n\n    let bloomFilter;\n\n    try {\n      // BloomFilter throws error if the inputs are invalid.\n      bloomFilter = new BloomFilter(normalizedBitmap, padding, hashCount);\n    } catch (err) {\n      if (err instanceof BloomFilterError) {\n        logWarn('BloomFilter error: ', err);\n      } else {\n        logWarn('Applying bloom filter failed: ', err);\n      }\n\n      return 1\n      /* BloomFilterApplicationStatus.Skipped */\n      ;\n    }\n\n    if (bloomFilter.bitCount === 0) {\n      return 1\n      /* BloomFilterApplicationStatus.Skipped */\n      ;\n    }\n\n    const removedDocumentCount = this.filterRemovedDocuments(watchChange.targetId, bloomFilter);\n\n    if (expectedCount !== currentCount - removedDocumentCount) {\n      return 2\n      /* BloomFilterApplicationStatus.FalsePositive */\n      ;\n    }\n\n    return 0\n    /* BloomFilterApplicationStatus.Success */\n    ;\n  }\n  /**\r\n   * Filter out removed documents based on bloom filter membership result and\r\n   * return number of documents removed.\r\n   */\n\n\n  filterRemovedDocuments(targetId, bloomFilter) {\n    const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\n    let removalCount = 0;\n    existingKeys.forEach(key => {\n      const databaseId = this.metadataProvider.getDatabaseId();\n      const documentPath = `projects/${databaseId.projectId}/databases/${databaseId.database}/documents/${key.path.canonicalString()}`;\n\n      if (!bloomFilter.mightContain(documentPath)) {\n        this.removeDocumentFromTarget(targetId, key,\n        /*updatedDocument=*/\n        null);\n        removalCount++;\n      }\n    });\n    return removalCount;\n  }\n  /**\r\n   * Converts the currently accumulated state into a remote event at the\r\n   * provided snapshot version. Resets the accumulated changes before returning.\r\n   */\n\n\n  createRemoteEvent(snapshotVersion) {\n    const targetChanges = new Map();\n    this.targetStates.forEach((targetState, targetId) => {\n      const targetData = this.targetDataForActiveTarget(targetId);\n\n      if (targetData) {\n        if (targetState.current && targetIsDocumentTarget(targetData.target)) {\n          // Document queries for document that don't exist can produce an empty\n          // result set. To update our local cache, we synthesize a document\n          // delete if we have not previously received the document. This\n          // resolves the limbo state of the document, removing it from\n          // limboDocumentRefs.\n          //\n          // TODO(dimond): Ideally we would have an explicit lookup target\n          // instead resulting in an explicit delete message and we could\n          // remove this special logic.\n          const key = new DocumentKey(targetData.target.path);\n\n          if (this.pendingDocumentUpdates.get(key) === null && !this.targetContainsDocument(targetId, key)) {\n            this.removeDocumentFromTarget(targetId, key, MutableDocument.newNoDocument(key, snapshotVersion));\n          }\n        }\n\n        if (targetState.hasPendingChanges) {\n          targetChanges.set(targetId, targetState.toTargetChange());\n          targetState.clearPendingChanges();\n        }\n      }\n    });\n    let resolvedLimboDocuments = documentKeySet(); // We extract the set of limbo-only document updates as the GC logic\n    // special-cases documents that do not appear in the target cache.\n    //\n    // TODO(gsoltis): Expand on this comment once GC is available in the JS\n    // client.\n\n    this.pendingDocumentTargetMapping.forEach((key, targets) => {\n      let isOnlyLimboTarget = true;\n      targets.forEachWhile(targetId => {\n        const targetData = this.targetDataForActiveTarget(targetId);\n\n        if (targetData && targetData.purpose !== \"TargetPurposeLimboResolution\"\n        /* TargetPurpose.LimboResolution */\n        ) {\n          isOnlyLimboTarget = false;\n          return false;\n        }\n\n        return true;\n      });\n\n      if (isOnlyLimboTarget) {\n        resolvedLimboDocuments = resolvedLimboDocuments.add(key);\n      }\n    });\n    this.pendingDocumentUpdates.forEach((_, doc) => doc.setReadTime(snapshotVersion));\n    const remoteEvent = new RemoteEvent(snapshotVersion, targetChanges, this.pendingTargetResets, this.pendingDocumentUpdates, resolvedLimboDocuments);\n    this.pendingDocumentUpdates = mutableDocumentMap();\n    this.pendingDocumentTargetMapping = documentTargetMap();\n    this.pendingTargetResets = new SortedMap(primitiveComparator);\n    return remoteEvent;\n  }\n  /**\r\n   * Adds the provided document to the internal list of document updates and\r\n   * its document key to the given target's mapping.\r\n   */\n  // Visible for testing.\n\n\n  addDocumentToTarget(targetId, document) {\n    if (!this.isActiveTarget(targetId)) {\n      return;\n    }\n\n    const changeType = this.targetContainsDocument(targetId, document.key) ? 2\n    /* ChangeType.Modified */\n    : 0\n    /* ChangeType.Added */\n    ;\n    const targetState = this.ensureTargetState(targetId);\n    targetState.addDocumentChange(document.key, changeType);\n    this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(document.key, document);\n    this.pendingDocumentTargetMapping = this.pendingDocumentTargetMapping.insert(document.key, this.ensureDocumentTargetMapping(document.key).add(targetId));\n  }\n  /**\r\n   * Removes the provided document from the target mapping. If the\r\n   * document no longer matches the target, but the document's state is still\r\n   * known (e.g. we know that the document was deleted or we received the change\r\n   * that caused the filter mismatch), the new document can be provided\r\n   * to update the remote document cache.\r\n   */\n  // Visible for testing.\n\n\n  removeDocumentFromTarget(targetId, key, updatedDocument) {\n    if (!this.isActiveTarget(targetId)) {\n      return;\n    }\n\n    const targetState = this.ensureTargetState(targetId);\n\n    if (this.targetContainsDocument(targetId, key)) {\n      targetState.addDocumentChange(key, 1\n      /* ChangeType.Removed */\n      );\n    } else {\n      // The document may have entered and left the target before we raised a\n      // snapshot, so we can just ignore the change.\n      targetState.removeDocumentChange(key);\n    }\n\n    this.pendingDocumentTargetMapping = this.pendingDocumentTargetMapping.insert(key, this.ensureDocumentTargetMapping(key).delete(targetId));\n\n    if (updatedDocument) {\n      this.pendingDocumentUpdates = this.pendingDocumentUpdates.insert(key, updatedDocument);\n    }\n  }\n\n  removeTarget(targetId) {\n    this.targetStates.delete(targetId);\n  }\n  /**\r\n   * Returns the current count of documents in the target. This includes both\r\n   * the number of documents that the LocalStore considers to be part of the\r\n   * target as well as any accumulated changes.\r\n   */\n\n\n  getCurrentDocumentCountForTarget(targetId) {\n    const targetState = this.ensureTargetState(targetId);\n    const targetChange = targetState.toTargetChange();\n    return this.metadataProvider.getRemoteKeysForTarget(targetId).size + targetChange.addedDocuments.size - targetChange.removedDocuments.size;\n  }\n  /**\r\n   * Increment the number of acks needed from watch before we can consider the\r\n   * server to be 'in-sync' with the client's active targets.\r\n   */\n\n\n  recordPendingTargetRequest(targetId) {\n    // For each request we get we need to record we need a response for it.\n    const targetState = this.ensureTargetState(targetId);\n    targetState.recordPendingTargetRequest();\n  }\n\n  ensureTargetState(targetId) {\n    let result = this.targetStates.get(targetId);\n\n    if (!result) {\n      result = new TargetState();\n      this.targetStates.set(targetId, result);\n    }\n\n    return result;\n  }\n\n  ensureDocumentTargetMapping(key) {\n    let targetMapping = this.pendingDocumentTargetMapping.get(key);\n\n    if (!targetMapping) {\n      targetMapping = new SortedSet(primitiveComparator);\n      this.pendingDocumentTargetMapping = this.pendingDocumentTargetMapping.insert(key, targetMapping);\n    }\n\n    return targetMapping;\n  }\n  /**\r\n   * Verifies that the user is still interested in this target (by calling\r\n   * `getTargetDataForTarget()`) and that we are not waiting for pending ADDs\r\n   * from watch.\r\n   */\n\n\n  isActiveTarget(targetId) {\n    const targetActive = this.targetDataForActiveTarget(targetId) !== null;\n\n    if (!targetActive) {\n      logDebug(LOG_TAG$g, 'Detected inactive target', targetId);\n    }\n\n    return targetActive;\n  }\n  /**\r\n   * Returns the TargetData for an active target (i.e. a target that the user\r\n   * is still interested in that has no outstanding target change requests).\r\n   */\n\n\n  targetDataForActiveTarget(targetId) {\n    const targetState = this.targetStates.get(targetId);\n    return targetState && targetState.isPending ? null : this.metadataProvider.getTargetDataForTarget(targetId);\n  }\n  /**\r\n   * Resets the state of a Watch target to its initial state (e.g. sets\r\n   * 'current' to false, clears the resume token and removes its target mapping\r\n   * from all documents).\r\n   */\n\n\n  resetTarget(targetId) {\n    this.targetStates.set(targetId, new TargetState()); // Trigger removal for any documents currently mapped to this target.\n    // These removals will be part of the initial snapshot if Watch does not\n    // resend these documents.\n\n    const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\n    existingKeys.forEach(key => {\n      this.removeDocumentFromTarget(targetId, key,\n      /*updatedDocument=*/\n      null);\n    });\n  }\n  /**\r\n   * Returns whether the LocalStore considers the document to be part of the\r\n   * specified target.\r\n   */\n\n\n  targetContainsDocument(targetId, key) {\n    const existingKeys = this.metadataProvider.getRemoteKeysForTarget(targetId);\n    return existingKeys.has(key);\n  }\n\n}\n\nfunction documentTargetMap() {\n  return new SortedMap(DocumentKey.comparator);\n}\n\nfunction snapshotChangesMap() {\n  return new SortedMap(DocumentKey.comparator);\n}\n\nfunction createExistenceFilterMismatchInfoForTestingHooks(status, localCacheCount, existenceFilter) {\n  var _a, _b, _c, _d, _e, _f;\n\n  const result = {\n    localCacheCount,\n    existenceFilterCount: existenceFilter.count\n  };\n  const unchangedNames = existenceFilter.unchangedNames;\n\n  if (unchangedNames) {\n    result.bloomFilter = {\n      applied: status === 0\n      /* BloomFilterApplicationStatus.Success */\n      ,\n      hashCount: (_a = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.hashCount) !== null && _a !== void 0 ? _a : 0,\n      bitmapLength: (_d = (_c = (_b = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.bits) === null || _b === void 0 ? void 0 : _b.bitmap) === null || _c === void 0 ? void 0 : _c.length) !== null && _d !== void 0 ? _d : 0,\n      padding: (_f = (_e = unchangedNames === null || unchangedNames === void 0 ? void 0 : unchangedNames.bits) === null || _e === void 0 ? void 0 : _e.padding) !== null && _f !== void 0 ? _f : 0\n    };\n  }\n\n  return result;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst DIRECTIONS = (() => {\n  const dirs = {};\n  dirs[\"asc\"\n  /* Direction.ASCENDING */\n  ] = 'ASCENDING';\n  dirs[\"desc\"\n  /* Direction.DESCENDING */\n  ] = 'DESCENDING';\n  return dirs;\n})();\n\nconst OPERATORS = (() => {\n  const ops = {};\n  ops[\"<\"\n  /* Operator.LESS_THAN */\n  ] = 'LESS_THAN';\n  ops[\"<=\"\n  /* Operator.LESS_THAN_OR_EQUAL */\n  ] = 'LESS_THAN_OR_EQUAL';\n  ops[\">\"\n  /* Operator.GREATER_THAN */\n  ] = 'GREATER_THAN';\n  ops[\">=\"\n  /* Operator.GREATER_THAN_OR_EQUAL */\n  ] = 'GREATER_THAN_OR_EQUAL';\n  ops[\"==\"\n  /* Operator.EQUAL */\n  ] = 'EQUAL';\n  ops[\"!=\"\n  /* Operator.NOT_EQUAL */\n  ] = 'NOT_EQUAL';\n  ops[\"array-contains\"\n  /* Operator.ARRAY_CONTAINS */\n  ] = 'ARRAY_CONTAINS';\n  ops[\"in\"\n  /* Operator.IN */\n  ] = 'IN';\n  ops[\"not-in\"\n  /* Operator.NOT_IN */\n  ] = 'NOT_IN';\n  ops[\"array-contains-any\"\n  /* Operator.ARRAY_CONTAINS_ANY */\n  ] = 'ARRAY_CONTAINS_ANY';\n  return ops;\n})();\n\nconst COMPOSITE_OPERATORS = (() => {\n  const ops = {};\n  ops[\"and\"\n  /* CompositeOperator.AND */\n  ] = 'AND';\n  ops[\"or\"\n  /* CompositeOperator.OR */\n  ] = 'OR';\n  return ops;\n})();\n\nfunction assertPresent(value, description) {}\n/**\r\n * This class generates JsonObject values for the Datastore API suitable for\r\n * sending to either GRPC stub methods or via the JSON/HTTP REST API.\r\n *\r\n * The serializer supports both Protobuf.js and Proto3 JSON formats. By\r\n * setting `useProto3Json` to true, the serializer will use the Proto3 JSON\r\n * format.\r\n *\r\n * For a description of the Proto3 JSON format check\r\n * https://developers.google.com/protocol-buffers/docs/proto3#json\r\n *\r\n * TODO(klimt): We can remove the databaseId argument if we keep the full\r\n * resource name in documents.\r\n */\n\n\nclass JsonProtoSerializer {\n  constructor(databaseId, useProto3Json) {\n    this.databaseId = databaseId;\n    this.useProto3Json = useProto3Json;\n  }\n\n}\n\nfunction fromRpcStatus(status) {\n  const code = status.code === undefined ? Code.UNKNOWN : mapCodeFromRpcCode(status.code);\n  return new FirestoreError(code, status.message || '');\n}\n/**\r\n * Returns a value for a number (or null) that's appropriate to put into\r\n * a google.protobuf.Int32Value proto.\r\n * DO NOT USE THIS FOR ANYTHING ELSE.\r\n * This method cheats. It's typed as returning \"number\" because that's what\r\n * our generated proto interfaces say Int32Value must be. But GRPC actually\r\n * expects a { value: <number> } struct.\r\n */\n\n\nfunction toInt32Proto(serializer, val) {\n  if (serializer.useProto3Json || isNullOrUndefined(val)) {\n    return val;\n  } else {\n    return {\n      value: val\n    };\n  }\n}\n/**\r\n * Returns a number (or null) from a google.protobuf.Int32Value proto.\r\n */\n\n\nfunction fromInt32Proto(val) {\n  let result;\n\n  if (typeof val === 'object') {\n    result = val.value;\n  } else {\n    result = val;\n  }\n\n  return isNullOrUndefined(result) ? null : result;\n}\n/**\r\n * Returns a value for a Date that's appropriate to put into a proto.\r\n */\n\n\nfunction toTimestamp(serializer, timestamp) {\n  if (serializer.useProto3Json) {\n    // Serialize to ISO-8601 date format, but with full nano resolution.\n    // Since JS Date has only millis, let's only use it for the seconds and\n    // then manually add the fractions to the end.\n    const jsDateStr = new Date(timestamp.seconds * 1000).toISOString(); // Remove .xxx frac part and Z in the end.\n\n    const strUntilSeconds = jsDateStr.replace(/\\.\\d*/, '').replace('Z', ''); // Pad the fraction out to 9 digits (nanos).\n\n    const nanoStr = ('000000000' + timestamp.nanoseconds).slice(-9);\n    return `${strUntilSeconds}.${nanoStr}Z`;\n  } else {\n    return {\n      seconds: '' + timestamp.seconds,\n      nanos: timestamp.nanoseconds // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n    };\n  }\n}\n\nfunction fromTimestamp(date) {\n  const timestamp = normalizeTimestamp(date);\n  return new Timestamp(timestamp.seconds, timestamp.nanos);\n}\n/**\r\n * Returns a value for bytes that's appropriate to put in a proto.\r\n *\r\n * Visible for testing.\r\n */\n\n\nfunction toBytes(serializer, bytes) {\n  if (serializer.useProto3Json) {\n    return bytes.toBase64();\n  } else {\n    return bytes.toUint8Array();\n  }\n}\n/**\r\n * Returns a ByteString based on the proto string value.\r\n */\n\n\nfunction fromBytes(serializer, value) {\n  if (serializer.useProto3Json) {\n    hardAssert(value === undefined || typeof value === 'string');\n    return ByteString.fromBase64String(value ? value : '');\n  } else {\n    hardAssert(value === undefined || value instanceof Uint8Array);\n    return ByteString.fromUint8Array(value ? value : new Uint8Array());\n  }\n}\n\nfunction toVersion(serializer, version) {\n  return toTimestamp(serializer, version.toTimestamp());\n}\n\nfunction fromVersion(version) {\n  hardAssert(!!version);\n  return SnapshotVersion.fromTimestamp(fromTimestamp(version));\n}\n\nfunction toResourceName(databaseId, path) {\n  return fullyQualifiedPrefixPath(databaseId).child('documents').child(path).canonicalString();\n}\n\nfunction fromResourceName(name) {\n  const resource = ResourcePath.fromString(name);\n  hardAssert(isValidResourceName(resource));\n  return resource;\n}\n\nfunction toName(serializer, key) {\n  return toResourceName(serializer.databaseId, key.path);\n}\n\nfunction fromName(serializer, name) {\n  const resource = fromResourceName(name);\n\n  if (resource.get(1) !== serializer.databaseId.projectId) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different project: ' + resource.get(1) + ' vs ' + serializer.databaseId.projectId);\n  }\n\n  if (resource.get(3) !== serializer.databaseId.database) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Tried to deserialize key from different database: ' + resource.get(3) + ' vs ' + serializer.databaseId.database);\n  }\n\n  return new DocumentKey(extractLocalPathFromResourceName(resource));\n}\n\nfunction toQueryPath(serializer, path) {\n  return toResourceName(serializer.databaseId, path);\n}\n\nfunction fromQueryPath(name) {\n  const resourceName = fromResourceName(name); // In v1beta1 queries for collections at the root did not have a trailing\n  // \"/documents\". In v1 all resource paths contain \"/documents\". Preserve the\n  // ability to read the v1beta1 form for compatibility with queries persisted\n  // in the local target cache.\n\n  if (resourceName.length === 4) {\n    return ResourcePath.emptyPath();\n  }\n\n  return extractLocalPathFromResourceName(resourceName);\n}\n\nfunction getEncodedDatabaseId(serializer) {\n  const path = new ResourcePath(['projects', serializer.databaseId.projectId, 'databases', serializer.databaseId.database]);\n  return path.canonicalString();\n}\n\nfunction fullyQualifiedPrefixPath(databaseId) {\n  return new ResourcePath(['projects', databaseId.projectId, 'databases', databaseId.database]);\n}\n\nfunction extractLocalPathFromResourceName(resourceName) {\n  hardAssert(resourceName.length > 4 && resourceName.get(4) === 'documents');\n  return resourceName.popFirst(5);\n}\n/** Creates a Document proto from key and fields (but no create/update time) */\n\n\nfunction toMutationDocument(serializer, key, fields) {\n  return {\n    name: toName(serializer, key),\n    fields: fields.value.mapValue.fields\n  };\n}\n\nfunction toDocument(serializer, document) {\n  return {\n    name: toName(serializer, document.key),\n    fields: document.data.value.mapValue.fields,\n    updateTime: toTimestamp(serializer, document.version.toTimestamp()),\n    createTime: toTimestamp(serializer, document.createTime.toTimestamp())\n  };\n}\n\nfunction fromDocument(serializer, document, hasCommittedMutations) {\n  const key = fromName(serializer, document.name);\n  const version = fromVersion(document.updateTime); // If we read a document from persistence that is missing createTime, it's due\n  // to older SDK versions not storing this information. In such cases, we'll\n  // set the createTime to zero. This can be removed in the long term.\n\n  const createTime = document.createTime ? fromVersion(document.createTime) : SnapshotVersion.min();\n  const data = new ObjectValue({\n    mapValue: {\n      fields: document.fields\n    }\n  });\n  const result = MutableDocument.newFoundDocument(key, version, createTime, data);\n\n  if (hasCommittedMutations) {\n    result.setHasCommittedMutations();\n  }\n\n  return hasCommittedMutations ? result.setHasCommittedMutations() : result;\n}\n\nfunction fromFound(serializer, doc) {\n  hardAssert(!!doc.found);\n  assertPresent(doc.found.name);\n  assertPresent(doc.found.updateTime);\n  const key = fromName(serializer, doc.found.name);\n  const version = fromVersion(doc.found.updateTime);\n  const createTime = doc.found.createTime ? fromVersion(doc.found.createTime) : SnapshotVersion.min();\n  const data = new ObjectValue({\n    mapValue: {\n      fields: doc.found.fields\n    }\n  });\n  return MutableDocument.newFoundDocument(key, version, createTime, data);\n}\n\nfunction fromMissing(serializer, result) {\n  hardAssert(!!result.missing);\n  hardAssert(!!result.readTime);\n  const key = fromName(serializer, result.missing);\n  const version = fromVersion(result.readTime);\n  return MutableDocument.newNoDocument(key, version);\n}\n\nfunction fromBatchGetDocumentsResponse(serializer, result) {\n  if ('found' in result) {\n    return fromFound(serializer, result);\n  } else if ('missing' in result) {\n    return fromMissing(serializer, result);\n  }\n\n  return fail();\n}\n\nfunction fromWatchChange(serializer, change) {\n  let watchChange;\n\n  if ('targetChange' in change) {\n    assertPresent(change.targetChange); // proto3 default value is unset in JSON (undefined), so use 'NO_CHANGE'\n    // if unset\n\n    const state = fromWatchTargetChangeState(change.targetChange.targetChangeType || 'NO_CHANGE');\n    const targetIds = change.targetChange.targetIds || [];\n    const resumeToken = fromBytes(serializer, change.targetChange.resumeToken);\n    const causeProto = change.targetChange.cause;\n    const cause = causeProto && fromRpcStatus(causeProto);\n    watchChange = new WatchTargetChange(state, targetIds, resumeToken, cause || null);\n  } else if ('documentChange' in change) {\n    assertPresent(change.documentChange);\n    const entityChange = change.documentChange;\n    assertPresent(entityChange.document);\n    assertPresent(entityChange.document.name);\n    assertPresent(entityChange.document.updateTime);\n    const key = fromName(serializer, entityChange.document.name);\n    const version = fromVersion(entityChange.document.updateTime);\n    const createTime = entityChange.document.createTime ? fromVersion(entityChange.document.createTime) : SnapshotVersion.min();\n    const data = new ObjectValue({\n      mapValue: {\n        fields: entityChange.document.fields\n      }\n    });\n    const doc = MutableDocument.newFoundDocument(key, version, createTime, data);\n    const updatedTargetIds = entityChange.targetIds || [];\n    const removedTargetIds = entityChange.removedTargetIds || [];\n    watchChange = new DocumentWatchChange(updatedTargetIds, removedTargetIds, doc.key, doc);\n  } else if ('documentDelete' in change) {\n    assertPresent(change.documentDelete);\n    const docDelete = change.documentDelete;\n    assertPresent(docDelete.document);\n    const key = fromName(serializer, docDelete.document);\n    const version = docDelete.readTime ? fromVersion(docDelete.readTime) : SnapshotVersion.min();\n    const doc = MutableDocument.newNoDocument(key, version);\n    const removedTargetIds = docDelete.removedTargetIds || [];\n    watchChange = new DocumentWatchChange([], removedTargetIds, doc.key, doc);\n  } else if ('documentRemove' in change) {\n    assertPresent(change.documentRemove);\n    const docRemove = change.documentRemove;\n    assertPresent(docRemove.document);\n    const key = fromName(serializer, docRemove.document);\n    const removedTargetIds = docRemove.removedTargetIds || [];\n    watchChange = new DocumentWatchChange([], removedTargetIds, key, null);\n  } else if ('filter' in change) {\n    // TODO(dimond): implement existence filter parsing with strategy.\n    assertPresent(change.filter);\n    const filter = change.filter;\n    assertPresent(filter.targetId);\n    const {\n      count = 0,\n      unchangedNames\n    } = filter;\n    const existenceFilter = new ExistenceFilter(count, unchangedNames);\n    const targetId = filter.targetId;\n    watchChange = new ExistenceFilterChange(targetId, existenceFilter);\n  } else {\n    return fail();\n  }\n\n  return watchChange;\n}\n\nfunction fromWatchTargetChangeState(state) {\n  if (state === 'NO_CHANGE') {\n    return 0\n    /* WatchTargetChangeState.NoChange */\n    ;\n  } else if (state === 'ADD') {\n    return 1\n    /* WatchTargetChangeState.Added */\n    ;\n  } else if (state === 'REMOVE') {\n    return 2\n    /* WatchTargetChangeState.Removed */\n    ;\n  } else if (state === 'CURRENT') {\n    return 3\n    /* WatchTargetChangeState.Current */\n    ;\n  } else if (state === 'RESET') {\n    return 4\n    /* WatchTargetChangeState.Reset */\n    ;\n  } else {\n    return fail();\n  }\n}\n\nfunction versionFromListenResponse(change) {\n  // We have only reached a consistent snapshot for the entire stream if there\n  // is a read_time set and it applies to all targets (i.e. the list of\n  // targets is empty). The backend is guaranteed to send such responses.\n  if (!('targetChange' in change)) {\n    return SnapshotVersion.min();\n  }\n\n  const targetChange = change.targetChange;\n\n  if (targetChange.targetIds && targetChange.targetIds.length) {\n    return SnapshotVersion.min();\n  }\n\n  if (!targetChange.readTime) {\n    return SnapshotVersion.min();\n  }\n\n  return fromVersion(targetChange.readTime);\n}\n\nfunction toMutation(serializer, mutation) {\n  let result;\n\n  if (mutation instanceof SetMutation) {\n    result = {\n      update: toMutationDocument(serializer, mutation.key, mutation.value)\n    };\n  } else if (mutation instanceof DeleteMutation) {\n    result = {\n      delete: toName(serializer, mutation.key)\n    };\n  } else if (mutation instanceof PatchMutation) {\n    result = {\n      update: toMutationDocument(serializer, mutation.key, mutation.data),\n      updateMask: toDocumentMask(mutation.fieldMask)\n    };\n  } else if (mutation instanceof VerifyMutation) {\n    result = {\n      verify: toName(serializer, mutation.key)\n    };\n  } else {\n    return fail();\n  }\n\n  if (mutation.fieldTransforms.length > 0) {\n    result.updateTransforms = mutation.fieldTransforms.map(transform => toFieldTransform(serializer, transform));\n  }\n\n  if (!mutation.precondition.isNone) {\n    result.currentDocument = toPrecondition(serializer, mutation.precondition);\n  }\n\n  return result;\n}\n\nfunction fromMutation(serializer, proto) {\n  const precondition = proto.currentDocument ? fromPrecondition(proto.currentDocument) : Precondition.none();\n  const fieldTransforms = proto.updateTransforms ? proto.updateTransforms.map(transform => fromFieldTransform(serializer, transform)) : [];\n\n  if (proto.update) {\n    assertPresent(proto.update.name);\n    const key = fromName(serializer, proto.update.name);\n    const value = new ObjectValue({\n      mapValue: {\n        fields: proto.update.fields\n      }\n    });\n\n    if (proto.updateMask) {\n      const fieldMask = fromDocumentMask(proto.updateMask);\n      return new PatchMutation(key, value, fieldMask, precondition, fieldTransforms);\n    } else {\n      return new SetMutation(key, value, precondition, fieldTransforms);\n    }\n  } else if (proto.delete) {\n    const key = fromName(serializer, proto.delete);\n    return new DeleteMutation(key, precondition);\n  } else if (proto.verify) {\n    const key = fromName(serializer, proto.verify);\n    return new VerifyMutation(key, precondition);\n  } else {\n    return fail();\n  }\n}\n\nfunction toPrecondition(serializer, precondition) {\n  if (precondition.updateTime !== undefined) {\n    return {\n      updateTime: toVersion(serializer, precondition.updateTime)\n    };\n  } else if (precondition.exists !== undefined) {\n    return {\n      exists: precondition.exists\n    };\n  } else {\n    return fail();\n  }\n}\n\nfunction fromPrecondition(precondition) {\n  if (precondition.updateTime !== undefined) {\n    return Precondition.updateTime(fromVersion(precondition.updateTime));\n  } else if (precondition.exists !== undefined) {\n    return Precondition.exists(precondition.exists);\n  } else {\n    return Precondition.none();\n  }\n}\n\nfunction fromWriteResult(proto, commitTime) {\n  // NOTE: Deletes don't have an updateTime.\n  let version = proto.updateTime ? fromVersion(proto.updateTime) : fromVersion(commitTime);\n\n  if (version.isEqual(SnapshotVersion.min())) {\n    // The Firestore Emulator currently returns an update time of 0 for\n    // deletes of non-existing documents (rather than null). This breaks the\n    // test \"get deleted doc while offline with source=cache\" as NoDocuments\n    // with version 0 are filtered by IndexedDb's RemoteDocumentCache.\n    // TODO(#2149): Remove this when Emulator is fixed\n    version = fromVersion(commitTime);\n  }\n\n  return new MutationResult(version, proto.transformResults || []);\n}\n\nfunction fromWriteResults(protos, commitTime) {\n  if (protos && protos.length > 0) {\n    hardAssert(commitTime !== undefined);\n    return protos.map(proto => fromWriteResult(proto, commitTime));\n  } else {\n    return [];\n  }\n}\n\nfunction toFieldTransform(serializer, fieldTransform) {\n  const transform = fieldTransform.transform;\n\n  if (transform instanceof ServerTimestampTransform) {\n    return {\n      fieldPath: fieldTransform.field.canonicalString(),\n      setToServerValue: 'REQUEST_TIME'\n    };\n  } else if (transform instanceof ArrayUnionTransformOperation) {\n    return {\n      fieldPath: fieldTransform.field.canonicalString(),\n      appendMissingElements: {\n        values: transform.elements\n      }\n    };\n  } else if (transform instanceof ArrayRemoveTransformOperation) {\n    return {\n      fieldPath: fieldTransform.field.canonicalString(),\n      removeAllFromArray: {\n        values: transform.elements\n      }\n    };\n  } else if (transform instanceof NumericIncrementTransformOperation) {\n    return {\n      fieldPath: fieldTransform.field.canonicalString(),\n      increment: transform.operand\n    };\n  } else {\n    throw fail();\n  }\n}\n\nfunction fromFieldTransform(serializer, proto) {\n  let transform = null;\n\n  if ('setToServerValue' in proto) {\n    hardAssert(proto.setToServerValue === 'REQUEST_TIME');\n    transform = new ServerTimestampTransform();\n  } else if ('appendMissingElements' in proto) {\n    const values = proto.appendMissingElements.values || [];\n    transform = new ArrayUnionTransformOperation(values);\n  } else if ('removeAllFromArray' in proto) {\n    const values = proto.removeAllFromArray.values || [];\n    transform = new ArrayRemoveTransformOperation(values);\n  } else if ('increment' in proto) {\n    transform = new NumericIncrementTransformOperation(serializer, proto.increment);\n  } else {\n    fail();\n  }\n\n  const fieldPath = FieldPath$1.fromServerFormat(proto.fieldPath);\n  return new FieldTransform(fieldPath, transform);\n}\n\nfunction toDocumentsTarget(serializer, target) {\n  return {\n    documents: [toQueryPath(serializer, target.path)]\n  };\n}\n\nfunction fromDocumentsTarget(documentsTarget) {\n  const count = documentsTarget.documents.length;\n  hardAssert(count === 1);\n  const name = documentsTarget.documents[0];\n  return queryToTarget(newQueryForPath(fromQueryPath(name)));\n}\n\nfunction toQueryTarget(serializer, target) {\n  // Dissect the path into parent, collectionId, and optional key filter.\n  const result = {\n    structuredQuery: {}\n  };\n  const path = target.path;\n\n  if (target.collectionGroup !== null) {\n    result.parent = toQueryPath(serializer, path);\n    result.structuredQuery.from = [{\n      collectionId: target.collectionGroup,\n      allDescendants: true\n    }];\n  } else {\n    result.parent = toQueryPath(serializer, path.popLast());\n    result.structuredQuery.from = [{\n      collectionId: path.lastSegment()\n    }];\n  }\n\n  const where = toFilters(target.filters);\n\n  if (where) {\n    result.structuredQuery.where = where;\n  }\n\n  const orderBy = toOrder(target.orderBy);\n\n  if (orderBy) {\n    result.structuredQuery.orderBy = orderBy;\n  }\n\n  const limit = toInt32Proto(serializer, target.limit);\n\n  if (limit !== null) {\n    result.structuredQuery.limit = limit;\n  }\n\n  if (target.startAt) {\n    result.structuredQuery.startAt = toStartAtCursor(target.startAt);\n  }\n\n  if (target.endAt) {\n    result.structuredQuery.endAt = toEndAtCursor(target.endAt);\n  }\n\n  return result;\n}\n\nfunction toRunAggregationQueryRequest(serializer, target, aggregates) {\n  const queryTarget = toQueryTarget(serializer, target);\n  const aliasMap = {};\n  const aggregations = [];\n  let aggregationNum = 0;\n  aggregates.forEach(aggregate => {\n    // Map all client-side aliases to a unique short-form\n    // alias. This avoids issues with client-side aliases that\n    // exceed the 1500-byte string size limit.\n    const serverAlias = `aggregate_${aggregationNum++}`;\n    aliasMap[serverAlias] = aggregate.alias;\n\n    if (aggregate.aggregateType === 'count') {\n      aggregations.push({\n        alias: serverAlias,\n        count: {}\n      });\n    } else if (aggregate.aggregateType === 'avg') {\n      aggregations.push({\n        alias: serverAlias,\n        avg: {\n          field: toFieldPathReference(aggregate.fieldPath)\n        }\n      });\n    } else if (aggregate.aggregateType === 'sum') {\n      aggregations.push({\n        alias: serverAlias,\n        sum: {\n          field: toFieldPathReference(aggregate.fieldPath)\n        }\n      });\n    }\n  });\n  return {\n    request: {\n      structuredAggregationQuery: {\n        aggregations,\n        structuredQuery: queryTarget.structuredQuery\n      },\n      parent: queryTarget.parent\n    },\n    aliasMap\n  };\n}\n\nfunction convertQueryTargetToQuery(target) {\n  let path = fromQueryPath(target.parent);\n  const query = target.structuredQuery;\n  const fromCount = query.from ? query.from.length : 0;\n  let collectionGroup = null;\n\n  if (fromCount > 0) {\n    hardAssert(fromCount === 1);\n    const from = query.from[0];\n\n    if (from.allDescendants) {\n      collectionGroup = from.collectionId;\n    } else {\n      path = path.child(from.collectionId);\n    }\n  }\n\n  let filterBy = [];\n\n  if (query.where) {\n    filterBy = fromFilters(query.where);\n  }\n\n  let orderBy = [];\n\n  if (query.orderBy) {\n    orderBy = fromOrder(query.orderBy);\n  }\n\n  let limit = null;\n\n  if (query.limit) {\n    limit = fromInt32Proto(query.limit);\n  }\n\n  let startAt = null;\n\n  if (query.startAt) {\n    startAt = fromStartAtCursor(query.startAt);\n  }\n\n  let endAt = null;\n\n  if (query.endAt) {\n    endAt = fromEndAtCursor(query.endAt);\n  }\n\n  return newQuery(path, collectionGroup, orderBy, filterBy, limit, \"F\"\n  /* LimitType.First */\n  , startAt, endAt);\n}\n\nfunction fromQueryTarget(target) {\n  return queryToTarget(convertQueryTargetToQuery(target));\n}\n\nfunction toListenRequestLabels(serializer, targetData) {\n  const value = toLabel(targetData.purpose);\n\n  if (value == null) {\n    return null;\n  } else {\n    return {\n      'goog-listen-tags': value\n    };\n  }\n}\n\nfunction toLabel(purpose) {\n  switch (purpose) {\n    case \"TargetPurposeListen\"\n    /* TargetPurpose.Listen */\n    :\n      return null;\n\n    case \"TargetPurposeExistenceFilterMismatch\"\n    /* TargetPurpose.ExistenceFilterMismatch */\n    :\n      return 'existence-filter-mismatch';\n\n    case \"TargetPurposeExistenceFilterMismatchBloom\"\n    /* TargetPurpose.ExistenceFilterMismatchBloom */\n    :\n      return 'existence-filter-mismatch-bloom';\n\n    case \"TargetPurposeLimboResolution\"\n    /* TargetPurpose.LimboResolution */\n    :\n      return 'limbo-document';\n\n    default:\n      return fail();\n  }\n}\n\nfunction toTarget(serializer, targetData) {\n  let result;\n  const target = targetData.target;\n\n  if (targetIsDocumentTarget(target)) {\n    result = {\n      documents: toDocumentsTarget(serializer, target)\n    };\n  } else {\n    result = {\n      query: toQueryTarget(serializer, target)\n    };\n  }\n\n  result.targetId = targetData.targetId;\n\n  if (targetData.resumeToken.approximateByteSize() > 0) {\n    result.resumeToken = toBytes(serializer, targetData.resumeToken);\n    const expectedCount = toInt32Proto(serializer, targetData.expectedCount);\n\n    if (expectedCount !== null) {\n      result.expectedCount = expectedCount;\n    }\n  } else if (targetData.snapshotVersion.compareTo(SnapshotVersion.min()) > 0) {\n    // TODO(wuandy): Consider removing above check because it is most likely true.\n    // Right now, many tests depend on this behaviour though (leaving min() out\n    // of serialization).\n    result.readTime = toTimestamp(serializer, targetData.snapshotVersion.toTimestamp());\n    const expectedCount = toInt32Proto(serializer, targetData.expectedCount);\n\n    if (expectedCount !== null) {\n      result.expectedCount = expectedCount;\n    }\n  }\n\n  return result;\n}\n\nfunction toFilters(filters) {\n  if (filters.length === 0) {\n    return;\n  }\n\n  return toFilter(CompositeFilter.create(filters, \"and\"\n  /* CompositeOperator.AND */\n  ));\n}\n\nfunction fromFilters(filter) {\n  const result = fromFilter(filter);\n\n  if (result instanceof CompositeFilter && compositeFilterIsFlatConjunction(result)) {\n    return result.getFilters();\n  }\n\n  return [result];\n}\n\nfunction fromFilter(filter) {\n  if (filter.unaryFilter !== undefined) {\n    return fromUnaryFilter(filter);\n  } else if (filter.fieldFilter !== undefined) {\n    return fromFieldFilter(filter);\n  } else if (filter.compositeFilter !== undefined) {\n    return fromCompositeFilter(filter);\n  } else {\n    return fail();\n  }\n}\n\nfunction toOrder(orderBys) {\n  if (orderBys.length === 0) {\n    return;\n  }\n\n  return orderBys.map(order => toPropertyOrder(order));\n}\n\nfunction fromOrder(orderBys) {\n  return orderBys.map(order => fromPropertyOrder(order));\n}\n\nfunction toStartAtCursor(cursor) {\n  return {\n    before: cursor.inclusive,\n    values: cursor.position\n  };\n}\n\nfunction toEndAtCursor(cursor) {\n  return {\n    before: !cursor.inclusive,\n    values: cursor.position\n  };\n}\n\nfunction fromStartAtCursor(cursor) {\n  const inclusive = !!cursor.before;\n  const position = cursor.values || [];\n  return new Bound(position, inclusive);\n}\n\nfunction fromEndAtCursor(cursor) {\n  const inclusive = !cursor.before;\n  const position = cursor.values || [];\n  return new Bound(position, inclusive);\n} // visible for testing\n\n\nfunction toDirection(dir) {\n  return DIRECTIONS[dir];\n} // visible for testing\n\n\nfunction fromDirection(dir) {\n  switch (dir) {\n    case 'ASCENDING':\n      return \"asc\"\n      /* Direction.ASCENDING */\n      ;\n\n    case 'DESCENDING':\n      return \"desc\"\n      /* Direction.DESCENDING */\n      ;\n\n    default:\n      return undefined;\n  }\n} // visible for testing\n\n\nfunction toOperatorName(op) {\n  return OPERATORS[op];\n}\n\nfunction toCompositeOperatorName(op) {\n  return COMPOSITE_OPERATORS[op];\n}\n\nfunction fromOperatorName(op) {\n  switch (op) {\n    case 'EQUAL':\n      return \"==\"\n      /* Operator.EQUAL */\n      ;\n\n    case 'NOT_EQUAL':\n      return \"!=\"\n      /* Operator.NOT_EQUAL */\n      ;\n\n    case 'GREATER_THAN':\n      return \">\"\n      /* Operator.GREATER_THAN */\n      ;\n\n    case 'GREATER_THAN_OR_EQUAL':\n      return \">=\"\n      /* Operator.GREATER_THAN_OR_EQUAL */\n      ;\n\n    case 'LESS_THAN':\n      return \"<\"\n      /* Operator.LESS_THAN */\n      ;\n\n    case 'LESS_THAN_OR_EQUAL':\n      return \"<=\"\n      /* Operator.LESS_THAN_OR_EQUAL */\n      ;\n\n    case 'ARRAY_CONTAINS':\n      return \"array-contains\"\n      /* Operator.ARRAY_CONTAINS */\n      ;\n\n    case 'IN':\n      return \"in\"\n      /* Operator.IN */\n      ;\n\n    case 'NOT_IN':\n      return \"not-in\"\n      /* Operator.NOT_IN */\n      ;\n\n    case 'ARRAY_CONTAINS_ANY':\n      return \"array-contains-any\"\n      /* Operator.ARRAY_CONTAINS_ANY */\n      ;\n\n    case 'OPERATOR_UNSPECIFIED':\n      return fail();\n\n    default:\n      return fail();\n  }\n}\n\nfunction fromCompositeOperatorName(op) {\n  switch (op) {\n    case 'AND':\n      return \"and\"\n      /* CompositeOperator.AND */\n      ;\n\n    case 'OR':\n      return \"or\"\n      /* CompositeOperator.OR */\n      ;\n\n    default:\n      return fail();\n  }\n}\n\nfunction toFieldPathReference(path) {\n  return {\n    fieldPath: path.canonicalString()\n  };\n}\n\nfunction fromFieldPathReference(fieldReference) {\n  return FieldPath$1.fromServerFormat(fieldReference.fieldPath);\n} // visible for testing\n\n\nfunction toPropertyOrder(orderBy) {\n  return {\n    field: toFieldPathReference(orderBy.field),\n    direction: toDirection(orderBy.dir)\n  };\n}\n\nfunction fromPropertyOrder(orderBy) {\n  return new OrderBy(fromFieldPathReference(orderBy.field), fromDirection(orderBy.direction));\n} // visible for testing\n\n\nfunction toFilter(filter) {\n  if (filter instanceof FieldFilter) {\n    return toUnaryOrFieldFilter(filter);\n  } else if (filter instanceof CompositeFilter) {\n    return toCompositeFilter(filter);\n  } else {\n    return fail();\n  }\n}\n\nfunction toCompositeFilter(filter) {\n  const protos = filter.getFilters().map(filter => toFilter(filter));\n\n  if (protos.length === 1) {\n    return protos[0];\n  }\n\n  return {\n    compositeFilter: {\n      op: toCompositeOperatorName(filter.op),\n      filters: protos\n    }\n  };\n}\n\nfunction toUnaryOrFieldFilter(filter) {\n  if (filter.op === \"==\"\n  /* Operator.EQUAL */\n  ) {\n    if (isNanValue(filter.value)) {\n      return {\n        unaryFilter: {\n          field: toFieldPathReference(filter.field),\n          op: 'IS_NAN'\n        }\n      };\n    } else if (isNullValue(filter.value)) {\n      return {\n        unaryFilter: {\n          field: toFieldPathReference(filter.field),\n          op: 'IS_NULL'\n        }\n      };\n    }\n  } else if (filter.op === \"!=\"\n  /* Operator.NOT_EQUAL */\n  ) {\n    if (isNanValue(filter.value)) {\n      return {\n        unaryFilter: {\n          field: toFieldPathReference(filter.field),\n          op: 'IS_NOT_NAN'\n        }\n      };\n    } else if (isNullValue(filter.value)) {\n      return {\n        unaryFilter: {\n          field: toFieldPathReference(filter.field),\n          op: 'IS_NOT_NULL'\n        }\n      };\n    }\n  }\n\n  return {\n    fieldFilter: {\n      field: toFieldPathReference(filter.field),\n      op: toOperatorName(filter.op),\n      value: filter.value\n    }\n  };\n}\n\nfunction fromUnaryFilter(filter) {\n  switch (filter.unaryFilter.op) {\n    case 'IS_NAN':\n      const nanField = fromFieldPathReference(filter.unaryFilter.field);\n      return FieldFilter.create(nanField, \"==\"\n      /* Operator.EQUAL */\n      , {\n        doubleValue: NaN\n      });\n\n    case 'IS_NULL':\n      const nullField = fromFieldPathReference(filter.unaryFilter.field);\n      return FieldFilter.create(nullField, \"==\"\n      /* Operator.EQUAL */\n      , {\n        nullValue: 'NULL_VALUE'\n      });\n\n    case 'IS_NOT_NAN':\n      const notNanField = fromFieldPathReference(filter.unaryFilter.field);\n      return FieldFilter.create(notNanField, \"!=\"\n      /* Operator.NOT_EQUAL */\n      , {\n        doubleValue: NaN\n      });\n\n    case 'IS_NOT_NULL':\n      const notNullField = fromFieldPathReference(filter.unaryFilter.field);\n      return FieldFilter.create(notNullField, \"!=\"\n      /* Operator.NOT_EQUAL */\n      , {\n        nullValue: 'NULL_VALUE'\n      });\n\n    case 'OPERATOR_UNSPECIFIED':\n      return fail();\n\n    default:\n      return fail();\n  }\n}\n\nfunction fromFieldFilter(filter) {\n  return FieldFilter.create(fromFieldPathReference(filter.fieldFilter.field), fromOperatorName(filter.fieldFilter.op), filter.fieldFilter.value);\n}\n\nfunction fromCompositeFilter(filter) {\n  return CompositeFilter.create(filter.compositeFilter.filters.map(filter => fromFilter(filter)), fromCompositeOperatorName(filter.compositeFilter.op));\n}\n\nfunction toDocumentMask(fieldMask) {\n  const canonicalFields = [];\n  fieldMask.fields.forEach(field => canonicalFields.push(field.canonicalString()));\n  return {\n    fieldPaths: canonicalFields\n  };\n}\n\nfunction fromDocumentMask(proto) {\n  const paths = proto.fieldPaths || [];\n  return new FieldMask(paths.map(path => FieldPath$1.fromServerFormat(path)));\n}\n\nfunction isValidResourceName(path) {\n  // Resource names have at least 4 components (project ID, database ID)\n  return path.length >= 4 && path.get(0) === 'projects' && path.get(2) === 'databases';\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An immutable set of metadata that the local store tracks for each target.\r\n */\n\n\nclass TargetData {\n  constructor(\n  /** The target being listened to. */\n  target,\n  /**\r\n   * The target ID to which the target corresponds; Assigned by the\r\n   * LocalStore for user listens and by the SyncEngine for limbo watches.\r\n   */\n  targetId,\n  /** The purpose of the target. */\n  purpose,\n  /**\r\n   * The sequence number of the last transaction during which this target data\r\n   * was modified.\r\n   */\n  sequenceNumber,\n  /** The latest snapshot version seen for this target. */\n  snapshotVersion = SnapshotVersion.min(),\n  /**\r\n   * The maximum snapshot version at which the associated view\r\n   * contained no limbo documents.\r\n   */\n  lastLimboFreeSnapshotVersion = SnapshotVersion.min(),\n  /**\r\n   * An opaque, server-assigned token that allows watching a target to be\r\n   * resumed after disconnecting without retransmitting all the data that\r\n   * matches the target. The resume token essentially identifies a point in\r\n   * time from which the server should resume sending results.\r\n   */\n  resumeToken = ByteString.EMPTY_BYTE_STRING,\n  /**\r\n   * The number of documents that last matched the query at the resume token or\r\n   * read time. Documents are counted only when making a listen request with\r\n   * resume token or read time, otherwise, keep it null.\r\n   */\n  expectedCount = null) {\n    this.target = target;\n    this.targetId = targetId;\n    this.purpose = purpose;\n    this.sequenceNumber = sequenceNumber;\n    this.snapshotVersion = snapshotVersion;\n    this.lastLimboFreeSnapshotVersion = lastLimboFreeSnapshotVersion;\n    this.resumeToken = resumeToken;\n    this.expectedCount = expectedCount;\n  }\n  /** Creates a new target data instance with an updated sequence number. */\n\n\n  withSequenceNumber(sequenceNumber) {\n    return new TargetData(this.target, this.targetId, this.purpose, sequenceNumber, this.snapshotVersion, this.lastLimboFreeSnapshotVersion, this.resumeToken, this.expectedCount);\n  }\n  /**\r\n   * Creates a new target data instance with an updated resume token and\r\n   * snapshot version.\r\n   */\n\n\n  withResumeToken(resumeToken, snapshotVersion) {\n    return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, snapshotVersion, this.lastLimboFreeSnapshotVersion, resumeToken,\n    /* expectedCount= */\n    null);\n  }\n  /**\r\n   * Creates a new target data instance with an updated expected count.\r\n   */\n\n\n  withExpectedCount(expectedCount) {\n    return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, this.snapshotVersion, this.lastLimboFreeSnapshotVersion, this.resumeToken, expectedCount);\n  }\n  /**\r\n   * Creates a new target data instance with an updated last limbo free\r\n   * snapshot version number.\r\n   */\n\n\n  withLastLimboFreeSnapshotVersion(lastLimboFreeSnapshotVersion) {\n    return new TargetData(this.target, this.targetId, this.purpose, this.sequenceNumber, this.snapshotVersion, lastLimboFreeSnapshotVersion, this.resumeToken, this.expectedCount);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Serializer for values stored in the LocalStore. */\n\n\nclass LocalSerializer {\n  constructor(remoteSerializer) {\n    this.remoteSerializer = remoteSerializer;\n  }\n\n}\n/** Decodes a remote document from storage locally to a Document. */\n\n\nfunction fromDbRemoteDocument(localSerializer, remoteDoc) {\n  let doc;\n\n  if (remoteDoc.document) {\n    doc = fromDocument(localSerializer.remoteSerializer, remoteDoc.document, !!remoteDoc.hasCommittedMutations);\n  } else if (remoteDoc.noDocument) {\n    const key = DocumentKey.fromSegments(remoteDoc.noDocument.path);\n    const version = fromDbTimestamp(remoteDoc.noDocument.readTime);\n    doc = MutableDocument.newNoDocument(key, version);\n\n    if (remoteDoc.hasCommittedMutations) {\n      doc.setHasCommittedMutations();\n    }\n  } else if (remoteDoc.unknownDocument) {\n    const key = DocumentKey.fromSegments(remoteDoc.unknownDocument.path);\n    const version = fromDbTimestamp(remoteDoc.unknownDocument.version);\n    doc = MutableDocument.newUnknownDocument(key, version);\n  } else {\n    return fail();\n  }\n\n  if (remoteDoc.readTime) {\n    doc.setReadTime(fromDbTimestampKey(remoteDoc.readTime));\n  }\n\n  return doc;\n}\n/** Encodes a document for storage locally. */\n\n\nfunction toDbRemoteDocument(localSerializer, document) {\n  const key = document.key;\n  const remoteDoc = {\n    prefixPath: key.getCollectionPath().popLast().toArray(),\n    collectionGroup: key.collectionGroup,\n    documentId: key.path.lastSegment(),\n    readTime: toDbTimestampKey(document.readTime),\n    hasCommittedMutations: document.hasCommittedMutations\n  };\n\n  if (document.isFoundDocument()) {\n    remoteDoc.document = toDocument(localSerializer.remoteSerializer, document);\n  } else if (document.isNoDocument()) {\n    remoteDoc.noDocument = {\n      path: key.path.toArray(),\n      readTime: toDbTimestamp(document.version)\n    };\n  } else if (document.isUnknownDocument()) {\n    remoteDoc.unknownDocument = {\n      path: key.path.toArray(),\n      version: toDbTimestamp(document.version)\n    };\n  } else {\n    return fail();\n  }\n\n  return remoteDoc;\n}\n\nfunction toDbTimestampKey(snapshotVersion) {\n  const timestamp = snapshotVersion.toTimestamp();\n  return [timestamp.seconds, timestamp.nanoseconds];\n}\n\nfunction fromDbTimestampKey(dbTimestampKey) {\n  const timestamp = new Timestamp(dbTimestampKey[0], dbTimestampKey[1]);\n  return SnapshotVersion.fromTimestamp(timestamp);\n}\n\nfunction toDbTimestamp(snapshotVersion) {\n  const timestamp = snapshotVersion.toTimestamp();\n  return {\n    seconds: timestamp.seconds,\n    nanoseconds: timestamp.nanoseconds\n  };\n}\n\nfunction fromDbTimestamp(dbTimestamp) {\n  const timestamp = new Timestamp(dbTimestamp.seconds, dbTimestamp.nanoseconds);\n  return SnapshotVersion.fromTimestamp(timestamp);\n}\n/** Encodes a batch of mutations into a DbMutationBatch for local storage. */\n\n\nfunction toDbMutationBatch(localSerializer, userId, batch) {\n  const serializedBaseMutations = batch.baseMutations.map(m => toMutation(localSerializer.remoteSerializer, m));\n  const serializedMutations = batch.mutations.map(m => toMutation(localSerializer.remoteSerializer, m));\n  return {\n    userId,\n    batchId: batch.batchId,\n    localWriteTimeMs: batch.localWriteTime.toMillis(),\n    baseMutations: serializedBaseMutations,\n    mutations: serializedMutations\n  };\n}\n/** Decodes a DbMutationBatch into a MutationBatch */\n\n\nfunction fromDbMutationBatch(localSerializer, dbBatch) {\n  const baseMutations = (dbBatch.baseMutations || []).map(m => fromMutation(localSerializer.remoteSerializer, m)); // Squash old transform mutations into existing patch or set mutations.\n  // The replacement of representing `transforms` with `update_transforms`\n  // on the SDK means that old `transform` mutations stored in IndexedDB need\n  // to be updated to `update_transforms`.\n  // TODO(b/174608374): Remove this code once we perform a schema migration.\n\n  for (let i = 0; i < dbBatch.mutations.length - 1; ++i) {\n    const currentMutation = dbBatch.mutations[i];\n    const hasTransform = i + 1 < dbBatch.mutations.length && dbBatch.mutations[i + 1].transform !== undefined;\n\n    if (hasTransform) {\n      const transformMutation = dbBatch.mutations[i + 1];\n      currentMutation.updateTransforms = transformMutation.transform.fieldTransforms;\n      dbBatch.mutations.splice(i + 1, 1);\n      ++i;\n    }\n  }\n\n  const mutations = dbBatch.mutations.map(m => fromMutation(localSerializer.remoteSerializer, m));\n  const timestamp = Timestamp.fromMillis(dbBatch.localWriteTimeMs);\n  return new MutationBatch(dbBatch.batchId, timestamp, baseMutations, mutations);\n}\n/** Decodes a DbTarget into TargetData */\n\n\nfunction fromDbTarget(dbTarget) {\n  const version = fromDbTimestamp(dbTarget.readTime);\n  const lastLimboFreeSnapshotVersion = dbTarget.lastLimboFreeSnapshotVersion !== undefined ? fromDbTimestamp(dbTarget.lastLimboFreeSnapshotVersion) : SnapshotVersion.min();\n  let target;\n\n  if (isDocumentQuery(dbTarget.query)) {\n    target = fromDocumentsTarget(dbTarget.query);\n  } else {\n    target = fromQueryTarget(dbTarget.query);\n  }\n\n  return new TargetData(target, dbTarget.targetId, \"TargetPurposeListen\"\n  /* TargetPurpose.Listen */\n  , dbTarget.lastListenSequenceNumber, version, lastLimboFreeSnapshotVersion, ByteString.fromBase64String(dbTarget.resumeToken));\n}\n/** Encodes TargetData into a DbTarget for storage locally. */\n\n\nfunction toDbTarget(localSerializer, targetData) {\n  const dbTimestamp = toDbTimestamp(targetData.snapshotVersion);\n  const dbLastLimboFreeTimestamp = toDbTimestamp(targetData.lastLimboFreeSnapshotVersion);\n  let queryProto;\n\n  if (targetIsDocumentTarget(targetData.target)) {\n    queryProto = toDocumentsTarget(localSerializer.remoteSerializer, targetData.target);\n  } else {\n    queryProto = toQueryTarget(localSerializer.remoteSerializer, targetData.target);\n  } // We can't store the resumeToken as a ByteString in IndexedDb, so we\n  // convert it to a base64 string for storage.\n\n\n  const resumeToken = targetData.resumeToken.toBase64(); // lastListenSequenceNumber is always 0 until we do real GC.\n\n  return {\n    targetId: targetData.targetId,\n    canonicalId: canonifyTarget(targetData.target),\n    readTime: dbTimestamp,\n    resumeToken,\n    lastListenSequenceNumber: targetData.sequenceNumber,\n    lastLimboFreeSnapshotVersion: dbLastLimboFreeTimestamp,\n    query: queryProto\n  };\n}\n/**\r\n * A helper function for figuring out what kind of query has been stored.\r\n */\n\n\nfunction isDocumentQuery(dbQuery) {\n  return dbQuery.documents !== undefined;\n}\n/** Encodes a DbBundle to a BundleMetadata object. */\n\n\nfunction fromDbBundle(dbBundle) {\n  return {\n    id: dbBundle.bundleId,\n    createTime: fromDbTimestamp(dbBundle.createTime),\n    version: dbBundle.version\n  };\n}\n/** Encodes a BundleMetadata to a DbBundle. */\n\n\nfunction toDbBundle(metadata) {\n  return {\n    bundleId: metadata.id,\n    createTime: toDbTimestamp(fromVersion(metadata.createTime)),\n    version: metadata.version\n  };\n}\n/** Encodes a DbNamedQuery to a NamedQuery. */\n\n\nfunction fromDbNamedQuery(dbNamedQuery) {\n  return {\n    name: dbNamedQuery.name,\n    query: fromBundledQuery(dbNamedQuery.bundledQuery),\n    readTime: fromDbTimestamp(dbNamedQuery.readTime)\n  };\n}\n/** Encodes a NamedQuery from a bundle proto to a DbNamedQuery. */\n\n\nfunction toDbNamedQuery(query) {\n  return {\n    name: query.name,\n    readTime: toDbTimestamp(fromVersion(query.readTime)),\n    bundledQuery: query.bundledQuery\n  };\n}\n/**\r\n * Encodes a `BundledQuery` from bundle proto to a Query object.\r\n *\r\n * This reconstructs the original query used to build the bundle being loaded,\r\n * including features exists only in SDKs (for example: limit-to-last).\r\n */\n\n\nfunction fromBundledQuery(bundledQuery) {\n  const query = convertQueryTargetToQuery({\n    parent: bundledQuery.parent,\n    structuredQuery: bundledQuery.structuredQuery\n  });\n\n  if (bundledQuery.limitType === 'LAST') {\n    return queryWithLimit(query, query.limit, \"L\"\n    /* LimitType.Last */\n    );\n  }\n\n  return query;\n}\n/** Encodes a NamedQuery proto object to a NamedQuery model object. */\n\n\nfunction fromProtoNamedQuery(namedQuery) {\n  return {\n    name: namedQuery.name,\n    query: fromBundledQuery(namedQuery.bundledQuery),\n    readTime: fromVersion(namedQuery.readTime)\n  };\n}\n/** Decodes a BundleMetadata proto into a BundleMetadata object. */\n\n\nfunction fromBundleMetadata(metadata) {\n  return {\n    id: metadata.id,\n    version: metadata.version,\n    createTime: fromVersion(metadata.createTime)\n  };\n}\n/** Encodes a DbDocumentOverlay object to an Overlay model object. */\n\n\nfunction fromDbDocumentOverlay(localSerializer, dbDocumentOverlay) {\n  return new Overlay(dbDocumentOverlay.largestBatchId, fromMutation(localSerializer.remoteSerializer, dbDocumentOverlay.overlayMutation));\n}\n/** Decodes an Overlay model object into a DbDocumentOverlay object. */\n\n\nfunction toDbDocumentOverlay(localSerializer, userId, overlay) {\n  const [_, collectionPath, documentId] = toDbDocumentOverlayKey(userId, overlay.mutation.key);\n  return {\n    userId,\n    collectionPath,\n    documentId,\n    collectionGroup: overlay.mutation.key.getCollectionGroup(),\n    largestBatchId: overlay.largestBatchId,\n    overlayMutation: toMutation(localSerializer.remoteSerializer, overlay.mutation)\n  };\n}\n/**\r\n * Returns the DbDocumentOverlayKey corresponding to the given user and\r\n * document key.\r\n */\n\n\nfunction toDbDocumentOverlayKey(userId, docKey) {\n  const docId = docKey.path.lastSegment();\n  const collectionPath = encodeResourcePath(docKey.path.popLast());\n  return [userId, collectionPath, docId];\n}\n\nfunction toDbIndexConfiguration(index) {\n  return {\n    indexId: index.indexId,\n    collectionGroup: index.collectionGroup,\n    fields: index.fields.map(s => [s.fieldPath.canonicalString(), s.kind])\n  };\n}\n\nfunction fromDbIndexConfiguration(index, state) {\n  const decodedState = state ? new IndexState(state.sequenceNumber, new IndexOffset(fromDbTimestamp(state.readTime), new DocumentKey(decodeResourcePath(state.documentKey)), state.largestBatchId)) : IndexState.empty();\n  const decodedSegments = index.fields.map(([fieldPath, kind]) => new IndexSegment(FieldPath$1.fromServerFormat(fieldPath), kind));\n  return new FieldIndex(index.indexId, index.collectionGroup, decodedSegments, decodedState);\n}\n\nfunction toDbIndexState(indexId, user, sequenceNumber, offset) {\n  return {\n    indexId,\n    uid: user.uid || '',\n    sequenceNumber,\n    readTime: toDbTimestamp(offset.readTime),\n    documentKey: encodeResourcePath(offset.documentKey.path),\n    largestBatchId: offset.largestBatchId\n  };\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass IndexedDbBundleCache {\n  getBundleMetadata(transaction, bundleId) {\n    return bundlesStore(transaction).get(bundleId).next(bundle => {\n      if (bundle) {\n        return fromDbBundle(bundle);\n      }\n\n      return undefined;\n    });\n  }\n\n  saveBundleMetadata(transaction, bundleMetadata) {\n    return bundlesStore(transaction).put(toDbBundle(bundleMetadata));\n  }\n\n  getNamedQuery(transaction, queryName) {\n    return namedQueriesStore(transaction).get(queryName).next(query => {\n      if (query) {\n        return fromDbNamedQuery(query);\n      }\n\n      return undefined;\n    });\n  }\n\n  saveNamedQuery(transaction, query) {\n    return namedQueriesStore(transaction).put(toDbNamedQuery(query));\n  }\n\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the bundles object store.\r\n */\n\n\nfunction bundlesStore(txn) {\n  return getStore(txn, DbBundleStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the namedQueries object store.\r\n */\n\n\nfunction namedQueriesStore(txn) {\n  return getStore(txn, DbNamedQueryStore);\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Implementation of DocumentOverlayCache using IndexedDb.\r\n */\n\n\nclass IndexedDbDocumentOverlayCache {\n  /**\r\n   * @param serializer - The document serializer.\r\n   * @param userId - The userId for which we are accessing overlays.\r\n   */\n  constructor(serializer, userId) {\n    this.serializer = serializer;\n    this.userId = userId;\n  }\n\n  static forUser(serializer, user) {\n    const userId = user.uid || '';\n    return new IndexedDbDocumentOverlayCache(serializer, userId);\n  }\n\n  getOverlay(transaction, key) {\n    return documentOverlayStore(transaction).get(toDbDocumentOverlayKey(this.userId, key)).next(dbOverlay => {\n      if (dbOverlay) {\n        return fromDbDocumentOverlay(this.serializer, dbOverlay);\n      }\n\n      return null;\n    });\n  }\n\n  getOverlays(transaction, keys) {\n    const result = newOverlayMap();\n    return PersistencePromise.forEach(keys, key => {\n      return this.getOverlay(transaction, key).next(overlay => {\n        if (overlay !== null) {\n          result.set(key, overlay);\n        }\n      });\n    }).next(() => result);\n  }\n\n  saveOverlays(transaction, largestBatchId, overlays) {\n    const promises = [];\n    overlays.forEach((_, mutation) => {\n      const overlay = new Overlay(largestBatchId, mutation);\n      promises.push(this.saveOverlay(transaction, overlay));\n    });\n    return PersistencePromise.waitFor(promises);\n  }\n\n  removeOverlaysForBatchId(transaction, documentKeys, batchId) {\n    const collectionPaths = new Set(); // Get the set of unique collection paths.\n\n    documentKeys.forEach(key => collectionPaths.add(encodeResourcePath(key.getCollectionPath())));\n    const promises = [];\n    collectionPaths.forEach(collectionPath => {\n      const range = IDBKeyRange.bound([this.userId, collectionPath, batchId], [this.userId, collectionPath, batchId + 1],\n      /*lowerOpen=*/\n      false,\n      /*upperOpen=*/\n      true);\n      promises.push(documentOverlayStore(transaction).deleteAll(DbDocumentOverlayCollectionPathOverlayIndex, range));\n    });\n    return PersistencePromise.waitFor(promises);\n  }\n\n  getOverlaysForCollection(transaction, collection, sinceBatchId) {\n    const result = newOverlayMap();\n    const collectionPath = encodeResourcePath(collection); // We want batch IDs larger than `sinceBatchId`, and so the lower bound\n    // is not inclusive.\n\n    const range = IDBKeyRange.bound([this.userId, collectionPath, sinceBatchId], [this.userId, collectionPath, Number.POSITIVE_INFINITY],\n    /*lowerOpen=*/\n    true);\n    return documentOverlayStore(transaction).loadAll(DbDocumentOverlayCollectionPathOverlayIndex, range).next(dbOverlays => {\n      for (const dbOverlay of dbOverlays) {\n        const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\n        result.set(overlay.getKey(), overlay);\n      }\n\n      return result;\n    });\n  }\n\n  getOverlaysForCollectionGroup(transaction, collectionGroup, sinceBatchId, count) {\n    const result = newOverlayMap();\n    let currentBatchId = undefined; // We want batch IDs larger than `sinceBatchId`, and so the lower bound\n    // is not inclusive.\n\n    const range = IDBKeyRange.bound([this.userId, collectionGroup, sinceBatchId], [this.userId, collectionGroup, Number.POSITIVE_INFINITY],\n    /*lowerOpen=*/\n    true);\n    return documentOverlayStore(transaction).iterate({\n      index: DbDocumentOverlayCollectionGroupOverlayIndex,\n      range\n    }, (_, dbOverlay, control) => {\n      // We do not want to return partial batch overlays, even if the size\n      // of the result set exceeds the given `count` argument. Therefore, we\n      // continue to aggregate results even after the result size exceeds\n      // `count` if there are more overlays from the `currentBatchId`.\n      const overlay = fromDbDocumentOverlay(this.serializer, dbOverlay);\n\n      if (result.size() < count || overlay.largestBatchId === currentBatchId) {\n        result.set(overlay.getKey(), overlay);\n        currentBatchId = overlay.largestBatchId;\n      } else {\n        control.done();\n      }\n    }).next(() => result);\n  }\n\n  saveOverlay(transaction, overlay) {\n    return documentOverlayStore(transaction).put(toDbDocumentOverlay(this.serializer, this.userId, overlay));\n  }\n\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the document overlay object store.\r\n */\n\n\nfunction documentOverlayStore(txn) {\n  return getStore(txn, DbDocumentOverlayStore);\n}\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// Note: This code is copied from the backend. Code that is not used by\n// Firestore was removed.\n\n\nconst INDEX_TYPE_NULL = 5;\nconst INDEX_TYPE_BOOLEAN = 10;\nconst INDEX_TYPE_NAN = 13;\nconst INDEX_TYPE_NUMBER = 15;\nconst INDEX_TYPE_TIMESTAMP = 20;\nconst INDEX_TYPE_STRING = 25;\nconst INDEX_TYPE_BLOB = 30;\nconst INDEX_TYPE_REFERENCE = 37;\nconst INDEX_TYPE_GEOPOINT = 45;\nconst INDEX_TYPE_ARRAY = 50;\nconst INDEX_TYPE_MAP = 55;\nconst INDEX_TYPE_REFERENCE_SEGMENT = 60; // A terminator that indicates that a truncatable value was not truncated.\n// This must be smaller than all other type labels.\n\nconst NOT_TRUNCATED = 2;\n/** Firestore index value writer.  */\n\nclass FirestoreIndexValueWriter {\n  constructor() {} // The write methods below short-circuit writing terminators for values\n  // containing a (terminating) truncated value.\n  //\n  // As an example, consider the resulting encoding for:\n  //\n  // [\"bar\", [2, \"foo\"]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TERM, TERM, TERM)\n  // [\"bar\", [2, truncated(\"foo\")]] -> (STRING, \"bar\", TERM, ARRAY, NUMBER, 2, STRING, \"foo\", TRUNC)\n  // [\"bar\", truncated([\"foo\"])] -> (STRING, \"bar\", TERM, ARRAY. STRING, \"foo\", TERM, TRUNC)\n\n  /** Writes an index value.  */\n\n\n  writeIndexValue(value, encoder) {\n    this.writeIndexValueAux(value, encoder); // Write separator to split index values\n    // (see go/firestore-storage-format#encodings).\n\n    encoder.writeInfinity();\n  }\n\n  writeIndexValueAux(indexValue, encoder) {\n    if ('nullValue' in indexValue) {\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_NULL);\n    } else if ('booleanValue' in indexValue) {\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_BOOLEAN);\n      encoder.writeNumber(indexValue.booleanValue ? 1 : 0);\n    } else if ('integerValue' in indexValue) {\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\n      encoder.writeNumber(normalizeNumber(indexValue.integerValue));\n    } else if ('doubleValue' in indexValue) {\n      const n = normalizeNumber(indexValue.doubleValue);\n\n      if (isNaN(n)) {\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_NAN);\n      } else {\n        this.writeValueTypeLabel(encoder, INDEX_TYPE_NUMBER);\n\n        if (isNegativeZero(n)) {\n          // -0.0, 0 and 0.0 are all considered the same\n          encoder.writeNumber(0.0);\n        } else {\n          encoder.writeNumber(n);\n        }\n      }\n    } else if ('timestampValue' in indexValue) {\n      const timestamp = indexValue.timestampValue;\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_TIMESTAMP);\n\n      if (typeof timestamp === 'string') {\n        encoder.writeString(timestamp);\n      } else {\n        encoder.writeString(`${timestamp.seconds || ''}`);\n        encoder.writeNumber(timestamp.nanos || 0);\n      }\n    } else if ('stringValue' in indexValue) {\n      this.writeIndexString(indexValue.stringValue, encoder);\n      this.writeTruncationMarker(encoder);\n    } else if ('bytesValue' in indexValue) {\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_BLOB);\n      encoder.writeBytes(normalizeByteString(indexValue.bytesValue));\n      this.writeTruncationMarker(encoder);\n    } else if ('referenceValue' in indexValue) {\n      this.writeIndexEntityRef(indexValue.referenceValue, encoder);\n    } else if ('geoPointValue' in indexValue) {\n      const geoPoint = indexValue.geoPointValue;\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_GEOPOINT);\n      encoder.writeNumber(geoPoint.latitude || 0);\n      encoder.writeNumber(geoPoint.longitude || 0);\n    } else if ('mapValue' in indexValue) {\n      if (isMaxValue(indexValue)) {\n        this.writeValueTypeLabel(encoder, Number.MAX_SAFE_INTEGER);\n      } else {\n        this.writeIndexMap(indexValue.mapValue, encoder);\n        this.writeTruncationMarker(encoder);\n      }\n    } else if ('arrayValue' in indexValue) {\n      this.writeIndexArray(indexValue.arrayValue, encoder);\n      this.writeTruncationMarker(encoder);\n    } else {\n      fail();\n    }\n  }\n\n  writeIndexString(stringIndexValue, encoder) {\n    this.writeValueTypeLabel(encoder, INDEX_TYPE_STRING);\n    this.writeUnlabeledIndexString(stringIndexValue, encoder);\n  }\n\n  writeUnlabeledIndexString(stringIndexValue, encoder) {\n    encoder.writeString(stringIndexValue);\n  }\n\n  writeIndexMap(mapIndexValue, encoder) {\n    const map = mapIndexValue.fields || {};\n    this.writeValueTypeLabel(encoder, INDEX_TYPE_MAP);\n\n    for (const key of Object.keys(map)) {\n      this.writeIndexString(key, encoder);\n      this.writeIndexValueAux(map[key], encoder);\n    }\n  }\n\n  writeIndexArray(arrayIndexValue, encoder) {\n    const values = arrayIndexValue.values || [];\n    this.writeValueTypeLabel(encoder, INDEX_TYPE_ARRAY);\n\n    for (const element of values) {\n      this.writeIndexValueAux(element, encoder);\n    }\n  }\n\n  writeIndexEntityRef(referenceValue, encoder) {\n    this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE);\n    const path = DocumentKey.fromName(referenceValue).path;\n    path.forEach(segment => {\n      this.writeValueTypeLabel(encoder, INDEX_TYPE_REFERENCE_SEGMENT);\n      this.writeUnlabeledIndexString(segment, encoder);\n    });\n  }\n\n  writeValueTypeLabel(encoder, typeOrder) {\n    encoder.writeNumber(typeOrder);\n  }\n\n  writeTruncationMarker(encoder) {\n    // While the SDK does not implement truncation, the truncation marker is\n    // used to terminate all variable length values (which are strings, bytes,\n    // references, arrays and maps).\n    encoder.writeNumber(NOT_TRUNCATED);\n  }\n\n}\n\nFirestoreIndexValueWriter.INSTANCE = new FirestoreIndexValueWriter();\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law | agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES | CONDITIONS OF ANY KIND, either express | implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** These constants are taken from the backend. */\n\nconst MIN_SURROGATE = '\\uD800';\nconst MAX_SURROGATE = '\\uDBFF';\nconst ESCAPE1 = 0x00;\nconst NULL_BYTE = 0xff; // Combined with ESCAPE1\n\nconst SEPARATOR = 0x01; // Combined with ESCAPE1\n\nconst ESCAPE2 = 0xff;\nconst INFINITY = 0xff; // Combined with ESCAPE2\n\nconst FF_BYTE = 0x00; // Combined with ESCAPE2\n\nconst LONG_SIZE = 64;\nconst BYTE_SIZE = 8;\n/**\r\n * The default size of the buffer. This is arbitrary, but likely larger than\r\n * most index values so that less copies of the underlying buffer will be made.\r\n * For large values, a single copy will made to double the buffer length.\r\n */\n\nconst DEFAULT_BUFFER_SIZE = 1024;\n/** Converts a JavaScript number to a byte array (using big endian encoding). */\n\nfunction doubleToLongBits(value) {\n  const dv = new DataView(new ArrayBuffer(8));\n  dv.setFloat64(0, value,\n  /* littleEndian= */\n  false);\n  return new Uint8Array(dv.buffer);\n}\n/**\r\n * Counts the number of zeros in a byte.\r\n *\r\n * Visible for testing.\r\n */\n\n\nfunction numberOfLeadingZerosInByte(x) {\n  if (x === 0) {\n    return 8;\n  }\n\n  let zeros = 0;\n\n  if (x >> 4 === 0) {\n    // Test if the first four bits are zero.\n    zeros += 4;\n    x = x << 4;\n  }\n\n  if (x >> 6 === 0) {\n    // Test if the first two (or next two) bits are zero.\n    zeros += 2;\n    x = x << 2;\n  }\n\n  if (x >> 7 === 0) {\n    // Test if the remaining bit is zero.\n    zeros += 1;\n  }\n\n  return zeros;\n}\n/** Counts the number of leading zeros in the given byte array. */\n\n\nfunction numberOfLeadingZeros(bytes) {\n  let leadingZeros = 0;\n\n  for (let i = 0; i < 8; ++i) {\n    const zeros = numberOfLeadingZerosInByte(bytes[i] & 0xff);\n    leadingZeros += zeros;\n\n    if (zeros !== 8) {\n      break;\n    }\n  }\n\n  return leadingZeros;\n}\n/**\r\n * Returns the number of bytes required to store \"value\". Leading zero bytes\r\n * are skipped.\r\n */\n\n\nfunction unsignedNumLength(value) {\n  // This is just the number of bytes for the unsigned representation of the number.\n  const numBits = LONG_SIZE - numberOfLeadingZeros(value);\n  return Math.ceil(numBits / BYTE_SIZE);\n}\n/**\r\n * OrderedCodeWriter is a minimal-allocation implementation of the writing\r\n * behavior defined by the backend.\r\n *\r\n * The code is ported from its Java counterpart.\r\n */\n\n\nclass OrderedCodeWriter {\n  constructor() {\n    this.buffer = new Uint8Array(DEFAULT_BUFFER_SIZE);\n    this.position = 0;\n  }\n\n  writeBytesAscending(value) {\n    const it = value[Symbol.iterator]();\n    let byte = it.next();\n\n    while (!byte.done) {\n      this.writeByteAscending(byte.value);\n      byte = it.next();\n    }\n\n    this.writeSeparatorAscending();\n  }\n\n  writeBytesDescending(value) {\n    const it = value[Symbol.iterator]();\n    let byte = it.next();\n\n    while (!byte.done) {\n      this.writeByteDescending(byte.value);\n      byte = it.next();\n    }\n\n    this.writeSeparatorDescending();\n  }\n  /** Writes utf8 bytes into this byte sequence, ascending. */\n\n\n  writeUtf8Ascending(sequence) {\n    for (const c of sequence) {\n      const charCode = c.charCodeAt(0);\n\n      if (charCode < 0x80) {\n        this.writeByteAscending(charCode);\n      } else if (charCode < 0x800) {\n        this.writeByteAscending(0x0f << 6 | charCode >>> 6);\n        this.writeByteAscending(0x80 | 0x3f & charCode);\n      } else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\n        this.writeByteAscending(0x0f << 5 | charCode >>> 12);\n        this.writeByteAscending(0x80 | 0x3f & charCode >>> 6);\n        this.writeByteAscending(0x80 | 0x3f & charCode);\n      } else {\n        const codePoint = c.codePointAt(0);\n        this.writeByteAscending(0x0f << 4 | codePoint >>> 18);\n        this.writeByteAscending(0x80 | 0x3f & codePoint >>> 12);\n        this.writeByteAscending(0x80 | 0x3f & codePoint >>> 6);\n        this.writeByteAscending(0x80 | 0x3f & codePoint);\n      }\n    }\n\n    this.writeSeparatorAscending();\n  }\n  /** Writes utf8 bytes into this byte sequence, descending */\n\n\n  writeUtf8Descending(sequence) {\n    for (const c of sequence) {\n      const charCode = c.charCodeAt(0);\n\n      if (charCode < 0x80) {\n        this.writeByteDescending(charCode);\n      } else if (charCode < 0x800) {\n        this.writeByteDescending(0x0f << 6 | charCode >>> 6);\n        this.writeByteDescending(0x80 | 0x3f & charCode);\n      } else if (c < MIN_SURROGATE || MAX_SURROGATE < c) {\n        this.writeByteDescending(0x0f << 5 | charCode >>> 12);\n        this.writeByteDescending(0x80 | 0x3f & charCode >>> 6);\n        this.writeByteDescending(0x80 | 0x3f & charCode);\n      } else {\n        const codePoint = c.codePointAt(0);\n        this.writeByteDescending(0x0f << 4 | codePoint >>> 18);\n        this.writeByteDescending(0x80 | 0x3f & codePoint >>> 12);\n        this.writeByteDescending(0x80 | 0x3f & codePoint >>> 6);\n        this.writeByteDescending(0x80 | 0x3f & codePoint);\n      }\n    }\n\n    this.writeSeparatorDescending();\n  }\n\n  writeNumberAscending(val) {\n    // Values are encoded with a single byte length prefix, followed by the\n    // actual value in big-endian format with leading 0 bytes dropped.\n    const value = this.toOrderedBits(val);\n    const len = unsignedNumLength(value);\n    this.ensureAvailable(1 + len);\n    this.buffer[this.position++] = len & 0xff; // Write the length\n\n    for (let i = value.length - len; i < value.length; ++i) {\n      this.buffer[this.position++] = value[i] & 0xff;\n    }\n  }\n\n  writeNumberDescending(val) {\n    // Values are encoded with a single byte length prefix, followed by the\n    // inverted value in big-endian format with leading 0 bytes dropped.\n    const value = this.toOrderedBits(val);\n    const len = unsignedNumLength(value);\n    this.ensureAvailable(1 + len);\n    this.buffer[this.position++] = ~(len & 0xff); // Write the length\n\n    for (let i = value.length - len; i < value.length; ++i) {\n      this.buffer[this.position++] = ~(value[i] & 0xff);\n    }\n  }\n  /**\r\n   * Writes the \"infinity\" byte sequence that sorts after all other byte\r\n   * sequences written in ascending order.\r\n   */\n\n\n  writeInfinityAscending() {\n    this.writeEscapedByteAscending(ESCAPE2);\n    this.writeEscapedByteAscending(INFINITY);\n  }\n  /**\r\n   * Writes the \"infinity\" byte sequence that sorts before all other byte\r\n   * sequences written in descending order.\r\n   */\n\n\n  writeInfinityDescending() {\n    this.writeEscapedByteDescending(ESCAPE2);\n    this.writeEscapedByteDescending(INFINITY);\n  }\n  /**\r\n   * Resets the buffer such that it is the same as when it was newly\r\n   * constructed.\r\n   */\n\n\n  reset() {\n    this.position = 0;\n  }\n\n  seed(encodedBytes) {\n    this.ensureAvailable(encodedBytes.length);\n    this.buffer.set(encodedBytes, this.position);\n    this.position += encodedBytes.length;\n  }\n  /** Makes a copy of the encoded bytes in this buffer.  */\n\n\n  encodedBytes() {\n    return this.buffer.slice(0, this.position);\n  }\n  /**\r\n   * Encodes `val` into an encoding so that the order matches the IEEE 754\r\n   * floating-point comparison results with the following exceptions:\r\n   *   -0.0 < 0.0\r\n   *   all non-NaN < NaN\r\n   *   NaN = NaN\r\n   */\n\n\n  toOrderedBits(val) {\n    const value = doubleToLongBits(val); // Check if the first bit is set. We use a bit mask since value[0] is\n    // encoded as a number from 0 to 255.\n\n    const isNegative = (value[0] & 0x80) !== 0; // Revert the two complement to get natural ordering\n\n    value[0] ^= isNegative ? 0xff : 0x80;\n\n    for (let i = 1; i < value.length; ++i) {\n      value[i] ^= isNegative ? 0xff : 0x00;\n    }\n\n    return value;\n  }\n  /** Writes a single byte ascending to the buffer. */\n\n\n  writeByteAscending(b) {\n    const masked = b & 0xff;\n\n    if (masked === ESCAPE1) {\n      this.writeEscapedByteAscending(ESCAPE1);\n      this.writeEscapedByteAscending(NULL_BYTE);\n    } else if (masked === ESCAPE2) {\n      this.writeEscapedByteAscending(ESCAPE2);\n      this.writeEscapedByteAscending(FF_BYTE);\n    } else {\n      this.writeEscapedByteAscending(masked);\n    }\n  }\n  /** Writes a single byte descending to the buffer.  */\n\n\n  writeByteDescending(b) {\n    const masked = b & 0xff;\n\n    if (masked === ESCAPE1) {\n      this.writeEscapedByteDescending(ESCAPE1);\n      this.writeEscapedByteDescending(NULL_BYTE);\n    } else if (masked === ESCAPE2) {\n      this.writeEscapedByteDescending(ESCAPE2);\n      this.writeEscapedByteDescending(FF_BYTE);\n    } else {\n      this.writeEscapedByteDescending(b);\n    }\n  }\n\n  writeSeparatorAscending() {\n    this.writeEscapedByteAscending(ESCAPE1);\n    this.writeEscapedByteAscending(SEPARATOR);\n  }\n\n  writeSeparatorDescending() {\n    this.writeEscapedByteDescending(ESCAPE1);\n    this.writeEscapedByteDescending(SEPARATOR);\n  }\n\n  writeEscapedByteAscending(b) {\n    this.ensureAvailable(1);\n    this.buffer[this.position++] = b;\n  }\n\n  writeEscapedByteDescending(b) {\n    this.ensureAvailable(1);\n    this.buffer[this.position++] = ~b;\n  }\n\n  ensureAvailable(bytes) {\n    const minCapacity = bytes + this.position;\n\n    if (minCapacity <= this.buffer.length) {\n      return;\n    } // Try doubling.\n\n\n    let newLength = this.buffer.length * 2; // Still not big enough? Just allocate the right size.\n\n    if (newLength < minCapacity) {\n      newLength = minCapacity;\n    } // Create the new buffer.\n\n\n    const newBuffer = new Uint8Array(newLength);\n    newBuffer.set(this.buffer); // copy old data\n\n    this.buffer = newBuffer;\n  }\n\n}\n\nclass AscendingIndexByteEncoder {\n  constructor(orderedCode) {\n    this.orderedCode = orderedCode;\n  }\n\n  writeBytes(value) {\n    this.orderedCode.writeBytesAscending(value);\n  }\n\n  writeString(value) {\n    this.orderedCode.writeUtf8Ascending(value);\n  }\n\n  writeNumber(value) {\n    this.orderedCode.writeNumberAscending(value);\n  }\n\n  writeInfinity() {\n    this.orderedCode.writeInfinityAscending();\n  }\n\n}\n\nclass DescendingIndexByteEncoder {\n  constructor(orderedCode) {\n    this.orderedCode = orderedCode;\n  }\n\n  writeBytes(value) {\n    this.orderedCode.writeBytesDescending(value);\n  }\n\n  writeString(value) {\n    this.orderedCode.writeUtf8Descending(value);\n  }\n\n  writeNumber(value) {\n    this.orderedCode.writeNumberDescending(value);\n  }\n\n  writeInfinity() {\n    this.orderedCode.writeInfinityDescending();\n  }\n\n}\n/**\r\n * Implements `DirectionalIndexByteEncoder` using `OrderedCodeWriter` for the\r\n * actual encoding.\r\n */\n\n\nclass IndexByteEncoder {\n  constructor() {\n    this.orderedCode = new OrderedCodeWriter();\n    this.ascending = new AscendingIndexByteEncoder(this.orderedCode);\n    this.descending = new DescendingIndexByteEncoder(this.orderedCode);\n  }\n\n  seed(encodedBytes) {\n    this.orderedCode.seed(encodedBytes);\n  }\n\n  forKind(kind) {\n    return kind === 0\n    /* IndexKind.ASCENDING */\n    ? this.ascending : this.descending;\n  }\n\n  encodedBytes() {\n    return this.orderedCode.encodedBytes();\n  }\n\n  reset() {\n    this.orderedCode.reset();\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Represents an index entry saved by the SDK in persisted storage. */\n\n\nclass IndexEntry {\n  constructor(indexId, documentKey, arrayValue, directionalValue) {\n    this.indexId = indexId;\n    this.documentKey = documentKey;\n    this.arrayValue = arrayValue;\n    this.directionalValue = directionalValue;\n  }\n  /**\r\n   * Returns an IndexEntry entry that sorts immediately after the current\r\n   * directional value.\r\n   */\n\n\n  successor() {\n    const currentLength = this.directionalValue.length;\n    const newLength = currentLength === 0 || this.directionalValue[currentLength - 1] === 255 ? currentLength + 1 : currentLength;\n    const successor = new Uint8Array(newLength);\n    successor.set(this.directionalValue, 0);\n\n    if (newLength !== currentLength) {\n      successor.set([0], this.directionalValue.length);\n    } else {\n      ++successor[successor.length - 1];\n    }\n\n    return new IndexEntry(this.indexId, this.documentKey, this.arrayValue, successor);\n  }\n\n}\n\nfunction indexEntryComparator(left, right) {\n  let cmp = left.indexId - right.indexId;\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  cmp = compareByteArrays(left.arrayValue, right.arrayValue);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  cmp = compareByteArrays(left.directionalValue, right.directionalValue);\n\n  if (cmp !== 0) {\n    return cmp;\n  }\n\n  return DocumentKey.comparator(left.documentKey, right.documentKey);\n}\n\nfunction compareByteArrays(left, right) {\n  for (let i = 0; i < left.length && i < right.length; ++i) {\n    const compare = left[i] - right[i];\n\n    if (compare !== 0) {\n      return compare;\n    }\n  }\n\n  return left.length - right.length;\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A light query planner for Firestore.\r\n *\r\n * This class matches a `FieldIndex` against a Firestore Query `Target`. It\r\n * determines whether a given index can be used to serve the specified target.\r\n *\r\n * The following table showcases some possible index configurations:\r\n *\r\n * Query                                               | Index\r\n * -----------------------------------------------------------------------------\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC, b DESC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | a ASC\r\n * where('a', '==', 'a').where('b', '==', 'b')         | b DESC\r\n * where('a', '>=', 'a').orderBy('a')                  | a ASC\r\n * where('a', '>=', 'a').orderBy('a', 'desc')          | a DESC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC, b ASC\r\n * where('a', '>=', 'a').orderBy('a').orderBy('b')     | a ASC\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS, b ASCENDING\r\n * where('a', 'array-contains', 'a').orderBy('b')      | a CONTAINS\r\n */\n\n\nclass TargetIndexMatcher {\n  constructor(target) {\n    this.collectionId = target.collectionGroup != null ? target.collectionGroup : target.path.lastSegment();\n    this.orderBys = target.orderBy;\n    this.equalityFilters = [];\n\n    for (const filter of target.filters) {\n      const fieldFilter = filter;\n\n      if (fieldFilter.isInequality()) {\n        this.inequalityFilter = fieldFilter;\n      } else {\n        this.equalityFilters.push(fieldFilter);\n      }\n    }\n  }\n  /**\r\n   * Returns whether the index can be used to serve the TargetIndexMatcher's\r\n   * target.\r\n   *\r\n   * An index is considered capable of serving the target when:\r\n   * - The target uses all index segments for its filters and orderBy clauses.\r\n   *   The target can have additional filter and orderBy clauses, but not\r\n   *   fewer.\r\n   * - If an ArrayContains/ArrayContainsAnyfilter is used, the index must also\r\n   *   have a corresponding `CONTAINS` segment.\r\n   * - All directional index segments can be mapped to the target as a series of\r\n   *   equality filters, a single inequality filter and a series of orderBy\r\n   *   clauses.\r\n   * - The segments that represent the equality filters may appear out of order.\r\n   * - The optional segment for the inequality filter must appear after all\r\n   *   equality segments.\r\n   * - The segments that represent that orderBy clause of the target must appear\r\n   *   in order after all equality and inequality segments. Single orderBy\r\n   *   clauses cannot be skipped, but a continuous orderBy suffix may be\r\n   *   omitted.\r\n   */\n\n\n  servedByIndex(index) {\n    hardAssert(index.collectionGroup === this.collectionId); // If there is an array element, find a matching filter.\n\n    const arraySegment = fieldIndexGetArraySegment(index);\n\n    if (arraySegment !== undefined && !this.hasMatchingEqualityFilter(arraySegment)) {\n      return false;\n    }\n\n    const segments = fieldIndexGetDirectionalSegments(index);\n    let equalitySegments = new Set();\n    let segmentIndex = 0;\n    let orderBysIndex = 0; // Process all equalities first. Equalities can appear out of order.\n\n    for (; segmentIndex < segments.length; ++segmentIndex) {\n      // We attempt to greedily match all segments to equality filters. If a\n      // filter matches an index segment, we can mark the segment as used.\n      if (this.hasMatchingEqualityFilter(segments[segmentIndex])) {\n        equalitySegments = equalitySegments.add(segments[segmentIndex].fieldPath.canonicalString());\n      } else {\n        // If we cannot find a matching filter, we need to verify whether the\n        // remaining segments map to the target's inequality and its orderBy\n        // clauses.\n        break;\n      }\n    } // If we already have processed all segments, all segments are used to serve\n    // the equality filters and we do not need to map any segments to the\n    // target's inequality and orderBy clauses.\n\n\n    if (segmentIndex === segments.length) {\n      return true;\n    }\n\n    if (this.inequalityFilter !== undefined) {\n      // If there is an inequality filter and the field was not in one of the\n      // equality filters above, the next segment must match both the filter\n      // and the first orderBy clause.\n      if (!equalitySegments.has(this.inequalityFilter.field.canonicalString())) {\n        const segment = segments[segmentIndex];\n\n        if (!this.matchesFilter(this.inequalityFilter, segment) || !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\n          return false;\n        }\n      }\n\n      ++segmentIndex;\n    } // All remaining segments need to represent the prefix of the target's\n    // orderBy.\n\n\n    for (; segmentIndex < segments.length; ++segmentIndex) {\n      const segment = segments[segmentIndex];\n\n      if (orderBysIndex >= this.orderBys.length || !this.matchesOrderBy(this.orderBys[orderBysIndex++], segment)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  hasMatchingEqualityFilter(segment) {\n    for (const filter of this.equalityFilters) {\n      if (this.matchesFilter(filter, segment)) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n\n  matchesFilter(filter, segment) {\n    if (filter === undefined || !filter.field.isEqual(segment.fieldPath)) {\n      return false;\n    }\n\n    const isArrayOperator = filter.op === \"array-contains\"\n    /* Operator.ARRAY_CONTAINS */\n    || filter.op === \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    ;\n    return segment.kind === 2\n    /* IndexKind.CONTAINS */\n    === isArrayOperator;\n  }\n\n  matchesOrderBy(orderBy, segment) {\n    if (!orderBy.field.isEqual(segment.fieldPath)) {\n      return false;\n    }\n\n    return segment.kind === 0\n    /* IndexKind.ASCENDING */\n    && orderBy.dir === \"asc\"\n    /* Direction.ASCENDING */\n    || segment.kind === 1\n    /* IndexKind.DESCENDING */\n    && orderBy.dir === \"desc\"\n    /* Direction.DESCENDING */\n    ;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Provides utility functions that help with boolean logic transformations needed for handling\r\n * complex filters used in queries.\r\n */\n\n/**\r\n * The `in` filter is only a syntactic sugar over a disjunction of equalities. For instance: `a in\r\n * [1,2,3]` is in fact `a==1 || a==2 || a==3`. This method expands any `in` filter in the given\r\n * input into a disjunction of equality filters and returns the expanded filter.\r\n */\n\n\nfunction computeInExpansion(filter) {\n  var _a, _b;\n\n  hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\n\n  if (filter instanceof FieldFilter) {\n    if (filter instanceof InFilter) {\n      const expandedFilters = ((_b = (_a = filter.value.arrayValue) === null || _a === void 0 ? void 0 : _a.values) === null || _b === void 0 ? void 0 : _b.map(value => FieldFilter.create(filter.field, \"==\"\n      /* Operator.EQUAL */\n      , value))) || [];\n      return CompositeFilter.create(expandedFilters, \"or\"\n      /* CompositeOperator.OR */\n      );\n    } else {\n      // We have reached other kinds of field filters.\n      return filter;\n    }\n  } // We have a composite filter.\n\n\n  const expandedFilters = filter.filters.map(subfilter => computeInExpansion(subfilter));\n  return CompositeFilter.create(expandedFilters, filter.op);\n}\n/**\r\n * Given a composite filter, returns the list of terms in its disjunctive normal form.\r\n *\r\n * <p>Each element in the return value is one term of the resulting DNF. For instance: For the\r\n * input: (A || B) && C, the DNF form is: (A && C) || (B && C), and the return value is a list\r\n * with two elements: a composite filter that performs (A && C), and a composite filter that\r\n * performs (B && C).\r\n *\r\n * @param filter the composite filter to calculate DNF transform for.\r\n * @return the terms in the DNF transform.\r\n */\n\n\nfunction getDnfTerms(filter) {\n  if (filter.getFilters().length === 0) {\n    return [];\n  }\n\n  const result = computeDistributedNormalForm(computeInExpansion(filter));\n  hardAssert(isDisjunctiveNormalForm(result));\n\n  if (isSingleFieldFilter(result) || isFlatConjunction(result)) {\n    return [result];\n  }\n\n  return result.getFilters();\n}\n/** Returns true if the given filter is a single field filter. e.g. (a == 10). */\n\n\nfunction isSingleFieldFilter(filter) {\n  return filter instanceof FieldFilter;\n}\n/**\r\n * Returns true if the given filter is the conjunction of one or more field filters. e.g. (a == 10\r\n * && b == 20)\r\n */\n\n\nfunction isFlatConjunction(filter) {\n  return filter instanceof CompositeFilter && compositeFilterIsFlatConjunction(filter);\n}\n/**\r\n * Returns whether or not the given filter is in disjunctive normal form (DNF).\r\n *\r\n * <p>In boolean logic, a disjunctive normal form (DNF) is a canonical normal form of a logical\r\n * formula consisting of a disjunction of conjunctions; it can also be described as an OR of ANDs.\r\n *\r\n * <p>For more info, visit: https://en.wikipedia.org/wiki/Disjunctive_normal_form\r\n */\n\n\nfunction isDisjunctiveNormalForm(filter) {\n  return isSingleFieldFilter(filter) || isFlatConjunction(filter) || isDisjunctionOfFieldFiltersAndFlatConjunctions(filter);\n}\n/**\r\n * Returns true if the given filter is the disjunction of one or more \"flat conjunctions\" and\r\n * field filters. e.g. (a == 10) || (b==20 && c==30)\r\n */\n\n\nfunction isDisjunctionOfFieldFiltersAndFlatConjunctions(filter) {\n  if (filter instanceof CompositeFilter) {\n    if (compositeFilterIsDisjunction(filter)) {\n      for (const subFilter of filter.getFilters()) {\n        if (!isSingleFieldFilter(subFilter) && !isFlatConjunction(subFilter)) {\n          return false;\n        }\n      }\n\n      return true;\n    }\n  }\n\n  return false;\n}\n\nfunction computeDistributedNormalForm(filter) {\n  hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\n\n  if (filter instanceof FieldFilter) {\n    return filter;\n  }\n\n  if (filter.filters.length === 1) {\n    return computeDistributedNormalForm(filter.filters[0]);\n  } // Compute DNF for each of the subfilters first\n\n\n  const result = filter.filters.map(subfilter => computeDistributedNormalForm(subfilter));\n  let newFilter = CompositeFilter.create(result, filter.op);\n  newFilter = applyAssociation(newFilter);\n\n  if (isDisjunctiveNormalForm(newFilter)) {\n    return newFilter;\n  }\n\n  hardAssert(newFilter instanceof CompositeFilter);\n  hardAssert(compositeFilterIsConjunction(newFilter));\n  hardAssert(newFilter.filters.length > 1);\n  return newFilter.filters.reduce((runningResult, filter) => applyDistribution(runningResult, filter));\n}\n\nfunction applyDistribution(lhs, rhs) {\n  hardAssert(lhs instanceof FieldFilter || lhs instanceof CompositeFilter);\n  hardAssert(rhs instanceof FieldFilter || rhs instanceof CompositeFilter);\n  let result;\n\n  if (lhs instanceof FieldFilter) {\n    if (rhs instanceof FieldFilter) {\n      // FieldFilter FieldFilter\n      result = applyDistributionFieldFilters(lhs, rhs);\n    } else {\n      // FieldFilter CompositeFilter\n      result = applyDistributionFieldAndCompositeFilters(lhs, rhs);\n    }\n  } else {\n    if (rhs instanceof FieldFilter) {\n      // CompositeFilter FieldFilter\n      result = applyDistributionFieldAndCompositeFilters(rhs, lhs);\n    } else {\n      // CompositeFilter CompositeFilter\n      result = applyDistributionCompositeFilters(lhs, rhs);\n    }\n  }\n\n  return applyAssociation(result);\n}\n\nfunction applyDistributionFieldFilters(lhs, rhs) {\n  // Conjunction distribution for two field filters is the conjunction of them.\n  return CompositeFilter.create([lhs, rhs], \"and\"\n  /* CompositeOperator.AND */\n  );\n}\n\nfunction applyDistributionCompositeFilters(lhs, rhs) {\n  hardAssert(lhs.filters.length > 0 && rhs.filters.length > 0); // There are four cases:\n  // (A & B) & (C & D) --> (A & B & C & D)\n  // (A & B) & (C | D) --> (A & B & C) | (A & B & D)\n  // (A | B) & (C & D) --> (C & D & A) | (C & D & B)\n  // (A | B) & (C | D) --> (A & C) | (A & D) | (B & C) | (B & D)\n  // Case 1 is a merge.\n\n  if (compositeFilterIsConjunction(lhs) && compositeFilterIsConjunction(rhs)) {\n    return compositeFilterWithAddedFilters(lhs, rhs.getFilters());\n  } // Case 2,3,4 all have at least one side (lhs or rhs) that is a disjunction. In all three cases\n  // we should take each element of the disjunction and distribute it over the other side, and\n  // return the disjunction of the distribution results.\n\n\n  const disjunctionSide = compositeFilterIsDisjunction(lhs) ? lhs : rhs;\n  const otherSide = compositeFilterIsDisjunction(lhs) ? rhs : lhs;\n  const results = disjunctionSide.filters.map(subfilter => applyDistribution(subfilter, otherSide));\n  return CompositeFilter.create(results, \"or\"\n  /* CompositeOperator.OR */\n  );\n}\n\nfunction applyDistributionFieldAndCompositeFilters(fieldFilter, compositeFilter) {\n  // There are two cases:\n  // A & (B & C) --> (A & B & C)\n  // A & (B | C) --> (A & B) | (A & C)\n  if (compositeFilterIsConjunction(compositeFilter)) {\n    // Case 1\n    return compositeFilterWithAddedFilters(compositeFilter, fieldFilter.getFilters());\n  } else {\n    // Case 2\n    const newFilters = compositeFilter.filters.map(subfilter => applyDistribution(fieldFilter, subfilter));\n    return CompositeFilter.create(newFilters, \"or\"\n    /* CompositeOperator.OR */\n    );\n  }\n}\n/**\r\n * Applies the associativity property to the given filter and returns the resulting filter.\r\n *\r\n * <ul>\r\n *   <li>A | (B | C) == (A | B) | C == (A | B | C)\r\n *   <li>A & (B & C) == (A & B) & C == (A & B & C)\r\n * </ul>\r\n *\r\n * <p>For more info, visit: https://en.wikipedia.org/wiki/Associative_property#Propositional_logic\r\n */\n\n\nfunction applyAssociation(filter) {\n  hardAssert(filter instanceof FieldFilter || filter instanceof CompositeFilter);\n\n  if (filter instanceof FieldFilter) {\n    return filter;\n  }\n\n  const filters = filter.getFilters(); // If the composite filter only contains 1 filter, apply associativity to it.\n\n  if (filters.length === 1) {\n    return applyAssociation(filters[0]);\n  } // Associativity applied to a flat composite filter results is itself.\n\n\n  if (compositeFilterIsFlat(filter)) {\n    return filter;\n  } // First apply associativity to all subfilters. This will in turn recursively apply\n  // associativity to all nested composite filters and field filters.\n\n\n  const updatedFilters = filters.map(subfilter => applyAssociation(subfilter)); // For composite subfilters that perform the same kind of logical operation as `compositeFilter`\n  // take out their filters and add them to `compositeFilter`. For example:\n  // compositeFilter = (A | (B | C | D))\n  // compositeSubfilter = (B | C | D)\n  // Result: (A | B | C | D)\n  // Note that the `compositeSubfilter` has been eliminated, and its filters (B, C, D) have been\n  // added to the top-level \"compositeFilter\".\n\n  const newSubfilters = [];\n  updatedFilters.forEach(subfilter => {\n    if (subfilter instanceof FieldFilter) {\n      newSubfilters.push(subfilter);\n    } else if (subfilter instanceof CompositeFilter) {\n      if (subfilter.op === filter.op) {\n        // compositeFilter: (A | (B | C))\n        // compositeSubfilter: (B | C)\n        // Result: (A | B | C)\n        newSubfilters.push(...subfilter.filters);\n      } else {\n        // compositeFilter: (A | (B & C))\n        // compositeSubfilter: (B & C)\n        // Result: (A | (B & C))\n        newSubfilters.push(subfilter);\n      }\n    }\n  });\n\n  if (newSubfilters.length === 1) {\n    return newSubfilters[0];\n  }\n\n  return CompositeFilter.create(newSubfilters, filter.op);\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An in-memory implementation of IndexManager.\r\n */\n\n\nclass MemoryIndexManager {\n  constructor() {\n    this.collectionParentIndex = new MemoryCollectionParentIndex();\n  }\n\n  addToCollectionParentIndex(transaction, collectionPath) {\n    this.collectionParentIndex.add(collectionPath);\n    return PersistencePromise.resolve();\n  }\n\n  getCollectionParents(transaction, collectionId) {\n    return PersistencePromise.resolve(this.collectionParentIndex.getEntries(collectionId));\n  }\n\n  addFieldIndex(transaction, index) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve();\n  }\n\n  deleteFieldIndex(transaction, index) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve();\n  }\n\n  getDocumentsMatchingTarget(transaction, target) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve(null);\n  }\n\n  getIndexType(transaction, target) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve(0\n    /* IndexType.NONE */\n    );\n  }\n\n  getFieldIndexes(transaction, collectionGroup) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve([]);\n  }\n\n  getNextCollectionGroupToUpdate(transaction) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve(null);\n  }\n\n  getMinOffset(transaction, target) {\n    return PersistencePromise.resolve(IndexOffset.min());\n  }\n\n  getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\n    return PersistencePromise.resolve(IndexOffset.min());\n  }\n\n  updateCollectionGroup(transaction, collectionGroup, offset) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve();\n  }\n\n  updateIndexEntries(transaction, documents) {\n    // Field indices are not supported with memory persistence.\n    return PersistencePromise.resolve();\n  }\n\n}\n/**\r\n * Internal implementation of the collection-parent index exposed by MemoryIndexManager.\r\n * Also used for in-memory caching by IndexedDbIndexManager and initial index population\r\n * in indexeddb_schema.ts\r\n */\n\n\nclass MemoryCollectionParentIndex {\n  constructor() {\n    this.index = {};\n  } // Returns false if the entry already existed.\n\n\n  add(collectionPath) {\n    const collectionId = collectionPath.lastSegment();\n    const parentPath = collectionPath.popLast();\n    const existingParents = this.index[collectionId] || new SortedSet(ResourcePath.comparator);\n    const added = !existingParents.has(parentPath);\n    this.index[collectionId] = existingParents.add(parentPath);\n    return added;\n  }\n\n  has(collectionPath) {\n    const collectionId = collectionPath.lastSegment();\n    const parentPath = collectionPath.popLast();\n    const existingParents = this.index[collectionId];\n    return existingParents && existingParents.has(parentPath);\n  }\n\n  getEntries(collectionId) {\n    const parentPaths = this.index[collectionId] || new SortedSet(ResourcePath.comparator);\n    return parentPaths.toArray();\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$f = 'IndexedDbIndexManager';\nconst EMPTY_VALUE = new Uint8Array(0);\n/**\r\n * A persisted implementation of IndexManager.\r\n *\r\n * PORTING NOTE: Unlike iOS and Android, the Web SDK does not memoize index\r\n * data as it supports multi-tab access.\r\n */\n\nclass IndexedDbIndexManager {\n  constructor(user, databaseId) {\n    this.user = user;\n    this.databaseId = databaseId;\n    /**\r\n     * An in-memory copy of the index entries we've already written since the SDK\r\n     * launched. Used to avoid re-writing the same entry repeatedly.\r\n     *\r\n     * This is *NOT* a complete cache of what's in persistence and so can never be\r\n     * used to satisfy reads.\r\n     */\n\n    this.collectionParentsCache = new MemoryCollectionParentIndex();\n    /**\r\n     * Maps from a target to its equivalent list of sub-targets. Each sub-target\r\n     * contains only one term from the target's disjunctive normal form (DNF).\r\n     */\n\n    this.targetToDnfSubTargets = new ObjectMap(t => canonifyTarget(t), (l, r) => targetEquals(l, r));\n    this.uid = user.uid || '';\n  }\n  /**\r\n   * Adds a new entry to the collection parent index.\r\n   *\r\n   * Repeated calls for the same collectionPath should be avoided within a\r\n   * transaction as IndexedDbIndexManager only caches writes once a transaction\r\n   * has been committed.\r\n   */\n\n\n  addToCollectionParentIndex(transaction, collectionPath) {\n    if (!this.collectionParentsCache.has(collectionPath)) {\n      const collectionId = collectionPath.lastSegment();\n      const parentPath = collectionPath.popLast();\n      transaction.addOnCommittedListener(() => {\n        // Add the collection to the in memory cache only if the transaction was\n        // successfully committed.\n        this.collectionParentsCache.add(collectionPath);\n      });\n      const collectionParent = {\n        collectionId,\n        parent: encodeResourcePath(parentPath)\n      };\n      return collectionParentsStore(transaction).put(collectionParent);\n    }\n\n    return PersistencePromise.resolve();\n  }\n\n  getCollectionParents(transaction, collectionId) {\n    const parentPaths = [];\n    const range = IDBKeyRange.bound([collectionId, ''], [immediateSuccessor(collectionId), ''],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true);\n    return collectionParentsStore(transaction).loadAll(range).next(entries => {\n      for (const entry of entries) {\n        // This collectionId guard shouldn't be necessary (and isn't as long\n        // as we're running in a real browser), but there's a bug in\n        // indexeddbshim that breaks our range in our tests running in node:\n        // https://github.com/axemclion/IndexedDBShim/issues/334\n        if (entry.collectionId !== collectionId) {\n          break;\n        }\n\n        parentPaths.push(decodeResourcePath(entry.parent));\n      }\n\n      return parentPaths;\n    });\n  }\n\n  addFieldIndex(transaction, index) {\n    // TODO(indexing): Verify that the auto-incrementing index ID works in\n    // Safari & Firefox.\n    const indexes = indexConfigurationStore(transaction);\n    const dbIndex = toDbIndexConfiguration(index);\n    delete dbIndex.indexId; // `indexId` is auto-populated by IndexedDb\n\n    const result = indexes.add(dbIndex);\n\n    if (index.indexState) {\n      const states = indexStateStore(transaction);\n      return result.next(indexId => {\n        states.put(toDbIndexState(indexId, this.user, index.indexState.sequenceNumber, index.indexState.offset));\n      });\n    } else {\n      return result.next();\n    }\n  }\n\n  deleteFieldIndex(transaction, index) {\n    const indexes = indexConfigurationStore(transaction);\n    const states = indexStateStore(transaction);\n    const entries = indexEntriesStore(transaction);\n    return indexes.delete(index.indexId).next(() => states.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true))).next(() => entries.delete(IDBKeyRange.bound([index.indexId], [index.indexId + 1],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true)));\n  }\n\n  getDocumentsMatchingTarget(transaction, target) {\n    const indexEntries = indexEntriesStore(transaction);\n    let canServeTarget = true;\n    const indexes = new Map();\n    return PersistencePromise.forEach(this.getSubTargets(target), subTarget => {\n      return this.getFieldIndex(transaction, subTarget).next(index => {\n        canServeTarget && (canServeTarget = !!index);\n        indexes.set(subTarget, index);\n      });\n    }).next(() => {\n      if (!canServeTarget) {\n        return PersistencePromise.resolve(null);\n      } else {\n        let existingKeys = documentKeySet();\n        const result = [];\n        return PersistencePromise.forEach(indexes, (index, subTarget) => {\n          logDebug(LOG_TAG$f, `Using index ${fieldIndexToString(index)} to execute ${canonifyTarget(target)}`);\n          const arrayValues = targetGetArrayValues(subTarget, index);\n          const notInValues = targetGetNotInValues(subTarget, index);\n          const lowerBound = targetGetLowerBound(subTarget, index);\n          const upperBound = targetGetUpperBound(subTarget, index);\n          const lowerBoundEncoded = this.encodeBound(index, subTarget, lowerBound);\n          const upperBoundEncoded = this.encodeBound(index, subTarget, upperBound);\n          const notInEncoded = this.encodeValues(index, subTarget, notInValues);\n          const indexRanges = this.generateIndexRanges(index.indexId, arrayValues, lowerBoundEncoded, lowerBound.inclusive, upperBoundEncoded, upperBound.inclusive, notInEncoded);\n          return PersistencePromise.forEach(indexRanges, indexRange => {\n            return indexEntries.loadFirst(indexRange, target.limit).next(entries => {\n              entries.forEach(entry => {\n                const documentKey = DocumentKey.fromSegments(entry.documentKey);\n\n                if (!existingKeys.has(documentKey)) {\n                  existingKeys = existingKeys.add(documentKey);\n                  result.push(documentKey);\n                }\n              });\n            });\n          });\n        }).next(() => result);\n      }\n    });\n  }\n\n  getSubTargets(target) {\n    let subTargets = this.targetToDnfSubTargets.get(target);\n\n    if (subTargets) {\n      return subTargets;\n    }\n\n    if (target.filters.length === 0) {\n      subTargets = [target];\n    } else {\n      // There is an implicit AND operation between all the filters stored in the target\n      const dnf = getDnfTerms(CompositeFilter.create(target.filters, \"and\"\n      /* CompositeOperator.AND */\n      ));\n      subTargets = dnf.map(term => newTarget(target.path, target.collectionGroup, target.orderBy, term.getFilters(), target.limit, target.startAt, target.endAt));\n    }\n\n    this.targetToDnfSubTargets.set(target, subTargets);\n    return subTargets;\n  }\n  /**\r\n   * Constructs a key range query on `DbIndexEntryStore` that unions all\r\n   * bounds.\r\n   */\n\n\n  generateIndexRanges(indexId, arrayValues, lowerBounds, lowerBoundInclusive, upperBounds, upperBoundInclusive, notInValues) {\n    // The number of total index scans we union together. This is similar to a\n    // distributed normal form, but adapted for array values. We create a single\n    // index range per value in an ARRAY_CONTAINS or ARRAY_CONTAINS_ANY filter\n    // combined with the values from the query bounds.\n    const totalScans = (arrayValues != null ? arrayValues.length : 1) * Math.max(lowerBounds.length, upperBounds.length);\n    const scansPerArrayElement = totalScans / (arrayValues != null ? arrayValues.length : 1);\n    const indexRanges = [];\n\n    for (let i = 0; i < totalScans; ++i) {\n      const arrayValue = arrayValues ? this.encodeSingleElement(arrayValues[i / scansPerArrayElement]) : EMPTY_VALUE;\n      const lowerBound = this.generateLowerBound(indexId, arrayValue, lowerBounds[i % scansPerArrayElement], lowerBoundInclusive);\n      const upperBound = this.generateUpperBound(indexId, arrayValue, upperBounds[i % scansPerArrayElement], upperBoundInclusive);\n      const notInBound = notInValues.map(notIn => this.generateLowerBound(indexId, arrayValue, notIn,\n      /* inclusive= */\n      true));\n      indexRanges.push(...this.createRange(lowerBound, upperBound, notInBound));\n    }\n\n    return indexRanges;\n  }\n  /** Generates the lower bound for `arrayValue` and `directionalValue`. */\n\n\n  generateLowerBound(indexId, arrayValue, directionalValue, inclusive) {\n    const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\n    return inclusive ? entry : entry.successor();\n  }\n  /** Generates the upper bound for `arrayValue` and `directionalValue`. */\n\n\n  generateUpperBound(indexId, arrayValue, directionalValue, inclusive) {\n    const entry = new IndexEntry(indexId, DocumentKey.empty(), arrayValue, directionalValue);\n    return inclusive ? entry.successor() : entry;\n  }\n\n  getFieldIndex(transaction, target) {\n    const targetIndexMatcher = new TargetIndexMatcher(target);\n    const collectionGroup = target.collectionGroup != null ? target.collectionGroup : target.path.lastSegment();\n    return this.getFieldIndexes(transaction, collectionGroup).next(indexes => {\n      // Return the index with the most number of segments.\n      let index = null;\n\n      for (const candidate of indexes) {\n        const matches = targetIndexMatcher.servedByIndex(candidate);\n\n        if (matches && (!index || candidate.fields.length > index.fields.length)) {\n          index = candidate;\n        }\n      }\n\n      return index;\n    });\n  }\n\n  getIndexType(transaction, target) {\n    let indexType = 2\n    /* IndexType.FULL */\n    ;\n    const subTargets = this.getSubTargets(target);\n    return PersistencePromise.forEach(subTargets, target => {\n      return this.getFieldIndex(transaction, target).next(index => {\n        if (!index) {\n          indexType = 0\n          /* IndexType.NONE */\n          ;\n        } else if (indexType !== 0\n        /* IndexType.NONE */\n        && index.fields.length < targetGetSegmentCount(target)) {\n          indexType = 1\n          /* IndexType.PARTIAL */\n          ;\n        }\n      });\n    }).next(() => {\n      // OR queries have more than one sub-target (one sub-target per DNF term). We currently consider\n      // OR queries that have a `limit` to have a partial index. For such queries we perform sorting\n      // and apply the limit in memory as a post-processing step.\n      if (targetHasLimit(target) && subTargets.length > 1 && indexType === 2\n      /* IndexType.FULL */\n      ) {\n        return 1\n        /* IndexType.PARTIAL */\n        ;\n      }\n\n      return indexType;\n    });\n  }\n  /**\r\n   * Returns the byte encoded form of the directional values in the field index.\r\n   * Returns `null` if the document does not have all fields specified in the\r\n   * index.\r\n   */\n\n\n  encodeDirectionalElements(fieldIndex, document) {\n    const encoder = new IndexByteEncoder();\n\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\n      const field = document.data.field(segment.fieldPath);\n\n      if (field == null) {\n        return null;\n      }\n\n      const directionalEncoder = encoder.forKind(segment.kind);\n      FirestoreIndexValueWriter.INSTANCE.writeIndexValue(field, directionalEncoder);\n    }\n\n    return encoder.encodedBytes();\n  }\n  /** Encodes a single value to the ascending index format. */\n\n\n  encodeSingleElement(value) {\n    const encoder = new IndexByteEncoder();\n    FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, encoder.forKind(0\n    /* IndexKind.ASCENDING */\n    ));\n    return encoder.encodedBytes();\n  }\n  /**\r\n   * Returns an encoded form of the document key that sorts based on the key\r\n   * ordering of the field index.\r\n   */\n\n\n  encodeDirectionalKey(fieldIndex, documentKey) {\n    const encoder = new IndexByteEncoder();\n    FirestoreIndexValueWriter.INSTANCE.writeIndexValue(refValue(this.databaseId, documentKey), encoder.forKind(fieldIndexGetKeyOrder(fieldIndex)));\n    return encoder.encodedBytes();\n  }\n  /**\r\n   * Encodes the given field values according to the specification in `target`.\r\n   * For IN queries, a list of possible values is returned.\r\n   */\n\n\n  encodeValues(fieldIndex, target, values) {\n    if (values === null) {\n      return [];\n    }\n\n    let encoders = [];\n    encoders.push(new IndexByteEncoder());\n    let valueIdx = 0;\n\n    for (const segment of fieldIndexGetDirectionalSegments(fieldIndex)) {\n      const value = values[valueIdx++];\n\n      for (const encoder of encoders) {\n        if (this.isInFilter(target, segment.fieldPath) && isArray(value)) {\n          encoders = this.expandIndexValues(encoders, segment, value);\n        } else {\n          const directionalEncoder = encoder.forKind(segment.kind);\n          FirestoreIndexValueWriter.INSTANCE.writeIndexValue(value, directionalEncoder);\n        }\n      }\n    }\n\n    return this.getEncodedBytes(encoders);\n  }\n  /**\r\n   * Encodes the given bounds according to the specification in `target`. For IN\r\n   * queries, a list of possible values is returned.\r\n   */\n\n\n  encodeBound(fieldIndex, target, bound) {\n    return this.encodeValues(fieldIndex, target, bound.position);\n  }\n  /** Returns the byte representation for the provided encoders. */\n\n\n  getEncodedBytes(encoders) {\n    const result = [];\n\n    for (let i = 0; i < encoders.length; ++i) {\n      result[i] = encoders[i].encodedBytes();\n    }\n\n    return result;\n  }\n  /**\r\n   * Creates a separate encoder for each element of an array.\r\n   *\r\n   * The method appends each value to all existing encoders (e.g. filter(\"a\",\r\n   * \"==\", \"a1\").filter(\"b\", \"in\", [\"b1\", \"b2\"]) becomes [\"a1,b1\", \"a1,b2\"]). A\r\n   * list of new encoders is returned.\r\n   */\n\n\n  expandIndexValues(encoders, segment, value) {\n    const prefixes = [...encoders];\n    const results = [];\n\n    for (const arrayElement of value.arrayValue.values || []) {\n      for (const prefix of prefixes) {\n        const clonedEncoder = new IndexByteEncoder();\n        clonedEncoder.seed(prefix.encodedBytes());\n        FirestoreIndexValueWriter.INSTANCE.writeIndexValue(arrayElement, clonedEncoder.forKind(segment.kind));\n        results.push(clonedEncoder);\n      }\n    }\n\n    return results;\n  }\n\n  isInFilter(target, fieldPath) {\n    return !!target.filters.find(f => f instanceof FieldFilter && f.field.isEqual(fieldPath) && (f.op === \"in\"\n    /* Operator.IN */\n    || f.op === \"not-in\"\n    /* Operator.NOT_IN */\n    ));\n  }\n\n  getFieldIndexes(transaction, collectionGroup) {\n    const indexes = indexConfigurationStore(transaction);\n    const states = indexStateStore(transaction);\n    return (collectionGroup ? indexes.loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup)) : indexes.loadAll()).next(indexConfigs => {\n      const result = [];\n      return PersistencePromise.forEach(indexConfigs, indexConfig => {\n        return states.get([indexConfig.indexId, this.uid]).next(indexState => {\n          result.push(fromDbIndexConfiguration(indexConfig, indexState));\n        });\n      }).next(() => result);\n    });\n  }\n\n  getNextCollectionGroupToUpdate(transaction) {\n    return this.getFieldIndexes(transaction).next(indexes => {\n      if (indexes.length === 0) {\n        return null;\n      }\n\n      indexes.sort((l, r) => {\n        const cmp = l.indexState.sequenceNumber - r.indexState.sequenceNumber;\n        return cmp !== 0 ? cmp : primitiveComparator(l.collectionGroup, r.collectionGroup);\n      });\n      return indexes[0].collectionGroup;\n    });\n  }\n\n  updateCollectionGroup(transaction, collectionGroup, offset) {\n    const indexes = indexConfigurationStore(transaction);\n    const states = indexStateStore(transaction);\n    return this.getNextSequenceNumber(transaction).next(nextSequenceNumber => indexes.loadAll(DbIndexConfigurationCollectionGroupIndex, IDBKeyRange.bound(collectionGroup, collectionGroup)).next(configs => PersistencePromise.forEach(configs, config => states.put(toDbIndexState(config.indexId, this.user, nextSequenceNumber, offset)))));\n  }\n\n  updateIndexEntries(transaction, documents) {\n    // Porting Note: `getFieldIndexes()` on Web does not cache index lookups as\n    // it could be used across different IndexedDB transactions. As any cached\n    // data might be invalidated by other multi-tab clients, we can only trust\n    // data within a single IndexedDB transaction. We therefore add a cache\n    // here.\n    const memoizedIndexes = new Map();\n    return PersistencePromise.forEach(documents, (key, doc) => {\n      const memoizedCollectionIndexes = memoizedIndexes.get(key.collectionGroup);\n      const fieldIndexes = memoizedCollectionIndexes ? PersistencePromise.resolve(memoizedCollectionIndexes) : this.getFieldIndexes(transaction, key.collectionGroup);\n      return fieldIndexes.next(fieldIndexes => {\n        memoizedIndexes.set(key.collectionGroup, fieldIndexes);\n        return PersistencePromise.forEach(fieldIndexes, fieldIndex => {\n          return this.getExistingIndexEntries(transaction, key, fieldIndex).next(existingEntries => {\n            const newEntries = this.computeIndexEntries(doc, fieldIndex);\n\n            if (!existingEntries.isEqual(newEntries)) {\n              return this.updateEntries(transaction, doc, fieldIndex, existingEntries, newEntries);\n            }\n\n            return PersistencePromise.resolve();\n          });\n        });\n      });\n    });\n  }\n\n  addIndexEntry(transaction, document, fieldIndex, indexEntry) {\n    const indexEntries = indexEntriesStore(transaction);\n    return indexEntries.put({\n      indexId: indexEntry.indexId,\n      uid: this.uid,\n      arrayValue: indexEntry.arrayValue,\n      directionalValue: indexEntry.directionalValue,\n      orderedDocumentKey: this.encodeDirectionalKey(fieldIndex, document.key),\n      documentKey: document.key.path.toArray()\n    });\n  }\n\n  deleteIndexEntry(transaction, document, fieldIndex, indexEntry) {\n    const indexEntries = indexEntriesStore(transaction);\n    return indexEntries.delete([indexEntry.indexId, this.uid, indexEntry.arrayValue, indexEntry.directionalValue, this.encodeDirectionalKey(fieldIndex, document.key), document.key.path.toArray()]);\n  }\n\n  getExistingIndexEntries(transaction, documentKey, fieldIndex) {\n    const indexEntries = indexEntriesStore(transaction);\n    let results = new SortedSet(indexEntryComparator);\n    return indexEntries.iterate({\n      index: DbIndexEntryDocumentKeyIndex,\n      range: IDBKeyRange.only([fieldIndex.indexId, this.uid, this.encodeDirectionalKey(fieldIndex, documentKey)])\n    }, (_, entry) => {\n      results = results.add(new IndexEntry(fieldIndex.indexId, documentKey, entry.arrayValue, entry.directionalValue));\n    }).next(() => results);\n  }\n  /** Creates the index entries for the given document. */\n\n\n  computeIndexEntries(document, fieldIndex) {\n    let results = new SortedSet(indexEntryComparator);\n    const directionalValue = this.encodeDirectionalElements(fieldIndex, document);\n\n    if (directionalValue == null) {\n      return results;\n    }\n\n    const arraySegment = fieldIndexGetArraySegment(fieldIndex);\n\n    if (arraySegment != null) {\n      const value = document.data.field(arraySegment.fieldPath);\n\n      if (isArray(value)) {\n        for (const arrayValue of value.arrayValue.values || []) {\n          results = results.add(new IndexEntry(fieldIndex.indexId, document.key, this.encodeSingleElement(arrayValue), directionalValue));\n        }\n      }\n    } else {\n      results = results.add(new IndexEntry(fieldIndex.indexId, document.key, EMPTY_VALUE, directionalValue));\n    }\n\n    return results;\n  }\n  /**\r\n   * Updates the index entries for the provided document by deleting entries\r\n   * that are no longer referenced in `newEntries` and adding all newly added\r\n   * entries.\r\n   */\n\n\n  updateEntries(transaction, document, fieldIndex, existingEntries, newEntries) {\n    logDebug(LOG_TAG$f, \"Updating index entries for document '%s'\", document.key);\n    const promises = [];\n    diffSortedSets(existingEntries, newEntries, indexEntryComparator,\n    /* onAdd= */\n    entry => {\n      promises.push(this.addIndexEntry(transaction, document, fieldIndex, entry));\n    },\n    /* onRemove= */\n    entry => {\n      promises.push(this.deleteIndexEntry(transaction, document, fieldIndex, entry));\n    });\n    return PersistencePromise.waitFor(promises);\n  }\n\n  getNextSequenceNumber(transaction) {\n    let nextSequenceNumber = 1;\n    const states = indexStateStore(transaction);\n    return states.iterate({\n      index: DbIndexStateSequenceNumberIndex,\n      reverse: true,\n      range: IDBKeyRange.upperBound([this.uid, Number.MAX_SAFE_INTEGER])\n    }, (_, state, controller) => {\n      controller.done();\n      nextSequenceNumber = state.sequenceNumber + 1;\n    }).next(() => nextSequenceNumber);\n  }\n  /**\r\n   * Returns a new set of IDB ranges that splits the existing range and excludes\r\n   * any values that match the `notInValue` from these ranges. As an example,\r\n   * '[foo > 2 && foo != 3]` becomes  `[foo > 2 && < 3, foo > 3]`.\r\n   */\n\n\n  createRange(lower, upper, notInValues) {\n    // The notIn values need to be sorted and unique so that we can return a\n    // sorted set of non-overlapping ranges.\n    notInValues = notInValues.sort((l, r) => indexEntryComparator(l, r)).filter((el, i, values) => !i || indexEntryComparator(el, values[i - 1]) !== 0);\n    const bounds = [];\n    bounds.push(lower);\n\n    for (const notInValue of notInValues) {\n      const cmpToLower = indexEntryComparator(notInValue, lower);\n      const cmpToUpper = indexEntryComparator(notInValue, upper);\n\n      if (cmpToLower === 0) {\n        // `notInValue` is the lower bound. We therefore need to raise the bound\n        // to the next value.\n        bounds[0] = lower.successor();\n      } else if (cmpToLower > 0 && cmpToUpper < 0) {\n        // `notInValue` is in the middle of the range\n        bounds.push(notInValue);\n        bounds.push(notInValue.successor());\n      } else if (cmpToUpper > 0) {\n        // `notInValue` (and all following values) are out of the range\n        break;\n      }\n    }\n\n    bounds.push(upper);\n    const ranges = [];\n\n    for (let i = 0; i < bounds.length; i += 2) {\n      // If we encounter two bounds that will create an unmatchable key range,\n      // then we return an empty set of key ranges.\n      if (this.isRangeMatchable(bounds[i], bounds[i + 1])) {\n        return [];\n      }\n\n      const lowerBound = [bounds[i].indexId, this.uid, bounds[i].arrayValue, bounds[i].directionalValue, EMPTY_VALUE, []];\n      const upperBound = [bounds[i + 1].indexId, this.uid, bounds[i + 1].arrayValue, bounds[i + 1].directionalValue, EMPTY_VALUE, []];\n      ranges.push(IDBKeyRange.bound(lowerBound, upperBound));\n    }\n\n    return ranges;\n  }\n\n  isRangeMatchable(lowerBound, upperBound) {\n    // If lower bound is greater than the upper bound, then the key\n    // range can never be matched.\n    return indexEntryComparator(lowerBound, upperBound) > 0;\n  }\n\n  getMinOffsetFromCollectionGroup(transaction, collectionGroup) {\n    return this.getFieldIndexes(transaction, collectionGroup).next(getMinOffsetFromFieldIndexes);\n  }\n\n  getMinOffset(transaction, target) {\n    return PersistencePromise.mapArray(this.getSubTargets(target), subTarget => this.getFieldIndex(transaction, subTarget).next(index => index ? index : fail())).next(getMinOffsetFromFieldIndexes);\n  }\n\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the collectionParents\r\n * document store.\r\n */\n\n\nfunction collectionParentsStore(txn) {\n  return getStore(txn, DbCollectionParentStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the index entry object store.\r\n */\n\n\nfunction indexEntriesStore(txn) {\n  return getStore(txn, DbIndexEntryStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the index configuration object store.\r\n */\n\n\nfunction indexConfigurationStore(txn) {\n  return getStore(txn, DbIndexConfigurationStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the index state object store.\r\n */\n\n\nfunction indexStateStore(txn) {\n  return getStore(txn, DbIndexStateStore);\n}\n\nfunction getMinOffsetFromFieldIndexes(fieldIndexes) {\n  hardAssert(fieldIndexes.length !== 0);\n  let minOffset = fieldIndexes[0].indexState.offset;\n  let maxBatchId = minOffset.largestBatchId;\n\n  for (let i = 1; i < fieldIndexes.length; i++) {\n    const newOffset = fieldIndexes[i].indexState.offset;\n\n    if (indexOffsetComparator(newOffset, minOffset) < 0) {\n      minOffset = newOffset;\n    }\n\n    if (maxBatchId < newOffset.largestBatchId) {\n      maxBatchId = newOffset.largestBatchId;\n    }\n  }\n\n  return new IndexOffset(minOffset.readTime, minOffset.documentKey, maxBatchId);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Delete a mutation batch and the associated document mutations.\r\n * @returns A PersistencePromise of the document mutations that were removed.\r\n */\n\n\nfunction removeMutationBatch(txn, userId, batch) {\n  const mutationStore = txn.store(DbMutationBatchStore);\n  const indexTxn = txn.store(DbDocumentMutationStore);\n  const promises = [];\n  const range = IDBKeyRange.only(batch.batchId);\n  let numDeleted = 0;\n  const removePromise = mutationStore.iterate({\n    range\n  }, (key, value, control) => {\n    numDeleted++;\n    return control.delete();\n  });\n  promises.push(removePromise.next(() => {\n    hardAssert(numDeleted === 1);\n  }));\n  const removedDocuments = [];\n\n  for (const mutation of batch.mutations) {\n    const indexKey = newDbDocumentMutationKey(userId, mutation.key.path, batch.batchId);\n    promises.push(indexTxn.delete(indexKey));\n    removedDocuments.push(mutation.key);\n  }\n\n  return PersistencePromise.waitFor(promises).next(() => removedDocuments);\n}\n/**\r\n * Returns an approximate size for the given document.\r\n */\n\n\nfunction dbDocumentSize(doc) {\n  if (!doc) {\n    return 0;\n  }\n\n  let value;\n\n  if (doc.document) {\n    value = doc.document;\n  } else if (doc.unknownDocument) {\n    value = doc.unknownDocument;\n  } else if (doc.noDocument) {\n    value = doc.noDocument;\n  } else {\n    throw fail();\n  }\n\n  return JSON.stringify(value).length;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** A mutation queue for a specific user, backed by IndexedDB. */\n\n\nclass IndexedDbMutationQueue {\n  constructor(\n  /**\r\n   * The normalized userId (e.g. null UID => \"\" userId) used to store /\r\n   * retrieve mutations.\r\n   */\n  userId, serializer, indexManager, referenceDelegate) {\n    this.userId = userId;\n    this.serializer = serializer;\n    this.indexManager = indexManager;\n    this.referenceDelegate = referenceDelegate;\n    /**\r\n     * Caches the document keys for pending mutation batches. If the mutation\r\n     * has been removed from IndexedDb, the cached value may continue to\r\n     * be used to retrieve the batch's document keys. To remove a cached value\r\n     * locally, `removeCachedMutationKeys()` should be invoked either directly\r\n     * or through `removeMutationBatches()`.\r\n     *\r\n     * With multi-tab, when the primary client acknowledges or rejects a mutation,\r\n     * this cache is used by secondary clients to invalidate the local\r\n     * view of the documents that were previously affected by the mutation.\r\n     */\n    // PORTING NOTE: Multi-tab only.\n\n    this.documentKeysByBatchId = {};\n  }\n  /**\r\n   * Creates a new mutation queue for the given user.\r\n   * @param user - The user for which to create a mutation queue.\r\n   * @param serializer - The serializer to use when persisting to IndexedDb.\r\n   */\n\n\n  static forUser(user, serializer, indexManager, referenceDelegate) {\n    // TODO(mcg): Figure out what constraints there are on userIDs\n    // In particular, are there any reserved characters? are empty ids allowed?\n    // For the moment store these together in the same mutations table assuming\n    // that empty userIDs aren't allowed.\n    hardAssert(user.uid !== '');\n    const userId = user.isAuthenticated() ? user.uid : '';\n    return new IndexedDbMutationQueue(userId, serializer, indexManager, referenceDelegate);\n  }\n\n  checkEmpty(transaction) {\n    let empty = true;\n    const range = IDBKeyRange.bound([this.userId, Number.NEGATIVE_INFINITY], [this.userId, Number.POSITIVE_INFINITY]);\n    return mutationsStore(transaction).iterate({\n      index: DbMutationBatchUserMutationsIndex,\n      range\n    }, (key, value, control) => {\n      empty = false;\n      control.done();\n    }).next(() => empty);\n  }\n\n  addMutationBatch(transaction, localWriteTime, baseMutations, mutations) {\n    const documentStore = documentMutationsStore(transaction);\n    const mutationStore = mutationsStore(transaction); // The IndexedDb implementation in Chrome (and Firefox) does not handle\n    // compound indices that include auto-generated keys correctly. To ensure\n    // that the index entry is added correctly in all browsers, we perform two\n    // writes: The first write is used to retrieve the next auto-generated Batch\n    // ID, and the second write populates the index and stores the actual\n    // mutation batch.\n    // See: https://bugs.chromium.org/p/chromium/issues/detail?id=701972\n    // We write an empty object to obtain key\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n    return mutationStore.add({}).next(batchId => {\n      hardAssert(typeof batchId === 'number');\n      const batch = new MutationBatch(batchId, localWriteTime, baseMutations, mutations);\n      const dbBatch = toDbMutationBatch(this.serializer, this.userId, batch);\n      const promises = [];\n      let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\n\n      for (const mutation of mutations) {\n        const indexKey = newDbDocumentMutationKey(this.userId, mutation.key.path, batchId);\n        collectionParents = collectionParents.add(mutation.key.path.popLast());\n        promises.push(mutationStore.put(dbBatch));\n        promises.push(documentStore.put(indexKey, DbDocumentMutationPlaceholder));\n      }\n\n      collectionParents.forEach(parent => {\n        promises.push(this.indexManager.addToCollectionParentIndex(transaction, parent));\n      });\n      transaction.addOnCommittedListener(() => {\n        this.documentKeysByBatchId[batchId] = batch.keys();\n      });\n      return PersistencePromise.waitFor(promises).next(() => batch);\n    });\n  }\n\n  lookupMutationBatch(transaction, batchId) {\n    return mutationsStore(transaction).get(batchId).next(dbBatch => {\n      if (dbBatch) {\n        hardAssert(dbBatch.userId === this.userId);\n        return fromDbMutationBatch(this.serializer, dbBatch);\n      }\n\n      return null;\n    });\n  }\n  /**\r\n   * Returns the document keys for the mutation batch with the given batchId.\r\n   * For primary clients, this method returns `null` after\r\n   * `removeMutationBatches()` has been called. Secondary clients return a\r\n   * cached result until `removeCachedMutationKeys()` is invoked.\r\n   */\n  // PORTING NOTE: Multi-tab only.\n\n\n  lookupMutationKeys(transaction, batchId) {\n    if (this.documentKeysByBatchId[batchId]) {\n      return PersistencePromise.resolve(this.documentKeysByBatchId[batchId]);\n    } else {\n      return this.lookupMutationBatch(transaction, batchId).next(batch => {\n        if (batch) {\n          const keys = batch.keys();\n          this.documentKeysByBatchId[batchId] = keys;\n          return keys;\n        } else {\n          return null;\n        }\n      });\n    }\n  }\n\n  getNextMutationBatchAfterBatchId(transaction, batchId) {\n    const nextBatchId = batchId + 1;\n    const range = IDBKeyRange.lowerBound([this.userId, nextBatchId]);\n    let foundBatch = null;\n    return mutationsStore(transaction).iterate({\n      index: DbMutationBatchUserMutationsIndex,\n      range\n    }, (key, dbBatch, control) => {\n      if (dbBatch.userId === this.userId) {\n        hardAssert(dbBatch.batchId >= nextBatchId);\n        foundBatch = fromDbMutationBatch(this.serializer, dbBatch);\n      }\n\n      control.done();\n    }).next(() => foundBatch);\n  }\n\n  getHighestUnacknowledgedBatchId(transaction) {\n    const range = IDBKeyRange.upperBound([this.userId, Number.POSITIVE_INFINITY]);\n    let batchId = BATCHID_UNKNOWN;\n    return mutationsStore(transaction).iterate({\n      index: DbMutationBatchUserMutationsIndex,\n      range,\n      reverse: true\n    }, (key, dbBatch, control) => {\n      batchId = dbBatch.batchId;\n      control.done();\n    }).next(() => batchId);\n  }\n\n  getAllMutationBatches(transaction) {\n    const range = IDBKeyRange.bound([this.userId, BATCHID_UNKNOWN], [this.userId, Number.POSITIVE_INFINITY]);\n    return mutationsStore(transaction).loadAll(DbMutationBatchUserMutationsIndex, range).next(dbBatches => dbBatches.map(dbBatch => fromDbMutationBatch(this.serializer, dbBatch)));\n  }\n\n  getAllMutationBatchesAffectingDocumentKey(transaction, documentKey) {\n    // Scan the document-mutation index starting with a prefix starting with\n    // the given documentKey.\n    const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\n    const indexStart = IDBKeyRange.lowerBound(indexPrefix);\n    const results = [];\n    return documentMutationsStore(transaction).iterate({\n      range: indexStart\n    }, (indexKey, _, control) => {\n      const [userID, encodedPath, batchId] = indexKey; // Only consider rows matching exactly the specific key of\n      // interest. Note that because we order by path first, and we\n      // order terminators before path separators, we'll encounter all\n      // the index rows for documentKey contiguously. In particular, all\n      // the rows for documentKey will occur before any rows for\n      // documents nested in a subcollection beneath documentKey so we\n      // can stop as soon as we hit any such row.\n\n      const path = decodeResourcePath(encodedPath);\n\n      if (userID !== this.userId || !documentKey.path.isEqual(path)) {\n        control.done();\n        return;\n      } // Look up the mutation batch in the store.\n\n\n      return mutationsStore(transaction).get(batchId).next(mutation => {\n        if (!mutation) {\n          throw fail();\n        }\n\n        hardAssert(mutation.userId === this.userId);\n        results.push(fromDbMutationBatch(this.serializer, mutation));\n      });\n    }).next(() => results);\n  }\n\n  getAllMutationBatchesAffectingDocumentKeys(transaction, documentKeys) {\n    let uniqueBatchIDs = new SortedSet(primitiveComparator);\n    const promises = [];\n    documentKeys.forEach(documentKey => {\n      const indexStart = newDbDocumentMutationPrefixForPath(this.userId, documentKey.path);\n      const range = IDBKeyRange.lowerBound(indexStart);\n      const promise = documentMutationsStore(transaction).iterate({\n        range\n      }, (indexKey, _, control) => {\n        const [userID, encodedPath, batchID] = indexKey; // Only consider rows matching exactly the specific key of\n        // interest. Note that because we order by path first, and we\n        // order terminators before path separators, we'll encounter all\n        // the index rows for documentKey contiguously. In particular, all\n        // the rows for documentKey will occur before any rows for\n        // documents nested in a subcollection beneath documentKey so we\n        // can stop as soon as we hit any such row.\n\n        const path = decodeResourcePath(encodedPath);\n\n        if (userID !== this.userId || !documentKey.path.isEqual(path)) {\n          control.done();\n          return;\n        }\n\n        uniqueBatchIDs = uniqueBatchIDs.add(batchID);\n      });\n      promises.push(promise);\n    });\n    return PersistencePromise.waitFor(promises).next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\n  }\n\n  getAllMutationBatchesAffectingQuery(transaction, query) {\n    const queryPath = query.path;\n    const immediateChildrenLength = queryPath.length + 1; // TODO(mcg): Actually implement a single-collection query\n    //\n    // This is actually executing an ancestor query, traversing the whole\n    // subtree below the collection which can be horrifically inefficient for\n    // some structures. The right way to solve this is to implement the full\n    // value index, but that's not in the cards in the near future so this is\n    // the best we can do for the moment.\n    //\n    // Since we don't yet index the actual properties in the mutations, our\n    // current approach is to just return all mutation batches that affect\n    // documents in the collection being queried.\n\n    const indexPrefix = newDbDocumentMutationPrefixForPath(this.userId, queryPath);\n    const indexStart = IDBKeyRange.lowerBound(indexPrefix); // Collect up unique batchIDs encountered during a scan of the index. Use a\n    // SortedSet to accumulate batch IDs so they can be traversed in order in a\n    // scan of the main table.\n\n    let uniqueBatchIDs = new SortedSet(primitiveComparator);\n    return documentMutationsStore(transaction).iterate({\n      range: indexStart\n    }, (indexKey, _, control) => {\n      const [userID, encodedPath, batchID] = indexKey;\n      const path = decodeResourcePath(encodedPath);\n\n      if (userID !== this.userId || !queryPath.isPrefixOf(path)) {\n        control.done();\n        return;\n      } // Rows with document keys more than one segment longer than the\n      // query path can't be matches. For example, a query on 'rooms'\n      // can't match the document /rooms/abc/messages/xyx.\n      // TODO(mcg): we'll need a different scanner when we implement\n      // ancestor queries.\n\n\n      if (path.length !== immediateChildrenLength) {\n        return;\n      }\n\n      uniqueBatchIDs = uniqueBatchIDs.add(batchID);\n    }).next(() => this.lookupMutationBatches(transaction, uniqueBatchIDs));\n  }\n\n  lookupMutationBatches(transaction, batchIDs) {\n    const results = [];\n    const promises = []; // TODO(rockwood): Implement this using iterate.\n\n    batchIDs.forEach(batchId => {\n      promises.push(mutationsStore(transaction).get(batchId).next(mutation => {\n        if (mutation === null) {\n          throw fail();\n        }\n\n        hardAssert(mutation.userId === this.userId);\n        results.push(fromDbMutationBatch(this.serializer, mutation));\n      }));\n    });\n    return PersistencePromise.waitFor(promises).next(() => results);\n  }\n\n  removeMutationBatch(transaction, batch) {\n    return removeMutationBatch(transaction.simpleDbTransaction, this.userId, batch).next(removedDocuments => {\n      transaction.addOnCommittedListener(() => {\n        this.removeCachedMutationKeys(batch.batchId);\n      });\n      return PersistencePromise.forEach(removedDocuments, key => {\n        return this.referenceDelegate.markPotentiallyOrphaned(transaction, key);\n      });\n    });\n  }\n  /**\r\n   * Clears the cached keys for a mutation batch. This method should be\r\n   * called by secondary clients after they process mutation updates.\r\n   *\r\n   * Note that this method does not have to be called from primary clients as\r\n   * the corresponding cache entries are cleared when an acknowledged or\r\n   * rejected batch is removed from the mutation queue.\r\n   */\n  // PORTING NOTE: Multi-tab only\n\n\n  removeCachedMutationKeys(batchId) {\n    delete this.documentKeysByBatchId[batchId];\n  }\n\n  performConsistencyCheck(txn) {\n    return this.checkEmpty(txn).next(empty => {\n      if (!empty) {\n        return PersistencePromise.resolve();\n      } // Verify that there are no entries in the documentMutations index if\n      // the queue is empty.\n\n\n      const startRange = IDBKeyRange.lowerBound(newDbDocumentMutationPrefixForUser(this.userId));\n      const danglingMutationReferences = [];\n      return documentMutationsStore(txn).iterate({\n        range: startRange\n      }, (key, _, control) => {\n        const userID = key[0];\n\n        if (userID !== this.userId) {\n          control.done();\n          return;\n        } else {\n          const path = decodeResourcePath(key[1]);\n          danglingMutationReferences.push(path);\n        }\n      }).next(() => {\n        hardAssert(danglingMutationReferences.length === 0);\n      });\n    });\n  }\n\n  containsKey(txn, key) {\n    return mutationQueueContainsKey(txn, this.userId, key);\n  } // PORTING NOTE: Multi-tab only (state is held in memory in other clients).\n\n  /** Returns the mutation queue's metadata from IndexedDb. */\n\n\n  getMutationQueueMetadata(transaction) {\n    return mutationQueuesStore(transaction).get(this.userId).next(metadata => {\n      return metadata || {\n        userId: this.userId,\n        lastAcknowledgedBatchId: BATCHID_UNKNOWN,\n        lastStreamToken: ''\n      };\n    });\n  }\n\n}\n/**\r\n * @returns true if the mutation queue for the given user contains a pending\r\n *         mutation for the given key.\r\n */\n\n\nfunction mutationQueueContainsKey(txn, userId, key) {\n  const indexKey = newDbDocumentMutationPrefixForPath(userId, key.path);\n  const encodedPath = indexKey[1];\n  const startRange = IDBKeyRange.lowerBound(indexKey);\n  let containsKey = false;\n  return documentMutationsStore(txn).iterate({\n    range: startRange,\n    keysOnly: true\n  }, (key, value, control) => {\n    const [userID, keyPath,\n    /*batchID*/\n    _] = key;\n\n    if (userID === userId && keyPath === encodedPath) {\n      containsKey = true;\n    }\n\n    control.done();\n  }).next(() => containsKey);\n}\n/** Returns true if any mutation queue contains the given document. */\n\n\nfunction mutationQueuesContainKey(txn, docKey) {\n  let found = false;\n  return mutationQueuesStore(txn).iterateSerial(userId => {\n    return mutationQueueContainsKey(txn, userId, docKey).next(containsKey => {\n      if (containsKey) {\n        found = true;\n      }\n\n      return PersistencePromise.resolve(!containsKey);\n    });\n  }).next(() => found);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the mutations object store.\r\n */\n\n\nfunction mutationsStore(txn) {\n  return getStore(txn, DbMutationBatchStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\n\n\nfunction documentMutationsStore(txn) {\n  return getStore(txn, DbDocumentMutationStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the mutationQueues object store.\r\n */\n\n\nfunction mutationQueuesStore(txn) {\n  return getStore(txn, DbMutationQueueStore);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Offset to ensure non-overlapping target ids. */\n\n\nconst OFFSET = 2;\n/**\r\n * Generates monotonically increasing target IDs for sending targets to the\r\n * watch stream.\r\n *\r\n * The client constructs two generators, one for the target cache, and one for\r\n * for the sync engine (to generate limbo documents targets). These\r\n * generators produce non-overlapping IDs (by using even and odd IDs\r\n * respectively).\r\n *\r\n * By separating the target ID space, the query cache can generate target IDs\r\n * that persist across client restarts, while sync engine can independently\r\n * generate in-memory target IDs that are transient and can be reused after a\r\n * restart.\r\n */\n\nclass TargetIdGenerator {\n  constructor(lastId) {\n    this.lastId = lastId;\n  }\n\n  next() {\n    this.lastId += OFFSET;\n    return this.lastId;\n  }\n\n  static forTargetCache() {\n    // The target cache generator must return '2' in its first call to `next()`\n    // as there is no differentiation in the protocol layer between an unset\n    // number and the number '0'. If we were to sent a target with target ID\n    // '0', the backend would consider it unset and replace it with its own ID.\n    return new TargetIdGenerator(2 - OFFSET);\n  }\n\n  static forSyncEngine() {\n    // Sync engine assigns target IDs for limbo document detection.\n    return new TargetIdGenerator(1 - OFFSET);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass IndexedDbTargetCache {\n  constructor(referenceDelegate, serializer) {\n    this.referenceDelegate = referenceDelegate;\n    this.serializer = serializer;\n  } // PORTING NOTE: We don't cache global metadata for the target cache, since\n  // some of it (in particular `highestTargetId`) can be modified by secondary\n  // tabs. We could perhaps be more granular (and e.g. still cache\n  // `lastRemoteSnapshotVersion` in memory) but for simplicity we currently go\n  // to IndexedDb whenever we need to read metadata. We can revisit if it turns\n  // out to have a meaningful performance impact.\n\n\n  allocateTargetId(transaction) {\n    return this.retrieveMetadata(transaction).next(metadata => {\n      const targetIdGenerator = new TargetIdGenerator(metadata.highestTargetId);\n      metadata.highestTargetId = targetIdGenerator.next();\n      return this.saveMetadata(transaction, metadata).next(() => metadata.highestTargetId);\n    });\n  }\n\n  getLastRemoteSnapshotVersion(transaction) {\n    return this.retrieveMetadata(transaction).next(metadata => {\n      return SnapshotVersion.fromTimestamp(new Timestamp(metadata.lastRemoteSnapshotVersion.seconds, metadata.lastRemoteSnapshotVersion.nanoseconds));\n    });\n  }\n\n  getHighestSequenceNumber(transaction) {\n    return this.retrieveMetadata(transaction).next(targetGlobal => targetGlobal.highestListenSequenceNumber);\n  }\n\n  setTargetsMetadata(transaction, highestListenSequenceNumber, lastRemoteSnapshotVersion) {\n    return this.retrieveMetadata(transaction).next(metadata => {\n      metadata.highestListenSequenceNumber = highestListenSequenceNumber;\n\n      if (lastRemoteSnapshotVersion) {\n        metadata.lastRemoteSnapshotVersion = lastRemoteSnapshotVersion.toTimestamp();\n      }\n\n      if (highestListenSequenceNumber > metadata.highestListenSequenceNumber) {\n        metadata.highestListenSequenceNumber = highestListenSequenceNumber;\n      }\n\n      return this.saveMetadata(transaction, metadata);\n    });\n  }\n\n  addTargetData(transaction, targetData) {\n    return this.saveTargetData(transaction, targetData).next(() => {\n      return this.retrieveMetadata(transaction).next(metadata => {\n        metadata.targetCount += 1;\n        this.updateMetadataFromTargetData(targetData, metadata);\n        return this.saveMetadata(transaction, metadata);\n      });\n    });\n  }\n\n  updateTargetData(transaction, targetData) {\n    return this.saveTargetData(transaction, targetData);\n  }\n\n  removeTargetData(transaction, targetData) {\n    return this.removeMatchingKeysForTargetId(transaction, targetData.targetId).next(() => targetsStore(transaction).delete(targetData.targetId)).next(() => this.retrieveMetadata(transaction)).next(metadata => {\n      hardAssert(metadata.targetCount > 0);\n      metadata.targetCount -= 1;\n      return this.saveMetadata(transaction, metadata);\n    });\n  }\n  /**\r\n   * Drops any targets with sequence number less than or equal to the upper bound, excepting those\r\n   * present in `activeTargetIds`. Document associations for the removed targets are also removed.\r\n   * Returns the number of targets removed.\r\n   */\n\n\n  removeTargets(txn, upperBound, activeTargetIds) {\n    let count = 0;\n    const promises = [];\n    return targetsStore(txn).iterate((key, value) => {\n      const targetData = fromDbTarget(value);\n\n      if (targetData.sequenceNumber <= upperBound && activeTargetIds.get(targetData.targetId) === null) {\n        count++;\n        promises.push(this.removeTargetData(txn, targetData));\n      }\n    }).next(() => PersistencePromise.waitFor(promises)).next(() => count);\n  }\n  /**\r\n   * Call provided function with each `TargetData` that we have cached.\r\n   */\n\n\n  forEachTarget(txn, f) {\n    return targetsStore(txn).iterate((key, value) => {\n      const targetData = fromDbTarget(value);\n      f(targetData);\n    });\n  }\n\n  retrieveMetadata(transaction) {\n    return globalTargetStore(transaction).get(DbTargetGlobalKey).next(metadata => {\n      hardAssert(metadata !== null);\n      return metadata;\n    });\n  }\n\n  saveMetadata(transaction, metadata) {\n    return globalTargetStore(transaction).put(DbTargetGlobalKey, metadata);\n  }\n\n  saveTargetData(transaction, targetData) {\n    return targetsStore(transaction).put(toDbTarget(this.serializer, targetData));\n  }\n  /**\r\n   * In-place updates the provided metadata to account for values in the given\r\n   * TargetData. Saving is done separately. Returns true if there were any\r\n   * changes to the metadata.\r\n   */\n\n\n  updateMetadataFromTargetData(targetData, metadata) {\n    let updated = false;\n\n    if (targetData.targetId > metadata.highestTargetId) {\n      metadata.highestTargetId = targetData.targetId;\n      updated = true;\n    }\n\n    if (targetData.sequenceNumber > metadata.highestListenSequenceNumber) {\n      metadata.highestListenSequenceNumber = targetData.sequenceNumber;\n      updated = true;\n    }\n\n    return updated;\n  }\n\n  getTargetCount(transaction) {\n    return this.retrieveMetadata(transaction).next(metadata => metadata.targetCount);\n  }\n\n  getTargetData(transaction, target) {\n    // Iterating by the canonicalId may yield more than one result because\n    // canonicalId values are not required to be unique per target. This query\n    // depends on the queryTargets index to be efficient.\n    const canonicalId = canonifyTarget(target);\n    const range = IDBKeyRange.bound([canonicalId, Number.NEGATIVE_INFINITY], [canonicalId, Number.POSITIVE_INFINITY]);\n    let result = null;\n    return targetsStore(transaction).iterate({\n      range,\n      index: DbTargetQueryTargetsIndexName\n    }, (key, value, control) => {\n      const found = fromDbTarget(value); // After finding a potential match, check that the target is\n      // actually equal to the requested target.\n\n      if (targetEquals(target, found.target)) {\n        result = found;\n        control.done();\n      }\n    }).next(() => result);\n  }\n\n  addMatchingKeys(txn, keys, targetId) {\n    // PORTING NOTE: The reverse index (documentsTargets) is maintained by\n    // IndexedDb.\n    const promises = [];\n    const store = documentTargetStore(txn);\n    keys.forEach(key => {\n      const path = encodeResourcePath(key.path);\n      promises.push(store.put({\n        targetId,\n        path\n      }));\n      promises.push(this.referenceDelegate.addReference(txn, targetId, key));\n    });\n    return PersistencePromise.waitFor(promises);\n  }\n\n  removeMatchingKeys(txn, keys, targetId) {\n    // PORTING NOTE: The reverse index (documentsTargets) is maintained by\n    // IndexedDb.\n    const store = documentTargetStore(txn);\n    return PersistencePromise.forEach(keys, key => {\n      const path = encodeResourcePath(key.path);\n      return PersistencePromise.waitFor([store.delete([targetId, path]), this.referenceDelegate.removeReference(txn, targetId, key)]);\n    });\n  }\n\n  removeMatchingKeysForTargetId(txn, targetId) {\n    const store = documentTargetStore(txn);\n    const range = IDBKeyRange.bound([targetId], [targetId + 1],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true);\n    return store.delete(range);\n  }\n\n  getMatchingKeysForTargetId(txn, targetId) {\n    const range = IDBKeyRange.bound([targetId], [targetId + 1],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true);\n    const store = documentTargetStore(txn);\n    let result = documentKeySet();\n    return store.iterate({\n      range,\n      keysOnly: true\n    }, (key, _, control) => {\n      const path = decodeResourcePath(key[1]);\n      const docKey = new DocumentKey(path);\n      result = result.add(docKey);\n    }).next(() => result);\n  }\n\n  containsKey(txn, key) {\n    const path = encodeResourcePath(key.path);\n    const range = IDBKeyRange.bound([path], [immediateSuccessor(path)],\n    /*lowerOpen=*/\n    false,\n    /*upperOpen=*/\n    true);\n    let count = 0;\n    return documentTargetStore(txn).iterate({\n      index: DbTargetDocumentDocumentTargetsIndex,\n      keysOnly: true,\n      range\n    }, ([targetId, path], _, control) => {\n      // Having a sentinel row for a document does not count as containing that document;\n      // For the target cache, containing the document means the document is part of some\n      // target.\n      if (targetId !== 0) {\n        count++;\n        control.done();\n      }\n    }).next(() => count > 0);\n  }\n  /**\r\n   * Looks up a TargetData entry by target ID.\r\n   *\r\n   * @param targetId - The target ID of the TargetData entry to look up.\r\n   * @returns The cached TargetData entry, or null if the cache has no entry for\r\n   * the target.\r\n   */\n  // PORTING NOTE: Multi-tab only.\n\n\n  getTargetDataForTarget(transaction, targetId) {\n    return targetsStore(transaction).get(targetId).next(found => {\n      if (found) {\n        return fromDbTarget(found);\n      } else {\n        return null;\n      }\n    });\n  }\n\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the queries object store.\r\n */\n\n\nfunction targetsStore(txn) {\n  return getStore(txn, DbTargetStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the target globals object store.\r\n */\n\n\nfunction globalTargetStore(txn) {\n  return getStore(txn, DbTargetGlobalStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the document target object store.\r\n */\n\n\nfunction documentTargetStore(txn) {\n  return getStore(txn, DbTargetDocumentStore);\n}\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst GC_DID_NOT_RUN = {\n  didRun: false,\n  sequenceNumbersCollected: 0,\n  targetsRemoved: 0,\n  documentsRemoved: 0\n};\nconst LRU_COLLECTION_DISABLED = -1;\nconst LRU_DEFAULT_CACHE_SIZE_BYTES = 40 * 1024 * 1024;\n\nclass LruParams {\n  constructor( // When we attempt to collect, we will only do so if the cache size is greater than this\n  // threshold. Passing `COLLECTION_DISABLED` here will cause collection to always be skipped.\n  cacheSizeCollectionThreshold, // The percentage of sequence numbers that we will attempt to collect\n  percentileToCollect, // A cap on the total number of sequence numbers that will be collected. This prevents\n  // us from collecting a huge number of sequence numbers if the cache has grown very large.\n  maximumSequenceNumbersToCollect) {\n    this.cacheSizeCollectionThreshold = cacheSizeCollectionThreshold;\n    this.percentileToCollect = percentileToCollect;\n    this.maximumSequenceNumbersToCollect = maximumSequenceNumbersToCollect;\n  }\n\n  static withCacheSize(cacheSize) {\n    return new LruParams(cacheSize, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\n  }\n\n}\n\nLruParams.DEFAULT_COLLECTION_PERCENTILE = 10;\nLruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT = 1000;\nLruParams.DEFAULT = new LruParams(LRU_DEFAULT_CACHE_SIZE_BYTES, LruParams.DEFAULT_COLLECTION_PERCENTILE, LruParams.DEFAULT_MAX_SEQUENCE_NUMBERS_TO_COLLECT);\nLruParams.DISABLED = new LruParams(LRU_COLLECTION_DISABLED, 0, 0);\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\nconst LOG_TAG$e = 'LruGarbageCollector';\nconst LRU_MINIMUM_CACHE_SIZE_BYTES = 1 * 1024 * 1024;\n/** How long we wait to try running LRU GC after SDK initialization. */\n\nconst INITIAL_GC_DELAY_MS = 1 * 60 * 1000;\n/** Minimum amount of time between GC checks, after the first one. */\n\nconst REGULAR_GC_DELAY_MS = 5 * 60 * 1000;\n\nfunction bufferEntryComparator([aSequence, aIndex], [bSequence, bIndex]) {\n  const seqCmp = primitiveComparator(aSequence, bSequence);\n\n  if (seqCmp === 0) {\n    // This order doesn't matter, but we can bias against churn by sorting\n    // entries created earlier as less than newer entries.\n    return primitiveComparator(aIndex, bIndex);\n  } else {\n    return seqCmp;\n  }\n}\n/**\r\n * Used to calculate the nth sequence number. Keeps a rolling buffer of the\r\n * lowest n values passed to `addElement`, and finally reports the largest of\r\n * them in `maxValue`.\r\n */\n\n\nclass RollingSequenceNumberBuffer {\n  constructor(maxElements) {\n    this.maxElements = maxElements;\n    this.buffer = new SortedSet(bufferEntryComparator);\n    this.previousIndex = 0;\n  }\n\n  nextIndex() {\n    return ++this.previousIndex;\n  }\n\n  addElement(sequenceNumber) {\n    const entry = [sequenceNumber, this.nextIndex()];\n\n    if (this.buffer.size < this.maxElements) {\n      this.buffer = this.buffer.add(entry);\n    } else {\n      const highestValue = this.buffer.last();\n\n      if (bufferEntryComparator(entry, highestValue) < 0) {\n        this.buffer = this.buffer.delete(highestValue).add(entry);\n      }\n    }\n  }\n\n  get maxValue() {\n    // Guaranteed to be non-empty. If we decide we are not collecting any\n    // sequence numbers, nthSequenceNumber below short-circuits. If we have\n    // decided that we are collecting n sequence numbers, it's because n is some\n    // percentage of the existing sequence numbers. That means we should never\n    // be in a situation where we are collecting sequence numbers but don't\n    // actually have any.\n    return this.buffer.last()[0];\n  }\n\n}\n/**\r\n * This class is responsible for the scheduling of LRU garbage collection. It handles checking\r\n * whether or not GC is enabled, as well as which delay to use before the next run.\r\n */\n\n\nclass LruScheduler {\n  constructor(garbageCollector, asyncQueue, localStore) {\n    this.garbageCollector = garbageCollector;\n    this.asyncQueue = asyncQueue;\n    this.localStore = localStore;\n    this.gcTask = null;\n  }\n\n  start() {\n    if (this.garbageCollector.params.cacheSizeCollectionThreshold !== LRU_COLLECTION_DISABLED) {\n      this.scheduleGC(INITIAL_GC_DELAY_MS);\n    }\n  }\n\n  stop() {\n    if (this.gcTask) {\n      this.gcTask.cancel();\n      this.gcTask = null;\n    }\n  }\n\n  get started() {\n    return this.gcTask !== null;\n  }\n\n  scheduleGC(delay) {\n    var _this6 = this;\n\n    logDebug(LOG_TAG$e, `Garbage collection scheduled in ${delay}ms`);\n    this.gcTask = this.asyncQueue.enqueueAfterDelay(\"lru_garbage_collection\"\n    /* TimerId.LruGarbageCollection */\n    , delay, /*#__PURE__*/_asyncToGenerator(function* () {\n      _this6.gcTask = null;\n\n      try {\n        yield _this6.localStore.collectGarbage(_this6.garbageCollector);\n      } catch (e) {\n        if (isIndexedDbTransactionError(e)) {\n          logDebug(LOG_TAG$e, 'Ignoring IndexedDB error during garbage collection: ', e);\n        } else {\n          yield ignoreIfPrimaryLeaseLoss(e);\n        }\n      }\n\n      yield _this6.scheduleGC(REGULAR_GC_DELAY_MS);\n    }));\n  }\n\n}\n/**\r\n * Implements the steps for LRU garbage collection.\r\n */\n\n\nclass LruGarbageCollectorImpl {\n  constructor(delegate, params) {\n    this.delegate = delegate;\n    this.params = params;\n  }\n\n  calculateTargetCount(txn, percentile) {\n    return this.delegate.getSequenceNumberCount(txn).next(targetCount => {\n      return Math.floor(percentile / 100.0 * targetCount);\n    });\n  }\n\n  nthSequenceNumber(txn, n) {\n    if (n === 0) {\n      return PersistencePromise.resolve(ListenSequence.INVALID);\n    }\n\n    const buffer = new RollingSequenceNumberBuffer(n);\n    return this.delegate.forEachTarget(txn, target => buffer.addElement(target.sequenceNumber)).next(() => {\n      return this.delegate.forEachOrphanedDocumentSequenceNumber(txn, sequenceNumber => buffer.addElement(sequenceNumber));\n    }).next(() => buffer.maxValue);\n  }\n\n  removeTargets(txn, upperBound, activeTargetIds) {\n    return this.delegate.removeTargets(txn, upperBound, activeTargetIds);\n  }\n\n  removeOrphanedDocuments(txn, upperBound) {\n    return this.delegate.removeOrphanedDocuments(txn, upperBound);\n  }\n\n  collect(txn, activeTargetIds) {\n    if (this.params.cacheSizeCollectionThreshold === LRU_COLLECTION_DISABLED) {\n      logDebug('LruGarbageCollector', 'Garbage collection skipped; disabled');\n      return PersistencePromise.resolve(GC_DID_NOT_RUN);\n    }\n\n    return this.getCacheSize(txn).next(cacheSize => {\n      if (cacheSize < this.params.cacheSizeCollectionThreshold) {\n        logDebug('LruGarbageCollector', `Garbage collection skipped; Cache size ${cacheSize} ` + `is lower than threshold ${this.params.cacheSizeCollectionThreshold}`);\n        return GC_DID_NOT_RUN;\n      } else {\n        return this.runGarbageCollection(txn, activeTargetIds);\n      }\n    });\n  }\n\n  getCacheSize(txn) {\n    return this.delegate.getCacheSize(txn);\n  }\n\n  runGarbageCollection(txn, activeTargetIds) {\n    let upperBoundSequenceNumber;\n    let sequenceNumbersToCollect, targetsRemoved; // Timestamps for various pieces of the process\n\n    let countedTargetsTs, foundUpperBoundTs, removedTargetsTs, removedDocumentsTs;\n    const startTs = Date.now();\n    return this.calculateTargetCount(txn, this.params.percentileToCollect).next(sequenceNumbers => {\n      // Cap at the configured max\n      if (sequenceNumbers > this.params.maximumSequenceNumbersToCollect) {\n        logDebug('LruGarbageCollector', 'Capping sequence numbers to collect down ' + `to the maximum of ${this.params.maximumSequenceNumbersToCollect} ` + `from ${sequenceNumbers}`);\n        sequenceNumbersToCollect = this.params.maximumSequenceNumbersToCollect;\n      } else {\n        sequenceNumbersToCollect = sequenceNumbers;\n      }\n\n      countedTargetsTs = Date.now();\n      return this.nthSequenceNumber(txn, sequenceNumbersToCollect);\n    }).next(upperBound => {\n      upperBoundSequenceNumber = upperBound;\n      foundUpperBoundTs = Date.now();\n      return this.removeTargets(txn, upperBoundSequenceNumber, activeTargetIds);\n    }).next(numTargetsRemoved => {\n      targetsRemoved = numTargetsRemoved;\n      removedTargetsTs = Date.now();\n      return this.removeOrphanedDocuments(txn, upperBoundSequenceNumber);\n    }).next(documentsRemoved => {\n      removedDocumentsTs = Date.now();\n\n      if (getLogLevel() <= LogLevel.DEBUG) {\n        const desc = 'LRU Garbage Collection\\n' + `\\tCounted targets in ${countedTargetsTs - startTs}ms\\n` + `\\tDetermined least recently used ${sequenceNumbersToCollect} in ` + `${foundUpperBoundTs - countedTargetsTs}ms\\n` + `\\tRemoved ${targetsRemoved} targets in ` + `${removedTargetsTs - foundUpperBoundTs}ms\\n` + `\\tRemoved ${documentsRemoved} documents in ` + `${removedDocumentsTs - removedTargetsTs}ms\\n` + `Total Duration: ${removedDocumentsTs - startTs}ms`;\n        logDebug('LruGarbageCollector', desc);\n      }\n\n      return PersistencePromise.resolve({\n        didRun: true,\n        sequenceNumbersCollected: sequenceNumbersToCollect,\n        targetsRemoved,\n        documentsRemoved\n      });\n    });\n  }\n\n}\n\nfunction newLruGarbageCollector(delegate, params) {\n  return new LruGarbageCollectorImpl(delegate, params);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Provides LRU functionality for IndexedDB persistence. */\n\n\nclass IndexedDbLruDelegateImpl {\n  constructor(db, params) {\n    this.db = db;\n    this.garbageCollector = newLruGarbageCollector(this, params);\n  }\n\n  getSequenceNumberCount(txn) {\n    const docCountPromise = this.orphanedDocumentCount(txn);\n    const targetCountPromise = this.db.getTargetCache().getTargetCount(txn);\n    return targetCountPromise.next(targetCount => docCountPromise.next(docCount => targetCount + docCount));\n  }\n\n  orphanedDocumentCount(txn) {\n    let orphanedCount = 0;\n    return this.forEachOrphanedDocumentSequenceNumber(txn, _ => {\n      orphanedCount++;\n    }).next(() => orphanedCount);\n  }\n\n  forEachTarget(txn, f) {\n    return this.db.getTargetCache().forEachTarget(txn, f);\n  }\n\n  forEachOrphanedDocumentSequenceNumber(txn, f) {\n    return this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => f(sequenceNumber));\n  }\n\n  addReference(txn, targetId, key) {\n    return writeSentinelKey(txn, key);\n  }\n\n  removeReference(txn, targetId, key) {\n    return writeSentinelKey(txn, key);\n  }\n\n  removeTargets(txn, upperBound, activeTargetIds) {\n    return this.db.getTargetCache().removeTargets(txn, upperBound, activeTargetIds);\n  }\n\n  markPotentiallyOrphaned(txn, key) {\n    return writeSentinelKey(txn, key);\n  }\n  /**\r\n   * Returns true if anything would prevent this document from being garbage\r\n   * collected, given that the document in question is not present in any\r\n   * targets and has a sequence number less than or equal to the upper bound for\r\n   * the collection run.\r\n   */\n\n\n  isPinned(txn, docKey) {\n    return mutationQueuesContainKey(txn, docKey);\n  }\n\n  removeOrphanedDocuments(txn, upperBound) {\n    const documentCache = this.db.getRemoteDocumentCache();\n    const changeBuffer = documentCache.newChangeBuffer();\n    const promises = [];\n    let documentCount = 0;\n    const iteration = this.forEachOrphanedDocument(txn, (docKey, sequenceNumber) => {\n      if (sequenceNumber <= upperBound) {\n        const p = this.isPinned(txn, docKey).next(isPinned => {\n          if (!isPinned) {\n            documentCount++; // Our size accounting requires us to read all documents before\n            // removing them.\n\n            return changeBuffer.getEntry(txn, docKey).next(() => {\n              changeBuffer.removeEntry(docKey, SnapshotVersion.min());\n              return documentTargetStore(txn).delete(sentinelKey$1(docKey));\n            });\n          }\n        });\n        promises.push(p);\n      }\n    });\n    return iteration.next(() => PersistencePromise.waitFor(promises)).next(() => changeBuffer.apply(txn)).next(() => documentCount);\n  }\n\n  removeTarget(txn, targetData) {\n    const updated = targetData.withSequenceNumber(txn.currentSequenceNumber);\n    return this.db.getTargetCache().updateTargetData(txn, updated);\n  }\n\n  updateLimboDocument(txn, key) {\n    return writeSentinelKey(txn, key);\n  }\n  /**\r\n   * Call provided function for each document in the cache that is 'orphaned'. Orphaned\r\n   * means not a part of any target, so the only entry in the target-document index for\r\n   * that document will be the sentinel row (targetId 0), which will also have the sequence\r\n   * number for the last time the document was accessed.\r\n   */\n\n\n  forEachOrphanedDocument(txn, f) {\n    const store = documentTargetStore(txn);\n    let nextToReport = ListenSequence.INVALID;\n    let nextPath;\n    return store.iterate({\n      index: DbTargetDocumentDocumentTargetsIndex\n    }, ([targetId, docKey], {\n      path,\n      sequenceNumber\n    }) => {\n      if (targetId === 0) {\n        // if nextToReport is valid, report it, this is a new key so the\n        // last one must not be a member of any targets.\n        if (nextToReport !== ListenSequence.INVALID) {\n          f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\n        } // set nextToReport to be this sequence number. It's the next one we\n        // might report, if we don't find any targets for this document.\n        // Note that the sequence number must be defined when the targetId\n        // is 0.\n\n\n        nextToReport = sequenceNumber;\n        nextPath = path;\n      } else {\n        // set nextToReport to be invalid, we know we don't need to report\n        // this one since we found a target for it.\n        nextToReport = ListenSequence.INVALID;\n      }\n    }).next(() => {\n      // Since we report sequence numbers after getting to the next key, we\n      // need to check if the last key we iterated over was an orphaned\n      // document and report it.\n      if (nextToReport !== ListenSequence.INVALID) {\n        f(new DocumentKey(decodeResourcePath(nextPath)), nextToReport);\n      }\n    });\n  }\n\n  getCacheSize(txn) {\n    return this.db.getRemoteDocumentCache().getSize(txn);\n  }\n\n}\n\nfunction sentinelKey$1(key) {\n  return [0, encodeResourcePath(key.path)];\n}\n/**\r\n * @returns A value suitable for writing a sentinel row in the target-document\r\n * store.\r\n */\n\n\nfunction sentinelRow(key, sequenceNumber) {\n  return {\n    targetId: 0,\n    path: encodeResourcePath(key.path),\n    sequenceNumber\n  };\n}\n\nfunction writeSentinelKey(txn, key) {\n  return documentTargetStore(txn).put(sentinelRow(key, txn.currentSequenceNumber));\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An in-memory buffer of entries to be written to a RemoteDocumentCache.\r\n * It can be used to batch up a set of changes to be written to the cache, but\r\n * additionally supports reading entries back with the `getEntry()` method,\r\n * falling back to the underlying RemoteDocumentCache if no entry is\r\n * buffered.\r\n *\r\n * Entries added to the cache *must* be read first. This is to facilitate\r\n * calculating the size delta of the pending changes.\r\n *\r\n * PORTING NOTE: This class was implemented then removed from other platforms.\r\n * If byte-counting ends up being needed on the other platforms, consider\r\n * porting this class as part of that implementation work.\r\n */\n\n\nclass RemoteDocumentChangeBuffer {\n  constructor() {\n    // A mapping of document key to the new cache entry that should be written.\n    this.changes = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\n    this.changesApplied = false;\n  }\n  /**\r\n   * Buffers a `RemoteDocumentCache.addEntry()` call.\r\n   *\r\n   * You can only modify documents that have already been retrieved via\r\n   * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n   */\n\n\n  addEntry(document) {\n    this.assertNotApplied();\n    this.changes.set(document.key, document);\n  }\n  /**\r\n   * Buffers a `RemoteDocumentCache.removeEntry()` call.\r\n   *\r\n   * You can only remove documents that have already been retrieved via\r\n   * `getEntry()/getEntries()` (enforced via IndexedDbs `apply()`).\r\n   */\n\n\n  removeEntry(key, readTime) {\n    this.assertNotApplied();\n    this.changes.set(key, MutableDocument.newInvalidDocument(key).setReadTime(readTime));\n  }\n  /**\r\n   * Looks up an entry in the cache. The buffered changes will first be checked,\r\n   * and if no buffered change applies, this will forward to\r\n   * `RemoteDocumentCache.getEntry()`.\r\n   *\r\n   * @param transaction - The transaction in which to perform any persistence\r\n   *     operations.\r\n   * @param documentKey - The key of the entry to look up.\r\n   * @returns The cached document or an invalid document if we have nothing\r\n   * cached.\r\n   */\n\n\n  getEntry(transaction, documentKey) {\n    this.assertNotApplied();\n    const bufferedEntry = this.changes.get(documentKey);\n\n    if (bufferedEntry !== undefined) {\n      return PersistencePromise.resolve(bufferedEntry);\n    } else {\n      return this.getFromCache(transaction, documentKey);\n    }\n  }\n  /**\r\n   * Looks up several entries in the cache, forwarding to\r\n   * `RemoteDocumentCache.getEntry()`.\r\n   *\r\n   * @param transaction - The transaction in which to perform any persistence\r\n   *     operations.\r\n   * @param documentKeys - The keys of the entries to look up.\r\n   * @returns A map of cached documents, indexed by key. If an entry cannot be\r\n   *     found, the corresponding key will be mapped to an invalid document.\r\n   */\n\n\n  getEntries(transaction, documentKeys) {\n    return this.getAllFromCache(transaction, documentKeys);\n  }\n  /**\r\n   * Applies buffered changes to the underlying RemoteDocumentCache, using\r\n   * the provided transaction.\r\n   */\n\n\n  apply(transaction) {\n    this.assertNotApplied();\n    this.changesApplied = true;\n    return this.applyChanges(transaction);\n  }\n  /** Helper to assert this.changes is not null  */\n\n\n  assertNotApplied() {}\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * The RemoteDocumentCache for IndexedDb. To construct, invoke\r\n * `newIndexedDbRemoteDocumentCache()`.\r\n */\n\n\nclass IndexedDbRemoteDocumentCacheImpl {\n  constructor(serializer) {\n    this.serializer = serializer;\n  }\n\n  setIndexManager(indexManager) {\n    this.indexManager = indexManager;\n  }\n  /**\r\n   * Adds the supplied entries to the cache.\r\n   *\r\n   * All calls of `addEntry` are required to go through the RemoteDocumentChangeBuffer\r\n   * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n   */\n\n\n  addEntry(transaction, key, doc) {\n    const documentStore = remoteDocumentsStore(transaction);\n    return documentStore.put(doc);\n  }\n  /**\r\n   * Removes a document from the cache.\r\n   *\r\n   * All calls of `removeEntry`  are required to go through the RemoteDocumentChangeBuffer\r\n   * returned by `newChangeBuffer()` to ensure proper accounting of metadata.\r\n   */\n\n\n  removeEntry(transaction, documentKey, readTime) {\n    const store = remoteDocumentsStore(transaction);\n    return store.delete(dbReadTimeKey(documentKey, readTime));\n  }\n  /**\r\n   * Updates the current cache size.\r\n   *\r\n   * Callers to `addEntry()` and `removeEntry()` *must* call this afterwards to update the\r\n   * cache's metadata.\r\n   */\n\n\n  updateMetadata(transaction, sizeDelta) {\n    return this.getMetadata(transaction).next(metadata => {\n      metadata.byteSize += sizeDelta;\n      return this.setMetadata(transaction, metadata);\n    });\n  }\n\n  getEntry(transaction, documentKey) {\n    let doc = MutableDocument.newInvalidDocument(documentKey);\n    return remoteDocumentsStore(transaction).iterate({\n      index: DbRemoteDocumentDocumentKeyIndex,\n      range: IDBKeyRange.only(dbKey(documentKey))\n    }, (_, dbRemoteDoc) => {\n      doc = this.maybeDecodeDocument(documentKey, dbRemoteDoc);\n    }).next(() => doc);\n  }\n  /**\r\n   * Looks up an entry in the cache.\r\n   *\r\n   * @param documentKey - The key of the entry to look up.\r\n   * @returns The cached document entry and its size.\r\n   */\n\n\n  getSizedEntry(transaction, documentKey) {\n    let result = {\n      size: 0,\n      document: MutableDocument.newInvalidDocument(documentKey)\n    };\n    return remoteDocumentsStore(transaction).iterate({\n      index: DbRemoteDocumentDocumentKeyIndex,\n      range: IDBKeyRange.only(dbKey(documentKey))\n    }, (_, dbRemoteDoc) => {\n      result = {\n        document: this.maybeDecodeDocument(documentKey, dbRemoteDoc),\n        size: dbDocumentSize(dbRemoteDoc)\n      };\n    }).next(() => result);\n  }\n\n  getEntries(transaction, documentKeys) {\n    let results = mutableDocumentMap();\n    return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\n      const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\n      results = results.insert(key, doc);\n    }).next(() => results);\n  }\n  /**\r\n   * Looks up several entries in the cache.\r\n   *\r\n   * @param documentKeys - The set of keys entries to look up.\r\n   * @returns A map of documents indexed by key and a map of sizes indexed by\r\n   *     key (zero if the document does not exist).\r\n   */\n\n\n  getSizedEntries(transaction, documentKeys) {\n    let results = mutableDocumentMap();\n    let sizeMap = new SortedMap(DocumentKey.comparator);\n    return this.forEachDbEntry(transaction, documentKeys, (key, dbRemoteDoc) => {\n      const doc = this.maybeDecodeDocument(key, dbRemoteDoc);\n      results = results.insert(key, doc);\n      sizeMap = sizeMap.insert(key, dbDocumentSize(dbRemoteDoc));\n    }).next(() => {\n      return {\n        documents: results,\n        sizeMap\n      };\n    });\n  }\n\n  forEachDbEntry(transaction, documentKeys, callback) {\n    if (documentKeys.isEmpty()) {\n      return PersistencePromise.resolve();\n    }\n\n    let sortedKeys = new SortedSet(dbKeyComparator);\n    documentKeys.forEach(e => sortedKeys = sortedKeys.add(e));\n    const range = IDBKeyRange.bound(dbKey(sortedKeys.first()), dbKey(sortedKeys.last()));\n    const keyIter = sortedKeys.getIterator();\n    let nextKey = keyIter.getNext();\n    return remoteDocumentsStore(transaction).iterate({\n      index: DbRemoteDocumentDocumentKeyIndex,\n      range\n    }, (_, dbRemoteDoc, control) => {\n      const potentialKey = DocumentKey.fromSegments([...dbRemoteDoc.prefixPath, dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId]); // Go through keys not found in cache.\n\n      while (nextKey && dbKeyComparator(nextKey, potentialKey) < 0) {\n        callback(nextKey, null);\n        nextKey = keyIter.getNext();\n      }\n\n      if (nextKey && nextKey.isEqual(potentialKey)) {\n        // Key found in cache.\n        callback(nextKey, dbRemoteDoc);\n        nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\n      } // Skip to the next key (if there is one).\n\n\n      if (nextKey) {\n        control.skip(dbKey(nextKey));\n      } else {\n        control.done();\n      }\n    }).next(() => {\n      // The rest of the keys are not in the cache. One case where `iterate`\n      // above won't go through them is when the cache is empty.\n      while (nextKey) {\n        callback(nextKey, null);\n        nextKey = keyIter.hasNext() ? keyIter.getNext() : null;\n      }\n    });\n  }\n\n  getDocumentsMatchingQuery(transaction, query, offset, mutatedDocs) {\n    const collection = query.path;\n    const startKey = [collection.popLast().toArray(), collection.lastSegment(), toDbTimestampKey(offset.readTime), offset.documentKey.path.isEmpty() ? '' : offset.documentKey.path.lastSegment()];\n    const endKey = [collection.popLast().toArray(), collection.lastSegment(), [Number.MAX_SAFE_INTEGER, Number.MAX_SAFE_INTEGER], ''];\n    return remoteDocumentsStore(transaction).loadAll(IDBKeyRange.bound(startKey, endKey, true)).next(dbRemoteDocs => {\n      let results = mutableDocumentMap();\n\n      for (const dbRemoteDoc of dbRemoteDocs) {\n        const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\n\n        if (document.isFoundDocument() && (queryMatches(query, document) || mutatedDocs.has(document.key))) {\n          // Either the document matches the given query, or it is mutated.\n          results = results.insert(document.key, document);\n        }\n      }\n\n      return results;\n    });\n  }\n\n  getAllFromCollectionGroup(transaction, collectionGroup, offset, limit) {\n    let results = mutableDocumentMap();\n    const startKey = dbCollectionGroupKey(collectionGroup, offset);\n    const endKey = dbCollectionGroupKey(collectionGroup, IndexOffset.max());\n    return remoteDocumentsStore(transaction).iterate({\n      index: DbRemoteDocumentCollectionGroupIndex,\n      range: IDBKeyRange.bound(startKey, endKey, true)\n    }, (_, dbRemoteDoc, control) => {\n      const document = this.maybeDecodeDocument(DocumentKey.fromSegments(dbRemoteDoc.prefixPath.concat(dbRemoteDoc.collectionGroup, dbRemoteDoc.documentId)), dbRemoteDoc);\n      results = results.insert(document.key, document);\n\n      if (results.size === limit) {\n        control.done();\n      }\n    }).next(() => results);\n  }\n\n  newChangeBuffer(options) {\n    return new IndexedDbRemoteDocumentChangeBuffer(this, !!options && options.trackRemovals);\n  }\n\n  getSize(txn) {\n    return this.getMetadata(txn).next(metadata => metadata.byteSize);\n  }\n\n  getMetadata(txn) {\n    return documentGlobalStore(txn).get(DbRemoteDocumentGlobalKey).next(metadata => {\n      hardAssert(!!metadata);\n      return metadata;\n    });\n  }\n\n  setMetadata(txn, metadata) {\n    return documentGlobalStore(txn).put(DbRemoteDocumentGlobalKey, metadata);\n  }\n  /**\r\n   * Decodes `dbRemoteDoc` and returns the document (or an invalid document if\r\n   * the document corresponds to the format used for sentinel deletes).\r\n   */\n\n\n  maybeDecodeDocument(documentKey, dbRemoteDoc) {\n    if (dbRemoteDoc) {\n      const doc = fromDbRemoteDocument(this.serializer, dbRemoteDoc); // Whether the document is a sentinel removal and should only be used in the\n      // `getNewDocumentChanges()`\n\n      const isSentinelRemoval = doc.isNoDocument() && doc.version.isEqual(SnapshotVersion.min());\n\n      if (!isSentinelRemoval) {\n        return doc;\n      }\n    }\n\n    return MutableDocument.newInvalidDocument(documentKey);\n  }\n\n}\n/** Creates a new IndexedDbRemoteDocumentCache. */\n\n\nfunction newIndexedDbRemoteDocumentCache(serializer) {\n  return new IndexedDbRemoteDocumentCacheImpl(serializer);\n}\n/**\r\n * Handles the details of adding and updating documents in the IndexedDbRemoteDocumentCache.\r\n *\r\n * Unlike the MemoryRemoteDocumentChangeBuffer, the IndexedDb implementation computes the size\r\n * delta for all submitted changes. This avoids having to re-read all documents from IndexedDb\r\n * when we apply the changes.\r\n */\n\n\nclass IndexedDbRemoteDocumentChangeBuffer extends RemoteDocumentChangeBuffer {\n  /**\r\n   * @param documentCache - The IndexedDbRemoteDocumentCache to apply the changes to.\r\n   * @param trackRemovals - Whether to create sentinel deletes that can be tracked by\r\n   * `getNewDocumentChanges()`.\r\n   */\n  constructor(documentCache, trackRemovals) {\n    super();\n    this.documentCache = documentCache;\n    this.trackRemovals = trackRemovals; // A map of document sizes and read times prior to applying the changes in\n    // this buffer.\n\n    this.documentStates = new ObjectMap(key => key.toString(), (l, r) => l.isEqual(r));\n  }\n\n  applyChanges(transaction) {\n    const promises = [];\n    let sizeDelta = 0;\n    let collectionParents = new SortedSet((l, r) => primitiveComparator(l.canonicalString(), r.canonicalString()));\n    this.changes.forEach((key, documentChange) => {\n      const previousDoc = this.documentStates.get(key);\n      promises.push(this.documentCache.removeEntry(transaction, key, previousDoc.readTime));\n\n      if (documentChange.isValidDocument()) {\n        const doc = toDbRemoteDocument(this.documentCache.serializer, documentChange);\n        collectionParents = collectionParents.add(key.path.popLast());\n        const size = dbDocumentSize(doc);\n        sizeDelta += size - previousDoc.size;\n        promises.push(this.documentCache.addEntry(transaction, key, doc));\n      } else {\n        sizeDelta -= previousDoc.size;\n\n        if (this.trackRemovals) {\n          // In order to track removals, we store a \"sentinel delete\" in the\n          // RemoteDocumentCache. This entry is represented by a NoDocument\n          // with a version of 0 and ignored by `maybeDecodeDocument()` but\n          // preserved in `getNewDocumentChanges()`.\n          const deletedDoc = toDbRemoteDocument(this.documentCache.serializer, documentChange.convertToNoDocument(SnapshotVersion.min()));\n          promises.push(this.documentCache.addEntry(transaction, key, deletedDoc));\n        }\n      }\n    });\n    collectionParents.forEach(parent => {\n      promises.push(this.documentCache.indexManager.addToCollectionParentIndex(transaction, parent));\n    });\n    promises.push(this.documentCache.updateMetadata(transaction, sizeDelta));\n    return PersistencePromise.waitFor(promises);\n  }\n\n  getFromCache(transaction, documentKey) {\n    // Record the size of everything we load from the cache so we can compute a delta later.\n    return this.documentCache.getSizedEntry(transaction, documentKey).next(getResult => {\n      this.documentStates.set(documentKey, {\n        size: getResult.size,\n        readTime: getResult.document.readTime\n      });\n      return getResult.document;\n    });\n  }\n\n  getAllFromCache(transaction, documentKeys) {\n    // Record the size of everything we load from the cache so we can compute\n    // a delta later.\n    return this.documentCache.getSizedEntries(transaction, documentKeys).next(({\n      documents,\n      sizeMap\n    }) => {\n      // Note: `getAllFromCache` returns two maps instead of a single map from\n      // keys to `DocumentSizeEntry`s. This is to allow returning the\n      // `MutableDocumentMap` directly, without a conversion.\n      sizeMap.forEach((documentKey, size) => {\n        this.documentStates.set(documentKey, {\n          size,\n          readTime: documents.get(documentKey).readTime\n        });\n      });\n      return documents;\n    });\n  }\n\n}\n\nfunction documentGlobalStore(txn) {\n  return getStore(txn, DbRemoteDocumentGlobalStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the remoteDocuments object store.\r\n */\n\n\nfunction remoteDocumentsStore(txn) {\n  return getStore(txn, DbRemoteDocumentStore);\n}\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentKeyIndex` index.\r\n */\n\n\nfunction dbKey(documentKey) {\n  const path = documentKey.path.toArray();\n  return [\n  /* prefix path */\n  path.slice(0, path.length - 2),\n  /* collection id */\n  path[path.length - 2],\n  /* document id */\n  path[path.length - 1]];\n}\n/**\r\n * Returns a key that can be used for document lookups via the primary key of\r\n * the DbRemoteDocument object store.\r\n */\n\n\nfunction dbReadTimeKey(documentKey, readTime) {\n  const path = documentKey.path.toArray();\n  return [\n  /* prefix path */\n  path.slice(0, path.length - 2),\n  /* collection id */\n  path[path.length - 2], toDbTimestampKey(readTime),\n  /* document id */\n  path[path.length - 1]];\n}\n/**\r\n * Returns a key that can be used for document lookups on the\r\n * `DbRemoteDocumentDocumentCollectionGroupIndex` index.\r\n */\n\n\nfunction dbCollectionGroupKey(collectionGroup, offset) {\n  const path = offset.documentKey.path.toArray();\n  return [\n  /* collection id */\n  collectionGroup, toDbTimestampKey(offset.readTime),\n  /* prefix path */\n  path.slice(0, path.length - 2),\n  /* document id */\n  path.length > 0 ? path[path.length - 1] : ''];\n}\n/**\r\n * Comparator that compares document keys according to the primary key sorting\r\n * used by the `DbRemoteDocumentDocument` store (by prefix path, collection id\r\n * and then document ID).\r\n *\r\n * Visible for testing.\r\n */\n\n\nfunction dbKeyComparator(l, r) {\n  const left = l.path.toArray();\n  const right = r.path.toArray(); // The ordering is based on https://chromium.googlesource.com/chromium/blink/+/fe5c21fef94dae71c1c3344775b8d8a7f7e6d9ec/Source/modules/indexeddb/IDBKey.cpp#74\n\n  let cmp = 0;\n\n  for (let i = 0; i < left.length - 2 && i < right.length - 2; ++i) {\n    cmp = primitiveComparator(left[i], right[i]);\n\n    if (cmp) {\n      return cmp;\n    }\n  }\n\n  cmp = primitiveComparator(left.length, right.length);\n\n  if (cmp) {\n    return cmp;\n  }\n\n  cmp = primitiveComparator(left[left.length - 2], right[right.length - 2]);\n\n  if (cmp) {\n    return cmp;\n  }\n\n  return primitiveComparator(left[left.length - 1], right[right.length - 1]);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Schema Version for the Web client:\r\n * 1.  Initial version including Mutation Queue, Query Cache, and Remote\r\n *     Document Cache\r\n * 2.  Used to ensure a targetGlobal object exists and add targetCount to it. No\r\n *     longer required because migration 3 unconditionally clears it.\r\n * 3.  Dropped and re-created Query Cache to deal with cache corruption related\r\n *     to limbo resolution. Addresses\r\n *     https://github.com/firebase/firebase-ios-sdk/issues/1548\r\n * 4.  Multi-Tab Support.\r\n * 5.  Removal of held write acks.\r\n * 6.  Create document global for tracking document cache size.\r\n * 7.  Ensure every cached document has a sentinel row with a sequence number.\r\n * 8.  Add collection-parent index for Collection Group queries.\r\n * 9.  Change RemoteDocumentChanges store to be keyed by readTime rather than\r\n *     an auto-incrementing ID. This is required for Index-Free queries.\r\n * 10. Rewrite the canonical IDs to the explicit Protobuf-based format.\r\n * 11. Add bundles and named_queries for bundle support.\r\n * 12. Add document overlays.\r\n * 13. Rewrite the keys of the remote document cache to allow for efficient\r\n *     document lookup via `getAll()`.\r\n * 14. Add overlays.\r\n * 15. Add indexing support.\r\n */\n\n\nconst SCHEMA_VERSION = 15;\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents a local view (overlay) of a document, and the fields that are\r\n * locally mutated.\r\n */\n\nclass OverlayedDocument {\n  constructor(overlayedDocument,\n  /**\r\n   * The fields that are locally mutated by patch mutations.\r\n   *\r\n   * If the overlayed\tdocument is from set or delete mutations, this is `null`.\r\n   * If there is no overlay (mutation) for the document, this is an empty `FieldMask`.\r\n   */\n  mutatedFields) {\n    this.overlayedDocument = overlayedDocument;\n    this.mutatedFields = mutatedFields;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A readonly view of the local state of all documents we're tracking (i.e. we\r\n * have a cached version in remoteDocumentCache or local mutations for the\r\n * document). The view is computed by applying the mutations in the\r\n * MutationQueue to the RemoteDocumentCache.\r\n */\n\n\nclass LocalDocumentsView {\n  constructor(remoteDocumentCache, mutationQueue, documentOverlayCache, indexManager) {\n    this.remoteDocumentCache = remoteDocumentCache;\n    this.mutationQueue = mutationQueue;\n    this.documentOverlayCache = documentOverlayCache;\n    this.indexManager = indexManager;\n  }\n  /**\r\n   * Get the local view of the document identified by `key`.\r\n   *\r\n   * @returns Local view of the document or null if we don't have any cached\r\n   * state for it.\r\n   */\n\n\n  getDocument(transaction, key) {\n    let overlay = null;\n    return this.documentOverlayCache.getOverlay(transaction, key).next(value => {\n      overlay = value;\n      return this.remoteDocumentCache.getEntry(transaction, key);\n    }).next(document => {\n      if (overlay !== null) {\n        mutationApplyToLocalView(overlay.mutation, document, FieldMask.empty(), Timestamp.now());\n      }\n\n      return document;\n    });\n  }\n  /**\r\n   * Gets the local view of the documents identified by `keys`.\r\n   *\r\n   * If we don't have cached state for a document in `keys`, a NoDocument will\r\n   * be stored for that key in the resulting set.\r\n   */\n\n\n  getDocuments(transaction, keys) {\n    return this.remoteDocumentCache.getEntries(transaction, keys).next(docs => this.getLocalViewOfDocuments(transaction, docs, documentKeySet()).next(() => docs));\n  }\n  /**\r\n   * Similar to `getDocuments`, but creates the local view from the given\r\n   * `baseDocs` without retrieving documents from the local store.\r\n   *\r\n   * @param transaction - The transaction this operation is scoped to.\r\n   * @param docs - The documents to apply local mutations to get the local views.\r\n   * @param existenceStateChanged - The set of document keys whose existence state\r\n   *   is changed. This is useful to determine if some documents overlay needs\r\n   *   to be recalculated.\r\n   */\n\n\n  getLocalViewOfDocuments(transaction, docs, existenceStateChanged = documentKeySet()) {\n    const overlays = newOverlayMap();\n    return this.populateOverlays(transaction, overlays, docs).next(() => {\n      return this.computeViews(transaction, docs, overlays, existenceStateChanged).next(computeViewsResult => {\n        let result = documentMap();\n        computeViewsResult.forEach((documentKey, overlayedDocument) => {\n          result = result.insert(documentKey, overlayedDocument.overlayedDocument);\n        });\n        return result;\n      });\n    });\n  }\n  /**\r\n   * Gets the overlayed documents for the given document map, which will include\r\n   * the local view of those documents and a `FieldMask` indicating which fields\r\n   * are mutated locally, `null` if overlay is a Set or Delete mutation.\r\n   */\n\n\n  getOverlayedDocuments(transaction, docs) {\n    const overlays = newOverlayMap();\n    return this.populateOverlays(transaction, overlays, docs).next(() => this.computeViews(transaction, docs, overlays, documentKeySet()));\n  }\n  /**\r\n   * Fetches the overlays for {@code docs} and adds them to provided overlay map\r\n   * if the map does not already contain an entry for the given document key.\r\n   */\n\n\n  populateOverlays(transaction, overlays, docs) {\n    const missingOverlays = [];\n    docs.forEach(key => {\n      if (!overlays.has(key)) {\n        missingOverlays.push(key);\n      }\n    });\n    return this.documentOverlayCache.getOverlays(transaction, missingOverlays).next(result => {\n      result.forEach((key, val) => {\n        overlays.set(key, val);\n      });\n    });\n  }\n  /**\r\n   * Computes the local view for the given documents.\r\n   *\r\n   * @param docs - The documents to compute views for. It also has the base\r\n   *   version of the documents.\r\n   * @param overlays - The overlays that need to be applied to the given base\r\n   *   version of the documents.\r\n   * @param existenceStateChanged - A set of documents whose existence states\r\n   *   might have changed. This is used to determine if we need to re-calculate\r\n   *   overlays from mutation queues.\r\n   * @return A map represents the local documents view.\r\n   */\n\n\n  computeViews(transaction, docs, overlays, existenceStateChanged) {\n    let recalculateDocuments = mutableDocumentMap();\n    const mutatedFields = newDocumentKeyMap();\n    const results = newOverlayedDocumentMap();\n    docs.forEach((_, doc) => {\n      const overlay = overlays.get(doc.key); // Recalculate an overlay if the document's existence state changed due to\n      // a remote event *and* the overlay is a PatchMutation. This is because\n      // document existence state can change if some patch mutation's\n      // preconditions are met.\n      // NOTE: we recalculate when `overlay` is undefined as well, because there\n      // might be a patch mutation whose precondition does not match before the\n      // change (hence overlay is undefined), but would now match.\n\n      if (existenceStateChanged.has(doc.key) && (overlay === undefined || overlay.mutation instanceof PatchMutation)) {\n        recalculateDocuments = recalculateDocuments.insert(doc.key, doc);\n      } else if (overlay !== undefined) {\n        mutatedFields.set(doc.key, overlay.mutation.getFieldMask());\n        mutationApplyToLocalView(overlay.mutation, doc, overlay.mutation.getFieldMask(), Timestamp.now());\n      } else {\n        // no overlay exists\n        // Using EMPTY to indicate there is no overlay for the document.\n        mutatedFields.set(doc.key, FieldMask.empty());\n      }\n    });\n    return this.recalculateAndSaveOverlays(transaction, recalculateDocuments).next(recalculatedFields => {\n      recalculatedFields.forEach((documentKey, mask) => mutatedFields.set(documentKey, mask));\n      docs.forEach((documentKey, document) => {\n        var _a;\n\n        return results.set(documentKey, new OverlayedDocument(document, (_a = mutatedFields.get(documentKey)) !== null && _a !== void 0 ? _a : null));\n      });\n      return results;\n    });\n  }\n\n  recalculateAndSaveOverlays(transaction, docs) {\n    const masks = newDocumentKeyMap(); // A reverse lookup map from batch id to the documents within that batch.\n\n    let documentsByBatchId = new SortedMap((key1, key2) => key1 - key2);\n    let processed = documentKeySet();\n    return this.mutationQueue.getAllMutationBatchesAffectingDocumentKeys(transaction, docs).next(batches => {\n      for (const batch of batches) {\n        batch.keys().forEach(key => {\n          const baseDoc = docs.get(key);\n\n          if (baseDoc === null) {\n            return;\n          }\n\n          let mask = masks.get(key) || FieldMask.empty();\n          mask = batch.applyToLocalView(baseDoc, mask);\n          masks.set(key, mask);\n          const newSet = (documentsByBatchId.get(batch.batchId) || documentKeySet()).add(key);\n          documentsByBatchId = documentsByBatchId.insert(batch.batchId, newSet);\n        });\n      }\n    }).next(() => {\n      const promises = []; // Iterate in descending order of batch IDs, and skip documents that are\n      // already saved.\n\n      const iter = documentsByBatchId.getReverseIterator();\n\n      while (iter.hasNext()) {\n        const entry = iter.getNext();\n        const batchId = entry.key;\n        const keys = entry.value;\n        const overlays = newMutationMap();\n        keys.forEach(key => {\n          if (!processed.has(key)) {\n            const overlayMutation = calculateOverlayMutation(docs.get(key), masks.get(key));\n\n            if (overlayMutation !== null) {\n              overlays.set(key, overlayMutation);\n            }\n\n            processed = processed.add(key);\n          }\n        });\n        promises.push(this.documentOverlayCache.saveOverlays(transaction, batchId, overlays));\n      }\n\n      return PersistencePromise.waitFor(promises);\n    }).next(() => masks);\n  }\n  /**\r\n   * Recalculates overlays by reading the documents from remote document cache\r\n   * first, and saves them after they are calculated.\r\n   */\n\n\n  recalculateAndSaveOverlaysForDocumentKeys(transaction, documentKeys) {\n    return this.remoteDocumentCache.getEntries(transaction, documentKeys).next(docs => this.recalculateAndSaveOverlays(transaction, docs));\n  }\n  /**\r\n   * Performs a query against the local view of all documents.\r\n   *\r\n   * @param transaction - The persistence transaction.\r\n   * @param query - The query to match documents against.\r\n   * @param offset - Read time and key to start scanning by (exclusive).\r\n   */\n\n\n  getDocumentsMatchingQuery(transaction, query, offset) {\n    if (isDocumentQuery$1(query)) {\n      return this.getDocumentsMatchingDocumentQuery(transaction, query.path);\n    } else if (isCollectionGroupQuery(query)) {\n      return this.getDocumentsMatchingCollectionGroupQuery(transaction, query, offset);\n    } else {\n      return this.getDocumentsMatchingCollectionQuery(transaction, query, offset);\n    }\n  }\n  /**\r\n   * Given a collection group, returns the next documents that follow the provided offset, along\r\n   * with an updated batch ID.\r\n   *\r\n   * <p>The documents returned by this method are ordered by remote version from the provided\r\n   * offset. If there are no more remote documents after the provided offset, documents with\r\n   * mutations in order of batch id from the offset are returned. Since all documents in a batch are\r\n   * returned together, the total number of documents returned can exceed {@code count}.\r\n   *\r\n   * @param transaction\r\n   * @param collectionGroup The collection group for the documents.\r\n   * @param offset The offset to index into.\r\n   * @param count The number of documents to return\r\n   * @return A LocalWriteResult with the documents that follow the provided offset and the last processed batch id.\r\n   */\n\n\n  getNextDocuments(transaction, collectionGroup, offset, count) {\n    return this.remoteDocumentCache.getAllFromCollectionGroup(transaction, collectionGroup, offset, count).next(originalDocs => {\n      const overlaysPromise = count - originalDocs.size > 0 ? this.documentOverlayCache.getOverlaysForCollectionGroup(transaction, collectionGroup, offset.largestBatchId, count - originalDocs.size) : PersistencePromise.resolve(newOverlayMap()); // The callsite will use the largest batch ID together with the latest read time to create\n      // a new index offset. Since we only process batch IDs if all remote documents have been read,\n      // no overlay will increase the overall read time. This is why we only need to special case\n      // the batch id.\n\n      let largestBatchId = INITIAL_LARGEST_BATCH_ID;\n      let modifiedDocs = originalDocs;\n      return overlaysPromise.next(overlays => {\n        return PersistencePromise.forEach(overlays, (key, overlay) => {\n          if (largestBatchId < overlay.largestBatchId) {\n            largestBatchId = overlay.largestBatchId;\n          }\n\n          if (originalDocs.get(key)) {\n            return PersistencePromise.resolve();\n          }\n\n          return this.remoteDocumentCache.getEntry(transaction, key).next(doc => {\n            modifiedDocs = modifiedDocs.insert(key, doc);\n          });\n        }).next(() => this.populateOverlays(transaction, overlays, originalDocs)).next(() => this.computeViews(transaction, modifiedDocs, overlays, documentKeySet())).next(localDocs => ({\n          batchId: largestBatchId,\n          changes: convertOverlayedDocumentMapToDocumentMap(localDocs)\n        }));\n      });\n    });\n  }\n\n  getDocumentsMatchingDocumentQuery(transaction, docPath) {\n    // Just do a simple document lookup.\n    return this.getDocument(transaction, new DocumentKey(docPath)).next(document => {\n      let result = documentMap();\n\n      if (document.isFoundDocument()) {\n        result = result.insert(document.key, document);\n      }\n\n      return result;\n    });\n  }\n\n  getDocumentsMatchingCollectionGroupQuery(transaction, query, offset) {\n    const collectionId = query.collectionGroup;\n    let results = documentMap();\n    return this.indexManager.getCollectionParents(transaction, collectionId).next(parents => {\n      // Perform a collection query against each parent that contains the\n      // collectionId and aggregate the results.\n      return PersistencePromise.forEach(parents, parent => {\n        const collectionQuery = asCollectionQueryAtPath(query, parent.child(collectionId));\n        return this.getDocumentsMatchingCollectionQuery(transaction, collectionQuery, offset).next(r => {\n          r.forEach((key, doc) => {\n            results = results.insert(key, doc);\n          });\n        });\n      }).next(() => results);\n    });\n  }\n\n  getDocumentsMatchingCollectionQuery(transaction, query, offset) {\n    // Query the remote documents and overlay mutations.\n    let overlays;\n    return this.documentOverlayCache.getOverlaysForCollection(transaction, query.path, offset.largestBatchId).next(result => {\n      overlays = result;\n      return this.remoteDocumentCache.getDocumentsMatchingQuery(transaction, query, offset, overlays);\n    }).next(remoteDocuments => {\n      // As documents might match the query because of their overlay we need to\n      // include documents for all overlays in the initial document set.\n      overlays.forEach((_, overlay) => {\n        const key = overlay.getKey();\n\n        if (remoteDocuments.get(key) === null) {\n          remoteDocuments = remoteDocuments.insert(key, MutableDocument.newInvalidDocument(key));\n        }\n      }); // Apply the overlays and match against the query.\n\n      let results = documentMap();\n      remoteDocuments.forEach((key, document) => {\n        const overlay = overlays.get(key);\n\n        if (overlay !== undefined) {\n          mutationApplyToLocalView(overlay.mutation, document, FieldMask.empty(), Timestamp.now());\n        } // Finally, insert the documents that still match the query\n\n\n        if (queryMatches(query, document)) {\n          results = results.insert(key, document);\n        }\n      });\n      return results;\n    });\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass MemoryBundleCache {\n  constructor(serializer) {\n    this.serializer = serializer;\n    this.bundles = new Map();\n    this.namedQueries = new Map();\n  }\n\n  getBundleMetadata(transaction, bundleId) {\n    return PersistencePromise.resolve(this.bundles.get(bundleId));\n  }\n\n  saveBundleMetadata(transaction, bundleMetadata) {\n    this.bundles.set(bundleMetadata.id, fromBundleMetadata(bundleMetadata));\n    return PersistencePromise.resolve();\n  }\n\n  getNamedQuery(transaction, queryName) {\n    return PersistencePromise.resolve(this.namedQueries.get(queryName));\n  }\n\n  saveNamedQuery(transaction, query) {\n    this.namedQueries.set(query.name, fromProtoNamedQuery(query));\n    return PersistencePromise.resolve();\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An in-memory implementation of DocumentOverlayCache.\r\n */\n\n\nclass MemoryDocumentOverlayCache {\n  constructor() {\n    // A map sorted by DocumentKey, whose value is a pair of the largest batch id\n    // for the overlay and the overlay itself.\n    this.overlays = new SortedMap(DocumentKey.comparator);\n    this.overlayByBatchId = new Map();\n  }\n\n  getOverlay(transaction, key) {\n    return PersistencePromise.resolve(this.overlays.get(key));\n  }\n\n  getOverlays(transaction, keys) {\n    const result = newOverlayMap();\n    return PersistencePromise.forEach(keys, key => {\n      return this.getOverlay(transaction, key).next(overlay => {\n        if (overlay !== null) {\n          result.set(key, overlay);\n        }\n      });\n    }).next(() => result);\n  }\n\n  saveOverlays(transaction, largestBatchId, overlays) {\n    overlays.forEach((_, mutation) => {\n      this.saveOverlay(transaction, largestBatchId, mutation);\n    });\n    return PersistencePromise.resolve();\n  }\n\n  removeOverlaysForBatchId(transaction, documentKeys, batchId) {\n    const keys = this.overlayByBatchId.get(batchId);\n\n    if (keys !== undefined) {\n      keys.forEach(key => this.overlays = this.overlays.remove(key));\n      this.overlayByBatchId.delete(batchId);\n    }\n\n    return PersistencePromise.resolve();\n  }\n\n  getOverlaysForCollection(transaction, collection, sinceBatchId) {\n    const result = newOverlayMap();\n    const immediateChildrenPathLength = collection.length + 1;\n    const prefix = new DocumentKey(collection.child(''));\n    const iter = this.overlays.getIteratorFrom(prefix);\n\n    while (iter.hasNext()) {\n      const entry = iter.getNext();\n      const overlay = entry.value;\n      const key = overlay.getKey();\n\n      if (!collection.isPrefixOf(key.path)) {\n        break;\n      } // Documents from sub-collections\n\n\n      if (key.path.length !== immediateChildrenPathLength) {\n        continue;\n      }\n\n      if (overlay.largestBatchId > sinceBatchId) {\n        result.set(overlay.getKey(), overlay);\n      }\n    }\n\n    return PersistencePromise.resolve(result);\n  }\n\n  getOverlaysForCollectionGroup(transaction, collectionGroup, sinceBatchId, count) {\n    let batchIdToOverlays = new SortedMap((key1, key2) => key1 - key2);\n    const iter = this.overlays.getIterator();\n\n    while (iter.hasNext()) {\n      const entry = iter.getNext();\n      const overlay = entry.value;\n      const key = overlay.getKey();\n\n      if (key.getCollectionGroup() !== collectionGroup) {\n        continue;\n      }\n\n      if (overlay.largestBatchId > sinceBatchId) {\n        let overlaysForBatchId = batchIdToOverlays.get(overlay.largestBatchId);\n\n        if (overlaysForBatchId === null) {\n          overlaysForBatchId = newOverlayMap();\n          batchIdToOverlays = batchIdToOverlays.insert(overlay.largestBatchId, overlaysForBatchId);\n        }\n\n        overlaysForBatchId.set(overlay.getKey(), overlay);\n      }\n    }\n\n    const result = newOverlayMap();\n    const batchIter = batchIdToOverlays.getIterator();\n\n    while (batchIter.hasNext()) {\n      const entry = batchIter.getNext();\n      const overlays = entry.value;\n      overlays.forEach((key, overlay) => result.set(key, overlay));\n\n      if (result.size() >= count) {\n        break;\n      }\n    }\n\n    return PersistencePromise.resolve(result);\n  }\n\n  saveOverlay(transaction, largestBatchId, mutation) {\n    // Remove the association of the overlay to its batch id.\n    const existing = this.overlays.get(mutation.key);\n\n    if (existing !== null) {\n      const newSet = this.overlayByBatchId.get(existing.largestBatchId).delete(mutation.key);\n      this.overlayByBatchId.set(existing.largestBatchId, newSet);\n    }\n\n    this.overlays = this.overlays.insert(mutation.key, new Overlay(largestBatchId, mutation)); // Create the association of this overlay to the given largestBatchId.\n\n    let batch = this.overlayByBatchId.get(largestBatchId);\n\n    if (batch === undefined) {\n      batch = documentKeySet();\n      this.overlayByBatchId.set(largestBatchId, batch);\n    }\n\n    this.overlayByBatchId.set(largestBatchId, batch.add(mutation.key));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A collection of references to a document from some kind of numbered entity\r\n * (either a target ID or batch ID). As references are added to or removed from\r\n * the set corresponding events are emitted to a registered garbage collector.\r\n *\r\n * Each reference is represented by a DocumentReference object. Each of them\r\n * contains enough information to uniquely identify the reference. They are all\r\n * stored primarily in a set sorted by key. A document is considered garbage if\r\n * there's no references in that set (this can be efficiently checked thanks to\r\n * sorting by key).\r\n *\r\n * ReferenceSet also keeps a secondary set that contains references sorted by\r\n * IDs. This one is used to efficiently implement removal of all references by\r\n * some target ID.\r\n */\n\n\nclass ReferenceSet {\n  constructor() {\n    // A set of outstanding references to a document sorted by key.\n    this.refsByKey = new SortedSet(DocReference.compareByKey); // A set of outstanding references to a document sorted by target id.\n\n    this.refsByTarget = new SortedSet(DocReference.compareByTargetId);\n  }\n  /** Returns true if the reference set contains no references. */\n\n\n  isEmpty() {\n    return this.refsByKey.isEmpty();\n  }\n  /** Adds a reference to the given document key for the given ID. */\n\n\n  addReference(key, id) {\n    const ref = new DocReference(key, id);\n    this.refsByKey = this.refsByKey.add(ref);\n    this.refsByTarget = this.refsByTarget.add(ref);\n  }\n  /** Add references to the given document keys for the given ID. */\n\n\n  addReferences(keys, id) {\n    keys.forEach(key => this.addReference(key, id));\n  }\n  /**\r\n   * Removes a reference to the given document key for the given\r\n   * ID.\r\n   */\n\n\n  removeReference(key, id) {\n    this.removeRef(new DocReference(key, id));\n  }\n\n  removeReferences(keys, id) {\n    keys.forEach(key => this.removeReference(key, id));\n  }\n  /**\r\n   * Clears all references with a given ID. Calls removeRef() for each key\r\n   * removed.\r\n   */\n\n\n  removeReferencesForId(id) {\n    const emptyKey = new DocumentKey(new ResourcePath([]));\n    const startRef = new DocReference(emptyKey, id);\n    const endRef = new DocReference(emptyKey, id + 1);\n    const keys = [];\n    this.refsByTarget.forEachInRange([startRef, endRef], ref => {\n      this.removeRef(ref);\n      keys.push(ref.key);\n    });\n    return keys;\n  }\n\n  removeAllReferences() {\n    this.refsByKey.forEach(ref => this.removeRef(ref));\n  }\n\n  removeRef(ref) {\n    this.refsByKey = this.refsByKey.delete(ref);\n    this.refsByTarget = this.refsByTarget.delete(ref);\n  }\n\n  referencesForId(id) {\n    const emptyKey = new DocumentKey(new ResourcePath([]));\n    const startRef = new DocReference(emptyKey, id);\n    const endRef = new DocReference(emptyKey, id + 1);\n    let keys = documentKeySet();\n    this.refsByTarget.forEachInRange([startRef, endRef], ref => {\n      keys = keys.add(ref.key);\n    });\n    return keys;\n  }\n\n  containsKey(key) {\n    const ref = new DocReference(key, 0);\n    const firstRef = this.refsByKey.firstAfterOrEqual(ref);\n    return firstRef !== null && key.isEqual(firstRef.key);\n  }\n\n}\n\nclass DocReference {\n  constructor(key, targetOrBatchId) {\n    this.key = key;\n    this.targetOrBatchId = targetOrBatchId;\n  }\n  /** Compare by key then by ID */\n\n\n  static compareByKey(left, right) {\n    return DocumentKey.comparator(left.key, right.key) || primitiveComparator(left.targetOrBatchId, right.targetOrBatchId);\n  }\n  /** Compare by ID then by key */\n\n\n  static compareByTargetId(left, right) {\n    return primitiveComparator(left.targetOrBatchId, right.targetOrBatchId) || DocumentKey.comparator(left.key, right.key);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass MemoryMutationQueue {\n  constructor(indexManager, referenceDelegate) {\n    this.indexManager = indexManager;\n    this.referenceDelegate = referenceDelegate;\n    /**\r\n     * The set of all mutations that have been sent but not yet been applied to\r\n     * the backend.\r\n     */\n\n    this.mutationQueue = [];\n    /** Next value to use when assigning sequential IDs to each mutation batch. */\n\n    this.nextBatchId = 1;\n    /** An ordered mapping between documents and the mutations batch IDs. */\n\n    this.batchesByDocumentKey = new SortedSet(DocReference.compareByKey);\n  }\n\n  checkEmpty(transaction) {\n    return PersistencePromise.resolve(this.mutationQueue.length === 0);\n  }\n\n  addMutationBatch(transaction, localWriteTime, baseMutations, mutations) {\n    const batchId = this.nextBatchId;\n    this.nextBatchId++;\n\n    if (this.mutationQueue.length > 0) {\n      this.mutationQueue[this.mutationQueue.length - 1];\n    }\n\n    const batch = new MutationBatch(batchId, localWriteTime, baseMutations, mutations);\n    this.mutationQueue.push(batch); // Track references by document key and index collection parents.\n\n    for (const mutation of mutations) {\n      this.batchesByDocumentKey = this.batchesByDocumentKey.add(new DocReference(mutation.key, batchId));\n      this.indexManager.addToCollectionParentIndex(transaction, mutation.key.path.popLast());\n    }\n\n    return PersistencePromise.resolve(batch);\n  }\n\n  lookupMutationBatch(transaction, batchId) {\n    return PersistencePromise.resolve(this.findMutationBatch(batchId));\n  }\n\n  getNextMutationBatchAfterBatchId(transaction, batchId) {\n    const nextBatchId = batchId + 1; // The requested batchId may still be out of range so normalize it to the\n    // start of the queue.\n\n    const rawIndex = this.indexOfBatchId(nextBatchId);\n    const index = rawIndex < 0 ? 0 : rawIndex;\n    return PersistencePromise.resolve(this.mutationQueue.length > index ? this.mutationQueue[index] : null);\n  }\n\n  getHighestUnacknowledgedBatchId() {\n    return PersistencePromise.resolve(this.mutationQueue.length === 0 ? BATCHID_UNKNOWN : this.nextBatchId - 1);\n  }\n\n  getAllMutationBatches(transaction) {\n    return PersistencePromise.resolve(this.mutationQueue.slice());\n  }\n\n  getAllMutationBatchesAffectingDocumentKey(transaction, documentKey) {\n    const start = new DocReference(documentKey, 0);\n    const end = new DocReference(documentKey, Number.POSITIVE_INFINITY);\n    const result = [];\n    this.batchesByDocumentKey.forEachInRange([start, end], ref => {\n      const batch = this.findMutationBatch(ref.targetOrBatchId);\n      result.push(batch);\n    });\n    return PersistencePromise.resolve(result);\n  }\n\n  getAllMutationBatchesAffectingDocumentKeys(transaction, documentKeys) {\n    let uniqueBatchIDs = new SortedSet(primitiveComparator);\n    documentKeys.forEach(documentKey => {\n      const start = new DocReference(documentKey, 0);\n      const end = new DocReference(documentKey, Number.POSITIVE_INFINITY);\n      this.batchesByDocumentKey.forEachInRange([start, end], ref => {\n        uniqueBatchIDs = uniqueBatchIDs.add(ref.targetOrBatchId);\n      });\n    });\n    return PersistencePromise.resolve(this.findMutationBatches(uniqueBatchIDs));\n  }\n\n  getAllMutationBatchesAffectingQuery(transaction, query) {\n    // Use the query path as a prefix for testing if a document matches the\n    // query.\n    const prefix = query.path;\n    const immediateChildrenPathLength = prefix.length + 1; // Construct a document reference for actually scanning the index. Unlike\n    // the prefix the document key in this reference must have an even number of\n    // segments. The empty segment can be used a suffix of the query path\n    // because it precedes all other segments in an ordered traversal.\n\n    let startPath = prefix;\n\n    if (!DocumentKey.isDocumentKey(startPath)) {\n      startPath = startPath.child('');\n    }\n\n    const start = new DocReference(new DocumentKey(startPath), 0); // Find unique batchIDs referenced by all documents potentially matching the\n    // query.\n\n    let uniqueBatchIDs = new SortedSet(primitiveComparator);\n    this.batchesByDocumentKey.forEachWhile(ref => {\n      const rowKeyPath = ref.key.path;\n\n      if (!prefix.isPrefixOf(rowKeyPath)) {\n        return false;\n      } else {\n        // Rows with document keys more than one segment longer than the query\n        // path can't be matches. For example, a query on 'rooms' can't match\n        // the document /rooms/abc/messages/xyx.\n        // TODO(mcg): we'll need a different scanner when we implement\n        // ancestor queries.\n        if (rowKeyPath.length === immediateChildrenPathLength) {\n          uniqueBatchIDs = uniqueBatchIDs.add(ref.targetOrBatchId);\n        }\n\n        return true;\n      }\n    }, start);\n    return PersistencePromise.resolve(this.findMutationBatches(uniqueBatchIDs));\n  }\n\n  findMutationBatches(batchIDs) {\n    // Construct an array of matching batches, sorted by batchID to ensure that\n    // multiple mutations affecting the same document key are applied in order.\n    const result = [];\n    batchIDs.forEach(batchId => {\n      const batch = this.findMutationBatch(batchId);\n\n      if (batch !== null) {\n        result.push(batch);\n      }\n    });\n    return result;\n  }\n\n  removeMutationBatch(transaction, batch) {\n    // Find the position of the first batch for removal.\n    const batchIndex = this.indexOfExistingBatchId(batch.batchId, 'removed');\n    hardAssert(batchIndex === 0);\n    this.mutationQueue.shift();\n    let references = this.batchesByDocumentKey;\n    return PersistencePromise.forEach(batch.mutations, mutation => {\n      const ref = new DocReference(mutation.key, batch.batchId);\n      references = references.delete(ref);\n      return this.referenceDelegate.markPotentiallyOrphaned(transaction, mutation.key);\n    }).next(() => {\n      this.batchesByDocumentKey = references;\n    });\n  }\n\n  removeCachedMutationKeys(batchId) {// No-op since the memory mutation queue does not maintain a separate cache.\n  }\n\n  containsKey(txn, key) {\n    const ref = new DocReference(key, 0);\n    const firstRef = this.batchesByDocumentKey.firstAfterOrEqual(ref);\n    return PersistencePromise.resolve(key.isEqual(firstRef && firstRef.key));\n  }\n\n  performConsistencyCheck(txn) {\n    if (this.mutationQueue.length === 0) ;\n    return PersistencePromise.resolve();\n  }\n  /**\r\n   * Finds the index of the given batchId in the mutation queue and asserts that\r\n   * the resulting index is within the bounds of the queue.\r\n   *\r\n   * @param batchId - The batchId to search for\r\n   * @param action - A description of what the caller is doing, phrased in passive\r\n   * form (e.g. \"acknowledged\" in a routine that acknowledges batches).\r\n   */\n\n\n  indexOfExistingBatchId(batchId, action) {\n    const index = this.indexOfBatchId(batchId);\n    return index;\n  }\n  /**\r\n   * Finds the index of the given batchId in the mutation queue. This operation\r\n   * is O(1).\r\n   *\r\n   * @returns The computed index of the batch with the given batchId, based on\r\n   * the state of the queue. Note this index can be negative if the requested\r\n   * batchId has already been remvoed from the queue or past the end of the\r\n   * queue if the batchId is larger than the last added batch.\r\n   */\n\n\n  indexOfBatchId(batchId) {\n    if (this.mutationQueue.length === 0) {\n      // As an index this is past the end of the queue\n      return 0;\n    } // Examine the front of the queue to figure out the difference between the\n    // batchId and indexes in the array. Note that since the queue is ordered\n    // by batchId, if the first batch has a larger batchId then the requested\n    // batchId doesn't exist in the queue.\n\n\n    const firstBatchId = this.mutationQueue[0].batchId;\n    return batchId - firstBatchId;\n  }\n  /**\r\n   * A version of lookupMutationBatch that doesn't return a promise, this makes\r\n   * other functions that uses this code easier to read and more efficent.\r\n   */\n\n\n  findMutationBatch(batchId) {\n    const index = this.indexOfBatchId(batchId);\n\n    if (index < 0 || index >= this.mutationQueue.length) {\n      return null;\n    }\n\n    const batch = this.mutationQueue[index];\n    return batch;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction documentEntryMap() {\n  return new SortedMap(DocumentKey.comparator);\n}\n/**\r\n * The memory-only RemoteDocumentCache for IndexedDb. To construct, invoke\r\n * `newMemoryRemoteDocumentCache()`.\r\n */\n\n\nclass MemoryRemoteDocumentCacheImpl {\n  /**\r\n   * @param sizer - Used to assess the size of a document. For eager GC, this is\r\n   * expected to just return 0 to avoid unnecessarily doing the work of\r\n   * calculating the size.\r\n   */\n  constructor(sizer) {\n    this.sizer = sizer;\n    /** Underlying cache of documents and their read times. */\n\n    this.docs = documentEntryMap();\n    /** Size of all cached documents. */\n\n    this.size = 0;\n  }\n\n  setIndexManager(indexManager) {\n    this.indexManager = indexManager;\n  }\n  /**\r\n   * Adds the supplied entry to the cache and updates the cache size as appropriate.\r\n   *\r\n   * All calls of `addEntry`  are required to go through the RemoteDocumentChangeBuffer\r\n   * returned by `newChangeBuffer()`.\r\n   */\n\n\n  addEntry(transaction, doc) {\n    const key = doc.key;\n    const entry = this.docs.get(key);\n    const previousSize = entry ? entry.size : 0;\n    const currentSize = this.sizer(doc);\n    this.docs = this.docs.insert(key, {\n      document: doc.mutableCopy(),\n      size: currentSize\n    });\n    this.size += currentSize - previousSize;\n    return this.indexManager.addToCollectionParentIndex(transaction, key.path.popLast());\n  }\n  /**\r\n   * Removes the specified entry from the cache and updates the cache size as appropriate.\r\n   *\r\n   * All calls of `removeEntry` are required to go through the RemoteDocumentChangeBuffer\r\n   * returned by `newChangeBuffer()`.\r\n   */\n\n\n  removeEntry(documentKey) {\n    const entry = this.docs.get(documentKey);\n\n    if (entry) {\n      this.docs = this.docs.remove(documentKey);\n      this.size -= entry.size;\n    }\n  }\n\n  getEntry(transaction, documentKey) {\n    const entry = this.docs.get(documentKey);\n    return PersistencePromise.resolve(entry ? entry.document.mutableCopy() : MutableDocument.newInvalidDocument(documentKey));\n  }\n\n  getEntries(transaction, documentKeys) {\n    let results = mutableDocumentMap();\n    documentKeys.forEach(documentKey => {\n      const entry = this.docs.get(documentKey);\n      results = results.insert(documentKey, entry ? entry.document.mutableCopy() : MutableDocument.newInvalidDocument(documentKey));\n    });\n    return PersistencePromise.resolve(results);\n  }\n\n  getDocumentsMatchingQuery(transaction, query, offset, mutatedDocs) {\n    let results = mutableDocumentMap(); // Documents are ordered by key, so we can use a prefix scan to narrow down\n    // the documents we need to match the query against.\n\n    const collectionPath = query.path;\n    const prefix = new DocumentKey(collectionPath.child(''));\n    const iterator = this.docs.getIteratorFrom(prefix);\n\n    while (iterator.hasNext()) {\n      const {\n        key,\n        value: {\n          document\n        }\n      } = iterator.getNext();\n\n      if (!collectionPath.isPrefixOf(key.path)) {\n        break;\n      }\n\n      if (key.path.length > collectionPath.length + 1) {\n        // Exclude entries from subcollections.\n        continue;\n      }\n\n      if (indexOffsetComparator(newIndexOffsetFromDocument(document), offset) <= 0) {\n        // The document sorts before the offset.\n        continue;\n      }\n\n      if (!mutatedDocs.has(document.key) && !queryMatches(query, document)) {\n        // The document cannot possibly match the query.\n        continue;\n      }\n\n      results = results.insert(document.key, document.mutableCopy());\n    }\n\n    return PersistencePromise.resolve(results);\n  }\n\n  getAllFromCollectionGroup(transaction, collectionGroup, offset, limti) {\n    // This method should only be called from the IndexBackfiller if persistence\n    // is enabled.\n    fail();\n  }\n\n  forEachDocumentKey(transaction, f) {\n    return PersistencePromise.forEach(this.docs, key => f(key));\n  }\n\n  newChangeBuffer(options) {\n    // `trackRemovals` is ignores since the MemoryRemoteDocumentCache keeps\n    // a separate changelog and does not need special handling for removals.\n    return new MemoryRemoteDocumentChangeBuffer(this);\n  }\n\n  getSize(txn) {\n    return PersistencePromise.resolve(this.size);\n  }\n\n}\n/**\r\n * Creates a new memory-only RemoteDocumentCache.\r\n *\r\n * @param sizer - Used to assess the size of a document. For eager GC, this is\r\n * expected to just return 0 to avoid unnecessarily doing the work of\r\n * calculating the size.\r\n */\n\n\nfunction newMemoryRemoteDocumentCache(sizer) {\n  return new MemoryRemoteDocumentCacheImpl(sizer);\n}\n/**\r\n * Handles the details of adding and updating documents in the MemoryRemoteDocumentCache.\r\n */\n\n\nclass MemoryRemoteDocumentChangeBuffer extends RemoteDocumentChangeBuffer {\n  constructor(documentCache) {\n    super();\n    this.documentCache = documentCache;\n  }\n\n  applyChanges(transaction) {\n    const promises = [];\n    this.changes.forEach((key, doc) => {\n      if (doc.isValidDocument()) {\n        promises.push(this.documentCache.addEntry(transaction, doc));\n      } else {\n        this.documentCache.removeEntry(key);\n      }\n    });\n    return PersistencePromise.waitFor(promises);\n  }\n\n  getFromCache(transaction, documentKey) {\n    return this.documentCache.getEntry(transaction, documentKey);\n  }\n\n  getAllFromCache(transaction, documentKeys) {\n    return this.documentCache.getEntries(transaction, documentKeys);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass MemoryTargetCache {\n  constructor(persistence) {\n    this.persistence = persistence;\n    /**\r\n     * Maps a target to the data about that target\r\n     */\n\n    this.targets = new ObjectMap(t => canonifyTarget(t), targetEquals);\n    /** The last received snapshot version. */\n\n    this.lastRemoteSnapshotVersion = SnapshotVersion.min();\n    /** The highest numbered target ID encountered. */\n\n    this.highestTargetId = 0;\n    /** The highest sequence number encountered. */\n\n    this.highestSequenceNumber = 0;\n    /**\r\n     * A ordered bidirectional mapping between documents and the remote target\r\n     * IDs.\r\n     */\n\n    this.references = new ReferenceSet();\n    this.targetCount = 0;\n    this.targetIdGenerator = TargetIdGenerator.forTargetCache();\n  }\n\n  forEachTarget(txn, f) {\n    this.targets.forEach((_, targetData) => f(targetData));\n    return PersistencePromise.resolve();\n  }\n\n  getLastRemoteSnapshotVersion(transaction) {\n    return PersistencePromise.resolve(this.lastRemoteSnapshotVersion);\n  }\n\n  getHighestSequenceNumber(transaction) {\n    return PersistencePromise.resolve(this.highestSequenceNumber);\n  }\n\n  allocateTargetId(transaction) {\n    this.highestTargetId = this.targetIdGenerator.next();\n    return PersistencePromise.resolve(this.highestTargetId);\n  }\n\n  setTargetsMetadata(transaction, highestListenSequenceNumber, lastRemoteSnapshotVersion) {\n    if (lastRemoteSnapshotVersion) {\n      this.lastRemoteSnapshotVersion = lastRemoteSnapshotVersion;\n    }\n\n    if (highestListenSequenceNumber > this.highestSequenceNumber) {\n      this.highestSequenceNumber = highestListenSequenceNumber;\n    }\n\n    return PersistencePromise.resolve();\n  }\n\n  saveTargetData(targetData) {\n    this.targets.set(targetData.target, targetData);\n    const targetId = targetData.targetId;\n\n    if (targetId > this.highestTargetId) {\n      this.targetIdGenerator = new TargetIdGenerator(targetId);\n      this.highestTargetId = targetId;\n    }\n\n    if (targetData.sequenceNumber > this.highestSequenceNumber) {\n      this.highestSequenceNumber = targetData.sequenceNumber;\n    }\n  }\n\n  addTargetData(transaction, targetData) {\n    this.saveTargetData(targetData);\n    this.targetCount += 1;\n    return PersistencePromise.resolve();\n  }\n\n  updateTargetData(transaction, targetData) {\n    this.saveTargetData(targetData);\n    return PersistencePromise.resolve();\n  }\n\n  removeTargetData(transaction, targetData) {\n    this.targets.delete(targetData.target);\n    this.references.removeReferencesForId(targetData.targetId);\n    this.targetCount -= 1;\n    return PersistencePromise.resolve();\n  }\n\n  removeTargets(transaction, upperBound, activeTargetIds) {\n    let count = 0;\n    const removals = [];\n    this.targets.forEach((key, targetData) => {\n      if (targetData.sequenceNumber <= upperBound && activeTargetIds.get(targetData.targetId) === null) {\n        this.targets.delete(key);\n        removals.push(this.removeMatchingKeysForTargetId(transaction, targetData.targetId));\n        count++;\n      }\n    });\n    return PersistencePromise.waitFor(removals).next(() => count);\n  }\n\n  getTargetCount(transaction) {\n    return PersistencePromise.resolve(this.targetCount);\n  }\n\n  getTargetData(transaction, target) {\n    const targetData = this.targets.get(target) || null;\n    return PersistencePromise.resolve(targetData);\n  }\n\n  addMatchingKeys(txn, keys, targetId) {\n    this.references.addReferences(keys, targetId);\n    return PersistencePromise.resolve();\n  }\n\n  removeMatchingKeys(txn, keys, targetId) {\n    this.references.removeReferences(keys, targetId);\n    const referenceDelegate = this.persistence.referenceDelegate;\n    const promises = [];\n\n    if (referenceDelegate) {\n      keys.forEach(key => {\n        promises.push(referenceDelegate.markPotentiallyOrphaned(txn, key));\n      });\n    }\n\n    return PersistencePromise.waitFor(promises);\n  }\n\n  removeMatchingKeysForTargetId(txn, targetId) {\n    this.references.removeReferencesForId(targetId);\n    return PersistencePromise.resolve();\n  }\n\n  getMatchingKeysForTargetId(txn, targetId) {\n    const matchingKeys = this.references.referencesForId(targetId);\n    return PersistencePromise.resolve(matchingKeys);\n  }\n\n  containsKey(txn, key) {\n    return PersistencePromise.resolve(this.references.containsKey(key));\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$d = 'MemoryPersistence';\n/**\r\n * A memory-backed instance of Persistence. Data is stored only in RAM and\r\n * not persisted across sessions.\r\n */\n\nclass MemoryPersistence {\n  /**\r\n   * The constructor accepts a factory for creating a reference delegate. This\r\n   * allows both the delegate and this instance to have strong references to\r\n   * each other without having nullable fields that would then need to be\r\n   * checked or asserted on every access.\r\n   */\n  constructor(referenceDelegateFactory, serializer) {\n    this.mutationQueues = {};\n    this.overlays = {};\n    this.listenSequence = new ListenSequence(0);\n    this._started = false;\n    this._started = true;\n    this.referenceDelegate = referenceDelegateFactory(this);\n    this.targetCache = new MemoryTargetCache(this);\n\n    const sizer = doc => this.referenceDelegate.documentSize(doc);\n\n    this.indexManager = new MemoryIndexManager();\n    this.remoteDocumentCache = newMemoryRemoteDocumentCache(sizer);\n    this.serializer = new LocalSerializer(serializer);\n    this.bundleCache = new MemoryBundleCache(this.serializer);\n  }\n\n  start() {\n    return Promise.resolve();\n  }\n\n  shutdown() {\n    // No durable state to ensure is closed on shutdown.\n    this._started = false;\n    return Promise.resolve();\n  }\n\n  get started() {\n    return this._started;\n  }\n\n  setDatabaseDeletedListener() {// No op.\n  }\n\n  setNetworkEnabled() {// No op.\n  }\n\n  getIndexManager(user) {\n    // We do not currently support indices for memory persistence, so we can\n    // return the same shared instance of the memory index manager.\n    return this.indexManager;\n  }\n\n  getDocumentOverlayCache(user) {\n    let overlay = this.overlays[user.toKey()];\n\n    if (!overlay) {\n      overlay = new MemoryDocumentOverlayCache();\n      this.overlays[user.toKey()] = overlay;\n    }\n\n    return overlay;\n  }\n\n  getMutationQueue(user, indexManager) {\n    let queue = this.mutationQueues[user.toKey()];\n\n    if (!queue) {\n      queue = new MemoryMutationQueue(indexManager, this.referenceDelegate);\n      this.mutationQueues[user.toKey()] = queue;\n    }\n\n    return queue;\n  }\n\n  getTargetCache() {\n    return this.targetCache;\n  }\n\n  getRemoteDocumentCache() {\n    return this.remoteDocumentCache;\n  }\n\n  getBundleCache() {\n    return this.bundleCache;\n  }\n\n  runTransaction(action, mode, transactionOperation) {\n    logDebug(LOG_TAG$d, 'Starting transaction:', action);\n    const txn = new MemoryTransaction(this.listenSequence.next());\n    this.referenceDelegate.onTransactionStarted();\n    return transactionOperation(txn).next(result => {\n      return this.referenceDelegate.onTransactionCommitted(txn).next(() => result);\n    }).toPromise().then(result => {\n      txn.raiseOnCommittedEvent();\n      return result;\n    });\n  }\n\n  mutationQueuesContainKey(transaction, key) {\n    return PersistencePromise.or(Object.values(this.mutationQueues).map(queue => () => queue.containsKey(transaction, key)));\n  }\n\n}\n/**\r\n * Memory persistence is not actually transactional, but future implementations\r\n * may have transaction-scoped state.\r\n */\n\n\nclass MemoryTransaction extends PersistenceTransaction {\n  constructor(currentSequenceNumber) {\n    super();\n    this.currentSequenceNumber = currentSequenceNumber;\n  }\n\n}\n\nclass MemoryEagerDelegate {\n  constructor(persistence) {\n    this.persistence = persistence;\n    /** Tracks all documents that are active in Query views. */\n\n    this.localViewReferences = new ReferenceSet();\n    /** The list of documents that are potentially GCed after each transaction. */\n\n    this._orphanedDocuments = null;\n  }\n\n  static factory(persistence) {\n    return new MemoryEagerDelegate(persistence);\n  }\n\n  get orphanedDocuments() {\n    if (!this._orphanedDocuments) {\n      throw fail();\n    } else {\n      return this._orphanedDocuments;\n    }\n  }\n\n  addReference(txn, targetId, key) {\n    this.localViewReferences.addReference(key, targetId);\n    this.orphanedDocuments.delete(key.toString());\n    return PersistencePromise.resolve();\n  }\n\n  removeReference(txn, targetId, key) {\n    this.localViewReferences.removeReference(key, targetId);\n    this.orphanedDocuments.add(key.toString());\n    return PersistencePromise.resolve();\n  }\n\n  markPotentiallyOrphaned(txn, key) {\n    this.orphanedDocuments.add(key.toString());\n    return PersistencePromise.resolve();\n  }\n\n  removeTarget(txn, targetData) {\n    const orphaned = this.localViewReferences.removeReferencesForId(targetData.targetId);\n    orphaned.forEach(key => this.orphanedDocuments.add(key.toString()));\n    const cache = this.persistence.getTargetCache();\n    return cache.getMatchingKeysForTargetId(txn, targetData.targetId).next(keys => {\n      keys.forEach(key => this.orphanedDocuments.add(key.toString()));\n    }).next(() => cache.removeTargetData(txn, targetData));\n  }\n\n  onTransactionStarted() {\n    this._orphanedDocuments = new Set();\n  }\n\n  onTransactionCommitted(txn) {\n    // Remove newly orphaned documents.\n    const cache = this.persistence.getRemoteDocumentCache();\n    const changeBuffer = cache.newChangeBuffer();\n    return PersistencePromise.forEach(this.orphanedDocuments, path => {\n      const key = DocumentKey.fromPath(path);\n      return this.isReferenced(txn, key).next(isReferenced => {\n        if (!isReferenced) {\n          changeBuffer.removeEntry(key, SnapshotVersion.min());\n        }\n      });\n    }).next(() => {\n      this._orphanedDocuments = null;\n      return changeBuffer.apply(txn);\n    });\n  }\n\n  updateLimboDocument(txn, key) {\n    return this.isReferenced(txn, key).next(isReferenced => {\n      if (isReferenced) {\n        this.orphanedDocuments.delete(key.toString());\n      } else {\n        this.orphanedDocuments.add(key.toString());\n      }\n    });\n  }\n\n  documentSize(doc) {\n    // For eager GC, we don't care about the document size, there are no size thresholds.\n    return 0;\n  }\n\n  isReferenced(txn, key) {\n    return PersistencePromise.or([() => PersistencePromise.resolve(this.localViewReferences.containsKey(key)), () => this.persistence.getTargetCache().containsKey(txn, key), () => this.persistence.mutationQueuesContainKey(txn, key)]);\n  }\n\n}\n\nclass MemoryLruDelegate {\n  constructor(persistence, lruParams) {\n    this.persistence = persistence;\n    this.orphanedSequenceNumbers = new ObjectMap(k => encodeResourcePath(k.path), (l, r) => l.isEqual(r));\n    this.garbageCollector = newLruGarbageCollector(this, lruParams);\n  }\n\n  static factory(persistence, lruParams) {\n    return new MemoryLruDelegate(persistence, lruParams);\n  } // No-ops, present so memory persistence doesn't have to care which delegate\n  // it has.\n\n\n  onTransactionStarted() {}\n\n  onTransactionCommitted(txn) {\n    return PersistencePromise.resolve();\n  }\n\n  forEachTarget(txn, f) {\n    return this.persistence.getTargetCache().forEachTarget(txn, f);\n  }\n\n  getSequenceNumberCount(txn) {\n    const docCountPromise = this.orphanedDocumentCount(txn);\n    const targetCountPromise = this.persistence.getTargetCache().getTargetCount(txn);\n    return targetCountPromise.next(targetCount => docCountPromise.next(docCount => targetCount + docCount));\n  }\n\n  orphanedDocumentCount(txn) {\n    let orphanedCount = 0;\n    return this.forEachOrphanedDocumentSequenceNumber(txn, _ => {\n      orphanedCount++;\n    }).next(() => orphanedCount);\n  }\n\n  forEachOrphanedDocumentSequenceNumber(txn, f) {\n    return PersistencePromise.forEach(this.orphanedSequenceNumbers, (key, sequenceNumber) => {\n      // Pass in the exact sequence number as the upper bound so we know it won't be pinned by\n      // being too recent.\n      return this.isPinned(txn, key, sequenceNumber).next(isPinned => {\n        if (!isPinned) {\n          return f(sequenceNumber);\n        } else {\n          return PersistencePromise.resolve();\n        }\n      });\n    });\n  }\n\n  removeTargets(txn, upperBound, activeTargetIds) {\n    return this.persistence.getTargetCache().removeTargets(txn, upperBound, activeTargetIds);\n  }\n\n  removeOrphanedDocuments(txn, upperBound) {\n    let count = 0;\n    const cache = this.persistence.getRemoteDocumentCache();\n    const changeBuffer = cache.newChangeBuffer();\n    const p = cache.forEachDocumentKey(txn, key => {\n      return this.isPinned(txn, key, upperBound).next(isPinned => {\n        if (!isPinned) {\n          count++;\n          changeBuffer.removeEntry(key, SnapshotVersion.min());\n        }\n      });\n    });\n    return p.next(() => changeBuffer.apply(txn)).next(() => count);\n  }\n\n  markPotentiallyOrphaned(txn, key) {\n    this.orphanedSequenceNumbers.set(key, txn.currentSequenceNumber);\n    return PersistencePromise.resolve();\n  }\n\n  removeTarget(txn, targetData) {\n    const updated = targetData.withSequenceNumber(txn.currentSequenceNumber);\n    return this.persistence.getTargetCache().updateTargetData(txn, updated);\n  }\n\n  addReference(txn, targetId, key) {\n    this.orphanedSequenceNumbers.set(key, txn.currentSequenceNumber);\n    return PersistencePromise.resolve();\n  }\n\n  removeReference(txn, targetId, key) {\n    this.orphanedSequenceNumbers.set(key, txn.currentSequenceNumber);\n    return PersistencePromise.resolve();\n  }\n\n  updateLimboDocument(txn, key) {\n    this.orphanedSequenceNumbers.set(key, txn.currentSequenceNumber);\n    return PersistencePromise.resolve();\n  }\n\n  documentSize(document) {\n    let documentSize = document.key.toString().length;\n\n    if (document.isFoundDocument()) {\n      documentSize += estimateByteSize(document.data.value);\n    }\n\n    return documentSize;\n  }\n\n  isPinned(txn, key, upperBound) {\n    return PersistencePromise.or([() => this.persistence.mutationQueuesContainKey(txn, key), () => this.persistence.getTargetCache().containsKey(txn, key), () => {\n      const orphanedAt = this.orphanedSequenceNumbers.get(key);\n      return PersistencePromise.resolve(orphanedAt !== undefined && orphanedAt > upperBound);\n    }]);\n  }\n\n  getCacheSize(txn) {\n    return this.persistence.getRemoteDocumentCache().getSize(txn);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Performs database creation and schema upgrades. */\n\n\nclass SchemaConverter {\n  constructor(serializer) {\n    this.serializer = serializer;\n  }\n  /**\r\n   * Performs database creation and schema upgrades.\r\n   *\r\n   * Note that in production, this method is only ever used to upgrade the schema\r\n   * to SCHEMA_VERSION. Different values of toVersion are only used for testing\r\n   * and local feature development.\r\n   */\n\n\n  createOrUpgrade(db, txn, fromVersion, toVersion) {\n    const simpleDbTransaction = new SimpleDbTransaction('createOrUpgrade', txn);\n\n    if (fromVersion < 1 && toVersion >= 1) {\n      createPrimaryClientStore(db);\n      createMutationQueue(db);\n      createQueryCache(db);\n      createLegacyRemoteDocumentCache(db);\n    } // Migration 2 to populate the targetGlobal object no longer needed since\n    // migration 3 unconditionally clears it.\n\n\n    let p = PersistencePromise.resolve();\n\n    if (fromVersion < 3 && toVersion >= 3) {\n      // Brand new clients don't need to drop and recreate--only clients that\n      // potentially have corrupt data.\n      if (fromVersion !== 0) {\n        dropQueryCache(db);\n        createQueryCache(db);\n      }\n\n      p = p.next(() => writeEmptyTargetGlobalEntry(simpleDbTransaction));\n    }\n\n    if (fromVersion < 4 && toVersion >= 4) {\n      if (fromVersion !== 0) {\n        // Schema version 3 uses auto-generated keys to generate globally unique\n        // mutation batch IDs (this was previously ensured internally by the\n        // client). To migrate to the new schema, we have to read all mutations\n        // and write them back out. We preserve the existing batch IDs to guarantee\n        // consistency with other object stores. Any further mutation batch IDs will\n        // be auto-generated.\n        p = p.next(() => upgradeMutationBatchSchemaAndMigrateData(db, simpleDbTransaction));\n      }\n\n      p = p.next(() => {\n        createClientMetadataStore(db);\n      });\n    }\n\n    if (fromVersion < 5 && toVersion >= 5) {\n      p = p.next(() => this.removeAcknowledgedMutations(simpleDbTransaction));\n    }\n\n    if (fromVersion < 6 && toVersion >= 6) {\n      p = p.next(() => {\n        createDocumentGlobalStore(db);\n        return this.addDocumentGlobal(simpleDbTransaction);\n      });\n    }\n\n    if (fromVersion < 7 && toVersion >= 7) {\n      p = p.next(() => this.ensureSequenceNumbers(simpleDbTransaction));\n    }\n\n    if (fromVersion < 8 && toVersion >= 8) {\n      p = p.next(() => this.createCollectionParentIndex(db, simpleDbTransaction));\n    }\n\n    if (fromVersion < 9 && toVersion >= 9) {\n      p = p.next(() => {\n        // Multi-Tab used to manage its own changelog, but this has been moved\n        // to the DbRemoteDocument object store itself. Since the previous change\n        // log only contained transient data, we can drop its object store.\n        dropRemoteDocumentChangesStore(db); // Note: Schema version 9 used to create a read time index for the\n        // RemoteDocumentCache. This is now done with schema version 13.\n      });\n    }\n\n    if (fromVersion < 10 && toVersion >= 10) {\n      p = p.next(() => this.rewriteCanonicalIds(simpleDbTransaction));\n    }\n\n    if (fromVersion < 11 && toVersion >= 11) {\n      p = p.next(() => {\n        createBundlesStore(db);\n        createNamedQueriesStore(db);\n      });\n    }\n\n    if (fromVersion < 12 && toVersion >= 12) {\n      p = p.next(() => {\n        createDocumentOverlayStore(db);\n      });\n    }\n\n    if (fromVersion < 13 && toVersion >= 13) {\n      p = p.next(() => createRemoteDocumentCache(db)).next(() => this.rewriteRemoteDocumentCache(db, simpleDbTransaction)).next(() => db.deleteObjectStore(DbRemoteDocumentStore$1));\n    }\n\n    if (fromVersion < 14 && toVersion >= 14) {\n      p = p.next(() => this.runOverlayMigration(db, simpleDbTransaction));\n    }\n\n    if (fromVersion < 15 && toVersion >= 15) {\n      p = p.next(() => createFieldIndex(db));\n    }\n\n    return p;\n  }\n\n  addDocumentGlobal(txn) {\n    let byteSize = 0;\n    return txn.store(DbRemoteDocumentStore$1).iterate((_, doc) => {\n      byteSize += dbDocumentSize(doc);\n    }).next(() => {\n      const metadata = {\n        byteSize\n      };\n      return txn.store(DbRemoteDocumentGlobalStore).put(DbRemoteDocumentGlobalKey, metadata);\n    });\n  }\n\n  removeAcknowledgedMutations(txn) {\n    const queuesStore = txn.store(DbMutationQueueStore);\n    const mutationsStore = txn.store(DbMutationBatchStore);\n    return queuesStore.loadAll().next(queues => {\n      return PersistencePromise.forEach(queues, queue => {\n        const range = IDBKeyRange.bound([queue.userId, BATCHID_UNKNOWN], [queue.userId, queue.lastAcknowledgedBatchId]);\n        return mutationsStore.loadAll(DbMutationBatchUserMutationsIndex, range).next(dbBatches => {\n          return PersistencePromise.forEach(dbBatches, dbBatch => {\n            hardAssert(dbBatch.userId === queue.userId);\n            const batch = fromDbMutationBatch(this.serializer, dbBatch);\n            return removeMutationBatch(txn, queue.userId, batch).next(() => {});\n          });\n        });\n      });\n    });\n  }\n  /**\r\n   * Ensures that every document in the remote document cache has a corresponding sentinel row\r\n   * with a sequence number. Missing rows are given the most recently used sequence number.\r\n   */\n\n\n  ensureSequenceNumbers(txn) {\n    const documentTargetStore = txn.store(DbTargetDocumentStore);\n    const documentsStore = txn.store(DbRemoteDocumentStore$1);\n    const globalTargetStore = txn.store(DbTargetGlobalStore);\n    return globalTargetStore.get(DbTargetGlobalKey).next(metadata => {\n      const writeSentinelKey = path => {\n        return documentTargetStore.put({\n          targetId: 0,\n          path: encodeResourcePath(path),\n          sequenceNumber: metadata.highestListenSequenceNumber\n        });\n      };\n\n      const promises = [];\n      return documentsStore.iterate((key, doc) => {\n        const path = new ResourcePath(key);\n        const docSentinelKey = sentinelKey(path);\n        promises.push(documentTargetStore.get(docSentinelKey).next(maybeSentinel => {\n          if (!maybeSentinel) {\n            return writeSentinelKey(path);\n          } else {\n            return PersistencePromise.resolve();\n          }\n        }));\n      }).next(() => PersistencePromise.waitFor(promises));\n    });\n  }\n\n  createCollectionParentIndex(db, txn) {\n    // Create the index.\n    db.createObjectStore(DbCollectionParentStore, {\n      keyPath: DbCollectionParentKeyPath\n    });\n    const collectionParentsStore = txn.store(DbCollectionParentStore); // Helper to add an index entry iff we haven't already written it.\n\n    const cache = new MemoryCollectionParentIndex();\n\n    const addEntry = collectionPath => {\n      if (cache.add(collectionPath)) {\n        const collectionId = collectionPath.lastSegment();\n        const parentPath = collectionPath.popLast();\n        return collectionParentsStore.put({\n          collectionId,\n          parent: encodeResourcePath(parentPath)\n        });\n      }\n    }; // Index existing remote documents.\n\n\n    return txn.store(DbRemoteDocumentStore$1).iterate({\n      keysOnly: true\n    }, (pathSegments, _) => {\n      const path = new ResourcePath(pathSegments);\n      return addEntry(path.popLast());\n    }).next(() => {\n      // Index existing mutations.\n      return txn.store(DbDocumentMutationStore).iterate({\n        keysOnly: true\n      }, ([userID, encodedPath, batchId], _) => {\n        const path = decodeResourcePath(encodedPath);\n        return addEntry(path.popLast());\n      });\n    });\n  }\n\n  rewriteCanonicalIds(txn) {\n    const targetStore = txn.store(DbTargetStore);\n    return targetStore.iterate((key, originalDbTarget) => {\n      const originalTargetData = fromDbTarget(originalDbTarget);\n      const updatedDbTarget = toDbTarget(this.serializer, originalTargetData);\n      return targetStore.put(updatedDbTarget);\n    });\n  }\n\n  rewriteRemoteDocumentCache(db, transaction) {\n    const legacyRemoteDocumentStore = transaction.store(DbRemoteDocumentStore$1);\n    const writes = [];\n    return legacyRemoteDocumentStore.iterate((_, legacyDocument) => {\n      const remoteDocumentStore = transaction.store(DbRemoteDocumentStore);\n      const path = extractKey(legacyDocument).path.toArray();\n      const dbRemoteDocument = {\n        prefixPath: path.slice(0, path.length - 2),\n        collectionGroup: path[path.length - 2],\n        documentId: path[path.length - 1],\n        readTime: legacyDocument.readTime || [0, 0],\n        unknownDocument: legacyDocument.unknownDocument,\n        noDocument: legacyDocument.noDocument,\n        document: legacyDocument.document,\n        hasCommittedMutations: !!legacyDocument.hasCommittedMutations\n      };\n      writes.push(remoteDocumentStore.put(dbRemoteDocument));\n    }).next(() => PersistencePromise.waitFor(writes));\n  }\n\n  runOverlayMigration(db, transaction) {\n    const mutationsStore = transaction.store(DbMutationBatchStore);\n    const remoteDocumentCache = newIndexedDbRemoteDocumentCache(this.serializer);\n    const memoryPersistence = new MemoryPersistence(MemoryEagerDelegate.factory, this.serializer.remoteSerializer);\n    return mutationsStore.loadAll().next(dbBatches => {\n      const userToDocumentSet = new Map();\n      dbBatches.forEach(dbBatch => {\n        var _a;\n\n        let documentSet = (_a = userToDocumentSet.get(dbBatch.userId)) !== null && _a !== void 0 ? _a : documentKeySet();\n        const batch = fromDbMutationBatch(this.serializer, dbBatch);\n        batch.keys().forEach(key => documentSet = documentSet.add(key));\n        userToDocumentSet.set(dbBatch.userId, documentSet);\n      });\n      return PersistencePromise.forEach(userToDocumentSet, (allDocumentKeysForUser, userId) => {\n        const user = new User(userId);\n        const documentOverlayCache = IndexedDbDocumentOverlayCache.forUser(this.serializer, user); // NOTE: The index manager and the reference delegate are\n        // irrelevant for the purpose of recalculating and saving\n        // overlays. We can therefore simply use the memory\n        // implementation.\n\n        const indexManager = memoryPersistence.getIndexManager(user);\n        const mutationQueue = IndexedDbMutationQueue.forUser(user, this.serializer, indexManager, memoryPersistence.referenceDelegate);\n        const localDocumentsView = new LocalDocumentsView(remoteDocumentCache, mutationQueue, documentOverlayCache, indexManager);\n        return localDocumentsView.recalculateAndSaveOverlaysForDocumentKeys(new IndexedDbTransaction(transaction, ListenSequence.INVALID), allDocumentKeysForUser).next();\n      });\n    });\n  }\n\n}\n\nfunction sentinelKey(path) {\n  return [0, encodeResourcePath(path)];\n}\n\nfunction createPrimaryClientStore(db) {\n  db.createObjectStore(DbPrimaryClientStore);\n}\n\nfunction createMutationQueue(db) {\n  db.createObjectStore(DbMutationQueueStore, {\n    keyPath: DbMutationQueueKeyPath\n  });\n  const mutationBatchesStore = db.createObjectStore(DbMutationBatchStore, {\n    keyPath: DbMutationBatchKeyPath,\n    autoIncrement: true\n  });\n  mutationBatchesStore.createIndex(DbMutationBatchUserMutationsIndex, DbMutationBatchUserMutationsKeyPath, {\n    unique: true\n  });\n  db.createObjectStore(DbDocumentMutationStore);\n}\n/**\r\n * Upgrade function to migrate the 'mutations' store from V1 to V3. Loads\r\n * and rewrites all data.\r\n */\n\n\nfunction upgradeMutationBatchSchemaAndMigrateData(db, txn) {\n  const v1MutationsStore = txn.store(DbMutationBatchStore);\n  return v1MutationsStore.loadAll().next(existingMutations => {\n    db.deleteObjectStore(DbMutationBatchStore);\n    const mutationsStore = db.createObjectStore(DbMutationBatchStore, {\n      keyPath: DbMutationBatchKeyPath,\n      autoIncrement: true\n    });\n    mutationsStore.createIndex(DbMutationBatchUserMutationsIndex, DbMutationBatchUserMutationsKeyPath, {\n      unique: true\n    });\n    const v3MutationsStore = txn.store(DbMutationBatchStore);\n    const writeAll = existingMutations.map(mutation => v3MutationsStore.put(mutation));\n    return PersistencePromise.waitFor(writeAll);\n  });\n}\n\nfunction createLegacyRemoteDocumentCache(db) {\n  db.createObjectStore(DbRemoteDocumentStore$1);\n}\n\nfunction createRemoteDocumentCache(db) {\n  const remoteDocumentStore = db.createObjectStore(DbRemoteDocumentStore, {\n    keyPath: DbRemoteDocumentKeyPath\n  });\n  remoteDocumentStore.createIndex(DbRemoteDocumentDocumentKeyIndex, DbRemoteDocumentDocumentKeyIndexPath);\n  remoteDocumentStore.createIndex(DbRemoteDocumentCollectionGroupIndex, DbRemoteDocumentCollectionGroupIndexPath);\n}\n\nfunction createDocumentGlobalStore(db) {\n  db.createObjectStore(DbRemoteDocumentGlobalStore);\n}\n\nfunction createQueryCache(db) {\n  const targetDocumentsStore = db.createObjectStore(DbTargetDocumentStore, {\n    keyPath: DbTargetDocumentKeyPath\n  });\n  targetDocumentsStore.createIndex(DbTargetDocumentDocumentTargetsIndex, DbTargetDocumentDocumentTargetsKeyPath, {\n    unique: true\n  });\n  const targetStore = db.createObjectStore(DbTargetStore, {\n    keyPath: DbTargetKeyPath\n  }); // NOTE: This is unique only because the TargetId is the suffix.\n\n  targetStore.createIndex(DbTargetQueryTargetsIndexName, DbTargetQueryTargetsKeyPath, {\n    unique: true\n  });\n  db.createObjectStore(DbTargetGlobalStore);\n}\n\nfunction dropQueryCache(db) {\n  db.deleteObjectStore(DbTargetDocumentStore);\n  db.deleteObjectStore(DbTargetStore);\n  db.deleteObjectStore(DbTargetGlobalStore);\n}\n\nfunction dropRemoteDocumentChangesStore(db) {\n  if (db.objectStoreNames.contains('remoteDocumentChanges')) {\n    db.deleteObjectStore('remoteDocumentChanges');\n  }\n}\n/**\r\n * Creates the target global singleton row.\r\n *\r\n * @param txn - The version upgrade transaction for indexeddb\r\n */\n\n\nfunction writeEmptyTargetGlobalEntry(txn) {\n  const globalStore = txn.store(DbTargetGlobalStore);\n  const metadata = {\n    highestTargetId: 0,\n    highestListenSequenceNumber: 0,\n    lastRemoteSnapshotVersion: SnapshotVersion.min().toTimestamp(),\n    targetCount: 0\n  };\n  return globalStore.put(DbTargetGlobalKey, metadata);\n}\n\nfunction createClientMetadataStore(db) {\n  db.createObjectStore(DbClientMetadataStore, {\n    keyPath: DbClientMetadataKeyPath\n  });\n}\n\nfunction createBundlesStore(db) {\n  db.createObjectStore(DbBundleStore, {\n    keyPath: DbBundleKeyPath\n  });\n}\n\nfunction createNamedQueriesStore(db) {\n  db.createObjectStore(DbNamedQueryStore, {\n    keyPath: DbNamedQueryKeyPath\n  });\n}\n\nfunction createFieldIndex(db) {\n  const indexConfigurationStore = db.createObjectStore(DbIndexConfigurationStore, {\n    keyPath: DbIndexConfigurationKeyPath,\n    autoIncrement: true\n  });\n  indexConfigurationStore.createIndex(DbIndexConfigurationCollectionGroupIndex, DbIndexConfigurationCollectionGroupIndexPath, {\n    unique: false\n  });\n  const indexStateStore = db.createObjectStore(DbIndexStateStore, {\n    keyPath: DbIndexStateKeyPath\n  });\n  indexStateStore.createIndex(DbIndexStateSequenceNumberIndex, DbIndexStateSequenceNumberIndexPath, {\n    unique: false\n  });\n  const indexEntryStore = db.createObjectStore(DbIndexEntryStore, {\n    keyPath: DbIndexEntryKeyPath\n  });\n  indexEntryStore.createIndex(DbIndexEntryDocumentKeyIndex, DbIndexEntryDocumentKeyIndexPath, {\n    unique: false\n  });\n}\n\nfunction createDocumentOverlayStore(db) {\n  const documentOverlayStore = db.createObjectStore(DbDocumentOverlayStore, {\n    keyPath: DbDocumentOverlayKeyPath\n  });\n  documentOverlayStore.createIndex(DbDocumentOverlayCollectionPathOverlayIndex, DbDocumentOverlayCollectionPathOverlayIndexPath, {\n    unique: false\n  });\n  documentOverlayStore.createIndex(DbDocumentOverlayCollectionGroupOverlayIndex, DbDocumentOverlayCollectionGroupOverlayIndexPath, {\n    unique: false\n  });\n}\n\nfunction extractKey(remoteDoc) {\n  if (remoteDoc.document) {\n    return new DocumentKey(ResourcePath.fromString(remoteDoc.document.name).popFirst(5));\n  } else if (remoteDoc.noDocument) {\n    return DocumentKey.fromSegments(remoteDoc.noDocument.path);\n  } else if (remoteDoc.unknownDocument) {\n    return DocumentKey.fromSegments(remoteDoc.unknownDocument.path);\n  } else {\n    return fail();\n  }\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$c = 'IndexedDbPersistence';\n/**\r\n * Oldest acceptable age in milliseconds for client metadata before the client\r\n * is considered inactive and its associated data is garbage collected.\r\n */\n\nconst MAX_CLIENT_AGE_MS = 30 * 60 * 1000; // 30 minutes\n\n/**\r\n * Oldest acceptable metadata age for clients that may participate in the\r\n * primary lease election. Clients that have not updated their client metadata\r\n * within 5 seconds are not eligible to receive a primary lease.\r\n */\n\nconst MAX_PRIMARY_ELIGIBLE_AGE_MS = 5000;\n/**\r\n * The interval at which clients will update their metadata, including\r\n * refreshing their primary lease if held or potentially trying to acquire it if\r\n * not held.\r\n *\r\n * Primary clients may opportunistically refresh their metadata earlier\r\n * if they're already performing an IndexedDB operation.\r\n */\n\nconst CLIENT_METADATA_REFRESH_INTERVAL_MS = 4000;\n/** User-facing error when the primary lease is required but not available. */\n\nconst PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG = 'Failed to obtain exclusive access to the persistence layer. To allow ' + 'shared access, multi-tab synchronization has to be enabled in all tabs. ' + 'If you are using `experimentalForceOwningTab:true`, make sure that only ' + 'one tab has persistence enabled at any given time.';\nconst UNSUPPORTED_PLATFORM_ERROR_MSG = 'This platform is either missing IndexedDB or is known to have ' + 'an incomplete implementation. Offline persistence has been disabled.'; // The format of the LocalStorage key that stores zombied client is:\n//     firestore_zombie_<persistence_prefix>_<instance_key>\n\nconst ZOMBIED_CLIENTS_KEY_PREFIX = 'firestore_zombie';\n/**\r\n * The name of the main (and currently only) IndexedDB database. This name is\r\n * appended to the prefix provided to the IndexedDbPersistence constructor.\r\n */\n\nconst MAIN_DATABASE = 'main';\n/**\r\n * An IndexedDB-backed instance of Persistence. Data is stored persistently\r\n * across sessions.\r\n *\r\n * On Web only, the Firestore SDKs support shared access to its persistence\r\n * layer. This allows multiple browser tabs to read and write to IndexedDb and\r\n * to synchronize state even without network connectivity. Shared access is\r\n * currently optional and not enabled unless all clients invoke\r\n * `enablePersistence()` with `{synchronizeTabs:true}`.\r\n *\r\n * In multi-tab mode, if multiple clients are active at the same time, the SDK\r\n * will designate one client as the “primary client”. An effort is made to pick\r\n * a visible, network-connected and active client, and this client is\r\n * responsible for letting other clients know about its presence. The primary\r\n * client writes a unique client-generated identifier (the client ID) to\r\n * IndexedDb’s “owner” store every 4 seconds. If the primary client fails to\r\n * update this entry, another client can acquire the lease and take over as\r\n * primary.\r\n *\r\n * Some persistence operations in the SDK are designated as primary-client only\r\n * operations. This includes the acknowledgment of mutations and all updates of\r\n * remote documents. The effects of these operations are written to persistence\r\n * and then broadcast to other tabs via LocalStorage (see\r\n * `WebStorageSharedClientState`), which then refresh their state from\r\n * persistence.\r\n *\r\n * Similarly, the primary client listens to notifications sent by secondary\r\n * clients to discover persistence changes written by secondary clients, such as\r\n * the addition of new mutations and query targets.\r\n *\r\n * If multi-tab is not enabled and another tab already obtained the primary\r\n * lease, IndexedDbPersistence enters a failed state and all subsequent\r\n * operations will automatically fail.\r\n *\r\n * Additionally, there is an optimization so that when a tab is closed, the\r\n * primary lease is released immediately (this is especially important to make\r\n * sure that a refreshed tab is able to immediately re-acquire the primary\r\n * lease). Unfortunately, IndexedDB cannot be reliably used in window.unload\r\n * since it is an asynchronous API. So in addition to attempting to give up the\r\n * lease, the leaseholder writes its client ID to a \"zombiedClient\" entry in\r\n * LocalStorage which acts as an indicator that another tab should go ahead and\r\n * take the primary lease immediately regardless of the current lease timestamp.\r\n *\r\n * TODO(b/114226234): Remove `synchronizeTabs` section when multi-tab is no\r\n * longer optional.\r\n */\n\nclass IndexedDbPersistence {\n  constructor(\n  /**\r\n   * Whether to synchronize the in-memory state of multiple tabs and share\r\n   * access to local persistence.\r\n   */\n  allowTabSynchronization, persistenceKey, clientId, lruParams, queue, window, document, serializer, sequenceNumberSyncer,\n  /**\r\n   * If set to true, forcefully obtains database access. Existing tabs will\r\n   * no longer be able to access IndexedDB.\r\n   */\n  forceOwningTab, schemaVersion = SCHEMA_VERSION) {\n    this.allowTabSynchronization = allowTabSynchronization;\n    this.persistenceKey = persistenceKey;\n    this.clientId = clientId;\n    this.queue = queue;\n    this.window = window;\n    this.document = document;\n    this.sequenceNumberSyncer = sequenceNumberSyncer;\n    this.forceOwningTab = forceOwningTab;\n    this.schemaVersion = schemaVersion;\n    this.listenSequence = null;\n    this._started = false;\n    this.isPrimary = false;\n    this.networkEnabled = true;\n    /** Our window.unload handler, if registered. */\n\n    this.windowUnloadHandler = null;\n    this.inForeground = false;\n    /** Our 'visibilitychange' listener if registered. */\n\n    this.documentVisibilityHandler = null;\n    /** The client metadata refresh task. */\n\n    this.clientMetadataRefresher = null;\n    /** The last time we garbage collected the client metadata object store. */\n\n    this.lastGarbageCollectionTime = Number.NEGATIVE_INFINITY;\n    /** A listener to notify on primary state changes. */\n\n    this.primaryStateListener = _ => Promise.resolve();\n\n    if (!IndexedDbPersistence.isAvailable()) {\n      throw new FirestoreError(Code.UNIMPLEMENTED, UNSUPPORTED_PLATFORM_ERROR_MSG);\n    }\n\n    this.referenceDelegate = new IndexedDbLruDelegateImpl(this, lruParams);\n    this.dbName = persistenceKey + MAIN_DATABASE;\n    this.serializer = new LocalSerializer(serializer);\n    this.simpleDb = new SimpleDb(this.dbName, this.schemaVersion, new SchemaConverter(this.serializer));\n    this.targetCache = new IndexedDbTargetCache(this.referenceDelegate, this.serializer);\n    this.remoteDocumentCache = newIndexedDbRemoteDocumentCache(this.serializer);\n    this.bundleCache = new IndexedDbBundleCache();\n\n    if (this.window && this.window.localStorage) {\n      this.webStorage = this.window.localStorage;\n    } else {\n      this.webStorage = null;\n\n      if (forceOwningTab === false) {\n        logError(LOG_TAG$c, 'LocalStorage is unavailable. As a result, persistence may not work ' + 'reliably. In particular enablePersistence() could fail immediately ' + 'after refreshing the page.');\n      }\n    }\n  }\n  /**\r\n   * Attempt to start IndexedDb persistence.\r\n   *\r\n   * @returns Whether persistence was enabled.\r\n   */\n\n\n  start() {\n    // NOTE: This is expected to fail sometimes (in the case of another tab\n    // already having the persistence lock), so it's the first thing we should\n    // do.\n    return this.updateClientMetadataAndTryBecomePrimary().then(() => {\n      if (!this.isPrimary && !this.allowTabSynchronization) {\n        // Fail `start()` if `synchronizeTabs` is disabled and we cannot\n        // obtain the primary lease.\n        throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\n      }\n\n      this.attachVisibilityHandler();\n      this.attachWindowUnloadHook();\n      this.scheduleClientMetadataAndPrimaryLeaseRefreshes();\n      return this.runTransaction('getHighestListenSequenceNumber', 'readonly', txn => this.targetCache.getHighestSequenceNumber(txn));\n    }).then(highestListenSequenceNumber => {\n      this.listenSequence = new ListenSequence(highestListenSequenceNumber, this.sequenceNumberSyncer);\n    }).then(() => {\n      this._started = true;\n    }).catch(reason => {\n      this.simpleDb && this.simpleDb.close();\n      return Promise.reject(reason);\n    });\n  }\n  /**\r\n   * Registers a listener that gets called when the primary state of the\r\n   * instance changes. Upon registering, this listener is invoked immediately\r\n   * with the current primary state.\r\n   *\r\n   * PORTING NOTE: This is only used for Web multi-tab.\r\n   */\n\n\n  setPrimaryStateListener(primaryStateListener) {\n    var _this7 = this;\n\n    this.primaryStateListener = /*#__PURE__*/function () {\n      var _ref4 = _asyncToGenerator(function* (primaryState) {\n        if (_this7.started) {\n          return primaryStateListener(primaryState);\n        }\n      });\n\n      return function (_x2) {\n        return _ref4.apply(this, arguments);\n      };\n    }();\n\n    return primaryStateListener(this.isPrimary);\n  }\n  /**\r\n   * Registers a listener that gets called when the database receives a\r\n   * version change event indicating that it has deleted.\r\n   *\r\n   * PORTING NOTE: This is only used for Web multi-tab.\r\n   */\n\n\n  setDatabaseDeletedListener(databaseDeletedListener) {\n    this.simpleDb.setVersionChangeListener( /*#__PURE__*/function () {\n      var _ref5 = _asyncToGenerator(function* (event) {\n        // Check if an attempt is made to delete IndexedDB.\n        if (event.newVersion === null) {\n          yield databaseDeletedListener();\n        }\n      });\n\n      return function (_x3) {\n        return _ref5.apply(this, arguments);\n      };\n    }());\n  }\n  /**\r\n   * Adjusts the current network state in the client's metadata, potentially\r\n   * affecting the primary lease.\r\n   *\r\n   * PORTING NOTE: This is only used for Web multi-tab.\r\n   */\n\n\n  setNetworkEnabled(networkEnabled) {\n    var _this8 = this;\n\n    if (this.networkEnabled !== networkEnabled) {\n      this.networkEnabled = networkEnabled; // Schedule a primary lease refresh for immediate execution. The eventual\n      // lease update will be propagated via `primaryStateListener`.\n\n      this.queue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n        if (_this8.started) {\n          yield _this8.updateClientMetadataAndTryBecomePrimary();\n        }\n      }));\n    }\n  }\n  /**\r\n   * Updates the client metadata in IndexedDb and attempts to either obtain or\r\n   * extend the primary lease for the local client. Asynchronously notifies the\r\n   * primary state listener if the client either newly obtained or released its\r\n   * primary lease.\r\n   */\n\n\n  updateClientMetadataAndTryBecomePrimary() {\n    return this.runTransaction('updateClientMetadataAndTryBecomePrimary', 'readwrite', txn => {\n      const metadataStore = clientMetadataStore(txn);\n      return metadataStore.put({\n        clientId: this.clientId,\n        updateTimeMs: Date.now(),\n        networkEnabled: this.networkEnabled,\n        inForeground: this.inForeground\n      }).next(() => {\n        if (this.isPrimary) {\n          return this.verifyPrimaryLease(txn).next(success => {\n            if (!success) {\n              this.isPrimary = false;\n              this.queue.enqueueRetryable(() => this.primaryStateListener(false));\n            }\n          });\n        }\n      }).next(() => this.canActAsPrimary(txn)).next(canActAsPrimary => {\n        if (this.isPrimary && !canActAsPrimary) {\n          return this.releasePrimaryLeaseIfHeld(txn).next(() => false);\n        } else if (canActAsPrimary) {\n          return this.acquireOrExtendPrimaryLease(txn).next(() => true);\n        } else {\n          return (\n            /* canActAsPrimary= */\n            false\n          );\n        }\n      });\n    }).catch(e => {\n      if (isIndexedDbTransactionError(e)) {\n        logDebug(LOG_TAG$c, 'Failed to extend owner lease: ', e); // Proceed with the existing state. Any subsequent access to\n        // IndexedDB will verify the lease.\n\n        return this.isPrimary;\n      }\n\n      if (!this.allowTabSynchronization) {\n        throw e;\n      }\n\n      logDebug(LOG_TAG$c, 'Releasing owner lease after error during lease refresh', e);\n      return (\n        /* isPrimary= */\n        false\n      );\n    }).then(isPrimary => {\n      if (this.isPrimary !== isPrimary) {\n        this.queue.enqueueRetryable(() => this.primaryStateListener(isPrimary));\n      }\n\n      this.isPrimary = isPrimary;\n    });\n  }\n\n  verifyPrimaryLease(txn) {\n    const store = primaryClientStore(txn);\n    return store.get(DbPrimaryClientKey).next(primaryClient => {\n      return PersistencePromise.resolve(this.isLocalClient(primaryClient));\n    });\n  }\n\n  removeClientMetadata(txn) {\n    const metadataStore = clientMetadataStore(txn);\n    return metadataStore.delete(this.clientId);\n  }\n  /**\r\n   * If the garbage collection threshold has passed, prunes the\r\n   * RemoteDocumentChanges and the ClientMetadata store based on the last update\r\n   * time of all clients.\r\n   */\n\n\n  maybeGarbageCollectMultiClientState() {\n    var _this9 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this9.isPrimary && !_this9.isWithinAge(_this9.lastGarbageCollectionTime, MAX_CLIENT_AGE_MS)) {\n        _this9.lastGarbageCollectionTime = Date.now();\n        const inactiveClients = yield _this9.runTransaction('maybeGarbageCollectMultiClientState', 'readwrite-primary', txn => {\n          const metadataStore = getStore(txn, DbClientMetadataStore);\n          return metadataStore.loadAll().next(existingClients => {\n            const active = _this9.filterActiveClients(existingClients, MAX_CLIENT_AGE_MS);\n\n            const inactive = existingClients.filter(client => active.indexOf(client) === -1); // Delete metadata for clients that are no longer considered active.\n\n            return PersistencePromise.forEach(inactive, inactiveClient => metadataStore.delete(inactiveClient.clientId)).next(() => inactive);\n          });\n        }).catch(() => {\n          // Ignore primary lease violations or any other type of error. The next\n          // primary will run `maybeGarbageCollectMultiClientState()` again.\n          // We don't use `ignoreIfPrimaryLeaseLoss()` since we don't want to depend\n          // on LocalStore.\n          return [];\n        }); // Delete potential leftover entries that may continue to mark the\n        // inactive clients as zombied in LocalStorage.\n        // Ideally we'd delete the IndexedDb and LocalStorage zombie entries for\n        // the client atomically, but we can't. So we opt to delete the IndexedDb\n        // entries first to avoid potentially reviving a zombied client.\n\n        if (_this9.webStorage) {\n          for (const inactiveClient of inactiveClients) {\n            _this9.webStorage.removeItem(_this9.zombiedClientLocalStorageKey(inactiveClient.clientId));\n          }\n        }\n      }\n    })();\n  }\n  /**\r\n   * Schedules a recurring timer to update the client metadata and to either\r\n   * extend or acquire the primary lease if the client is eligible.\r\n   */\n\n\n  scheduleClientMetadataAndPrimaryLeaseRefreshes() {\n    this.clientMetadataRefresher = this.queue.enqueueAfterDelay(\"client_metadata_refresh\"\n    /* TimerId.ClientMetadataRefresh */\n    , CLIENT_METADATA_REFRESH_INTERVAL_MS, () => {\n      return this.updateClientMetadataAndTryBecomePrimary().then(() => this.maybeGarbageCollectMultiClientState()).then(() => this.scheduleClientMetadataAndPrimaryLeaseRefreshes());\n    });\n  }\n  /** Checks whether `client` is the local client. */\n\n\n  isLocalClient(client) {\n    return client ? client.ownerId === this.clientId : false;\n  }\n  /**\r\n   * Evaluate the state of all active clients and determine whether the local\r\n   * client is or can act as the holder of the primary lease. Returns whether\r\n   * the client is eligible for the lease, but does not actually acquire it.\r\n   * May return 'false' even if there is no active leaseholder and another\r\n   * (foreground) client should become leaseholder instead.\r\n   */\n\n\n  canActAsPrimary(txn) {\n    if (this.forceOwningTab) {\n      return PersistencePromise.resolve(true);\n    }\n\n    const store = primaryClientStore(txn);\n    return store.get(DbPrimaryClientKey).next(currentPrimary => {\n      const currentLeaseIsValid = currentPrimary !== null && this.isWithinAge(currentPrimary.leaseTimestampMs, MAX_PRIMARY_ELIGIBLE_AGE_MS) && !this.isClientZombied(currentPrimary.ownerId); // A client is eligible for the primary lease if:\n      // - its network is enabled and the client's tab is in the foreground.\n      // - its network is enabled and no other client's tab is in the\n      //   foreground.\n      // - every clients network is disabled and the client's tab is in the\n      //   foreground.\n      // - every clients network is disabled and no other client's tab is in\n      //   the foreground.\n      // - the `forceOwningTab` setting was passed in.\n\n      if (currentLeaseIsValid) {\n        if (this.isLocalClient(currentPrimary) && this.networkEnabled) {\n          return true;\n        }\n\n        if (!this.isLocalClient(currentPrimary)) {\n          if (!currentPrimary.allowTabSynchronization) {\n            // Fail the `canActAsPrimary` check if the current leaseholder has\n            // not opted into multi-tab synchronization. If this happens at\n            // client startup, we reject the Promise returned by\n            // `enablePersistence()` and the user can continue to use Firestore\n            // with in-memory persistence.\n            // If this fails during a lease refresh, we will instead block the\n            // AsyncQueue from executing further operations. Note that this is\n            // acceptable since mixing & matching different `synchronizeTabs`\n            // settings is not supported.\n            //\n            // TODO(b/114226234): Remove this check when `synchronizeTabs` can\n            // no longer be turned off.\n            throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\n          }\n\n          return false;\n        }\n      }\n\n      if (this.networkEnabled && this.inForeground) {\n        return true;\n      }\n\n      return clientMetadataStore(txn).loadAll().next(existingClients => {\n        // Process all existing clients and determine whether at least one of\n        // them is better suited to obtain the primary lease.\n        const preferredCandidate = this.filterActiveClients(existingClients, MAX_PRIMARY_ELIGIBLE_AGE_MS).find(otherClient => {\n          if (this.clientId !== otherClient.clientId) {\n            const otherClientHasBetterNetworkState = !this.networkEnabled && otherClient.networkEnabled;\n            const otherClientHasBetterVisibility = !this.inForeground && otherClient.inForeground;\n            const otherClientHasSameNetworkState = this.networkEnabled === otherClient.networkEnabled;\n\n            if (otherClientHasBetterNetworkState || otherClientHasBetterVisibility && otherClientHasSameNetworkState) {\n              return true;\n            }\n          }\n\n          return false;\n        });\n        return preferredCandidate === undefined;\n      });\n    }).next(canActAsPrimary => {\n      if (this.isPrimary !== canActAsPrimary) {\n        logDebug(LOG_TAG$c, `Client ${canActAsPrimary ? 'is' : 'is not'} eligible for a primary lease.`);\n      }\n\n      return canActAsPrimary;\n    });\n  }\n\n  shutdown() {\n    var _this10 = this;\n\n    return _asyncToGenerator(function* () {\n      // The shutdown() operations are idempotent and can be called even when\n      // start() aborted (e.g. because it couldn't acquire the persistence lease).\n      _this10._started = false;\n\n      _this10.markClientZombied();\n\n      if (_this10.clientMetadataRefresher) {\n        _this10.clientMetadataRefresher.cancel();\n\n        _this10.clientMetadataRefresher = null;\n      }\n\n      _this10.detachVisibilityHandler();\n\n      _this10.detachWindowUnloadHook(); // Use `SimpleDb.runTransaction` directly to avoid failing if another tab\n      // has obtained the primary lease.\n\n\n      yield _this10.simpleDb.runTransaction('shutdown', 'readwrite', [DbPrimaryClientStore, DbClientMetadataStore], simpleDbTxn => {\n        const persistenceTransaction = new IndexedDbTransaction(simpleDbTxn, ListenSequence.INVALID);\n        return _this10.releasePrimaryLeaseIfHeld(persistenceTransaction).next(() => _this10.removeClientMetadata(persistenceTransaction));\n      });\n\n      _this10.simpleDb.close(); // Remove the entry marking the client as zombied from LocalStorage since\n      // we successfully deleted its metadata from IndexedDb.\n\n\n      _this10.removeClientZombiedEntry();\n    })();\n  }\n  /**\r\n   * Returns clients that are not zombied and have an updateTime within the\r\n   * provided threshold.\r\n   */\n\n\n  filterActiveClients(clients, activityThresholdMs) {\n    return clients.filter(client => this.isWithinAge(client.updateTimeMs, activityThresholdMs) && !this.isClientZombied(client.clientId));\n  }\n  /**\r\n   * Returns the IDs of the clients that are currently active. If multi-tab\r\n   * is not supported, returns an array that only contains the local client's\r\n   * ID.\r\n   *\r\n   * PORTING NOTE: This is only used for Web multi-tab.\r\n   */\n\n\n  getActiveClients() {\n    return this.runTransaction('getActiveClients', 'readonly', txn => {\n      return clientMetadataStore(txn).loadAll().next(clients => this.filterActiveClients(clients, MAX_CLIENT_AGE_MS).map(clientMetadata => clientMetadata.clientId));\n    });\n  }\n\n  get started() {\n    return this._started;\n  }\n\n  getMutationQueue(user, indexManager) {\n    return IndexedDbMutationQueue.forUser(user, this.serializer, indexManager, this.referenceDelegate);\n  }\n\n  getTargetCache() {\n    return this.targetCache;\n  }\n\n  getRemoteDocumentCache() {\n    return this.remoteDocumentCache;\n  }\n\n  getIndexManager(user) {\n    return new IndexedDbIndexManager(user, this.serializer.remoteSerializer.databaseId);\n  }\n\n  getDocumentOverlayCache(user) {\n    return IndexedDbDocumentOverlayCache.forUser(this.serializer, user);\n  }\n\n  getBundleCache() {\n    return this.bundleCache;\n  }\n\n  runTransaction(action, mode, transactionOperation) {\n    logDebug(LOG_TAG$c, 'Starting transaction:', action);\n    const simpleDbMode = mode === 'readonly' ? 'readonly' : 'readwrite';\n    const objectStores = getObjectStores(this.schemaVersion);\n    let persistenceTransaction; // Do all transactions as readwrite against all object stores, since we\n    // are the only reader/writer.\n\n    return this.simpleDb.runTransaction(action, simpleDbMode, objectStores, simpleDbTxn => {\n      persistenceTransaction = new IndexedDbTransaction(simpleDbTxn, this.listenSequence ? this.listenSequence.next() : ListenSequence.INVALID);\n\n      if (mode === 'readwrite-primary') {\n        // While we merely verify that we have (or can acquire) the lease\n        // immediately, we wait to extend the primary lease until after\n        // executing transactionOperation(). This ensures that even if the\n        // transactionOperation takes a long time, we'll use a recent\n        // leaseTimestampMs in the extended (or newly acquired) lease.\n        return this.verifyPrimaryLease(persistenceTransaction).next(holdsPrimaryLease => {\n          if (holdsPrimaryLease) {\n            return (\n              /* holdsPrimaryLease= */\n              true\n            );\n          }\n\n          return this.canActAsPrimary(persistenceTransaction);\n        }).next(holdsPrimaryLease => {\n          if (!holdsPrimaryLease) {\n            logError(`Failed to obtain primary lease for action '${action}'.`);\n            this.isPrimary = false;\n            this.queue.enqueueRetryable(() => this.primaryStateListener(false));\n            throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_LOST_ERROR_MSG);\n          }\n\n          return transactionOperation(persistenceTransaction);\n        }).next(result => {\n          return this.acquireOrExtendPrimaryLease(persistenceTransaction).next(() => result);\n        });\n      } else {\n        return this.verifyAllowTabSynchronization(persistenceTransaction).next(() => transactionOperation(persistenceTransaction));\n      }\n    }).then(result => {\n      persistenceTransaction.raiseOnCommittedEvent();\n      return result;\n    });\n  }\n  /**\r\n   * Verifies that the current tab is the primary leaseholder or alternatively\r\n   * that the leaseholder has opted into multi-tab synchronization.\r\n   */\n  // TODO(b/114226234): Remove this check when `synchronizeTabs` can no longer\n  // be turned off.\n\n\n  verifyAllowTabSynchronization(txn) {\n    const store = primaryClientStore(txn);\n    return store.get(DbPrimaryClientKey).next(currentPrimary => {\n      const currentLeaseIsValid = currentPrimary !== null && this.isWithinAge(currentPrimary.leaseTimestampMs, MAX_PRIMARY_ELIGIBLE_AGE_MS) && !this.isClientZombied(currentPrimary.ownerId);\n\n      if (currentLeaseIsValid && !this.isLocalClient(currentPrimary)) {\n        if (!this.forceOwningTab && (!this.allowTabSynchronization || !currentPrimary.allowTabSynchronization)) {\n          throw new FirestoreError(Code.FAILED_PRECONDITION, PRIMARY_LEASE_EXCLUSIVE_ERROR_MSG);\n        }\n      }\n    });\n  }\n  /**\r\n   * Obtains or extends the new primary lease for the local client. This\r\n   * method does not verify that the client is eligible for this lease.\r\n   */\n\n\n  acquireOrExtendPrimaryLease(txn) {\n    const newPrimary = {\n      ownerId: this.clientId,\n      allowTabSynchronization: this.allowTabSynchronization,\n      leaseTimestampMs: Date.now()\n    };\n    return primaryClientStore(txn).put(DbPrimaryClientKey, newPrimary);\n  }\n\n  static isAvailable() {\n    return SimpleDb.isAvailable();\n  }\n  /** Checks the primary lease and removes it if we are the current primary. */\n\n\n  releasePrimaryLeaseIfHeld(txn) {\n    const store = primaryClientStore(txn);\n    return store.get(DbPrimaryClientKey).next(primaryClient => {\n      if (this.isLocalClient(primaryClient)) {\n        logDebug(LOG_TAG$c, 'Releasing primary lease.');\n        return store.delete(DbPrimaryClientKey);\n      } else {\n        return PersistencePromise.resolve();\n      }\n    });\n  }\n  /** Verifies that `updateTimeMs` is within `maxAgeMs`. */\n\n\n  isWithinAge(updateTimeMs, maxAgeMs) {\n    const now = Date.now();\n    const minAcceptable = now - maxAgeMs;\n    const maxAcceptable = now;\n\n    if (updateTimeMs < minAcceptable) {\n      return false;\n    } else if (updateTimeMs > maxAcceptable) {\n      logError(`Detected an update time that is in the future: ${updateTimeMs} > ${maxAcceptable}`);\n      return false;\n    }\n\n    return true;\n  }\n\n  attachVisibilityHandler() {\n    if (this.document !== null && typeof this.document.addEventListener === 'function') {\n      this.documentVisibilityHandler = () => {\n        this.queue.enqueueAndForget(() => {\n          this.inForeground = this.document.visibilityState === 'visible';\n          return this.updateClientMetadataAndTryBecomePrimary();\n        });\n      };\n\n      this.document.addEventListener('visibilitychange', this.documentVisibilityHandler);\n      this.inForeground = this.document.visibilityState === 'visible';\n    }\n  }\n\n  detachVisibilityHandler() {\n    if (this.documentVisibilityHandler) {\n      this.document.removeEventListener('visibilitychange', this.documentVisibilityHandler);\n      this.documentVisibilityHandler = null;\n    }\n  }\n  /**\r\n   * Attaches a window.unload handler that will synchronously write our\r\n   * clientId to a \"zombie client id\" location in LocalStorage. This can be used\r\n   * by tabs trying to acquire the primary lease to determine that the lease\r\n   * is no longer valid even if the timestamp is recent. This is particularly\r\n   * important for the refresh case (so the tab correctly re-acquires the\r\n   * primary lease). LocalStorage is used for this rather than IndexedDb because\r\n   * it is a synchronous API and so can be used reliably from  an unload\r\n   * handler.\r\n   */\n\n\n  attachWindowUnloadHook() {\n    var _a;\n\n    if (typeof ((_a = this.window) === null || _a === void 0 ? void 0 : _a.addEventListener) === 'function') {\n      this.windowUnloadHandler = () => {\n        // Note: In theory, this should be scheduled on the AsyncQueue since it\n        // accesses internal state. We execute this code directly during shutdown\n        // to make sure it gets a chance to run.\n        this.markClientZombied();\n        const safariIndexdbBugVersionRegex = /(?:Version|Mobile)\\/1[456]/;\n\n        if (isSafari() && (navigator.appVersion.match(safariIndexdbBugVersionRegex) || navigator.userAgent.match(safariIndexdbBugVersionRegex))) {\n          // On Safari 14, 15, and 16, we do not run any cleanup actions as it might\n          // trigger a bug that prevents Safari from re-opening IndexedDB during\n          // the next page load.\n          // See https://bugs.webkit.org/show_bug.cgi?id=226547\n          this.queue.enterRestrictedMode(\n          /* purgeExistingTasks= */\n          true);\n        }\n\n        this.queue.enqueueAndForget(() => {\n          // Attempt graceful shutdown (including releasing our primary lease),\n          // but there's no guarantee it will complete.\n          return this.shutdown();\n        });\n      };\n\n      this.window.addEventListener('pagehide', this.windowUnloadHandler);\n    }\n  }\n\n  detachWindowUnloadHook() {\n    if (this.windowUnloadHandler) {\n      this.window.removeEventListener('pagehide', this.windowUnloadHandler);\n      this.windowUnloadHandler = null;\n    }\n  }\n  /**\r\n   * Returns whether a client is \"zombied\" based on its LocalStorage entry.\r\n   * Clients become zombied when their tab closes without running all of the\r\n   * cleanup logic in `shutdown()`.\r\n   */\n\n\n  isClientZombied(clientId) {\n    var _a;\n\n    try {\n      const isZombied = ((_a = this.webStorage) === null || _a === void 0 ? void 0 : _a.getItem(this.zombiedClientLocalStorageKey(clientId))) !== null;\n      logDebug(LOG_TAG$c, `Client '${clientId}' ${isZombied ? 'is' : 'is not'} zombied in LocalStorage`);\n      return isZombied;\n    } catch (e) {\n      // Gracefully handle if LocalStorage isn't working.\n      logError(LOG_TAG$c, 'Failed to get zombied client id.', e);\n      return false;\n    }\n  }\n  /**\r\n   * Record client as zombied (a client that had its tab closed). Zombied\r\n   * clients are ignored during primary tab selection.\r\n   */\n\n\n  markClientZombied() {\n    if (!this.webStorage) {\n      return;\n    }\n\n    try {\n      this.webStorage.setItem(this.zombiedClientLocalStorageKey(this.clientId), String(Date.now()));\n    } catch (e) {\n      // Gracefully handle if LocalStorage isn't available / working.\n      logError('Failed to set zombie client id.', e);\n    }\n  }\n  /** Removes the zombied client entry if it exists. */\n\n\n  removeClientZombiedEntry() {\n    if (!this.webStorage) {\n      return;\n    }\n\n    try {\n      this.webStorage.removeItem(this.zombiedClientLocalStorageKey(this.clientId));\n    } catch (e) {// Ignore\n    }\n  }\n\n  zombiedClientLocalStorageKey(clientId) {\n    return `${ZOMBIED_CLIENTS_KEY_PREFIX}_${this.persistenceKey}_${clientId}`;\n  }\n\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the primary client object store.\r\n */\n\n\nfunction primaryClientStore(txn) {\n  return getStore(txn, DbPrimaryClientStore);\n}\n/**\r\n * Helper to get a typed SimpleDbStore for the client metadata object store.\r\n */\n\n\nfunction clientMetadataStore(txn) {\n  return getStore(txn, DbClientMetadataStore);\n}\n/**\r\n * Generates a string used as a prefix when storing data in IndexedDB and\r\n * LocalStorage.\r\n */\n\n\nfunction indexedDbStoragePrefix(databaseId, persistenceKey) {\n  // Use two different prefix formats:\n  //\n  //   * firestore / persistenceKey / projectID . databaseID / ...\n  //   * firestore / persistenceKey / projectID / ...\n  //\n  // projectIDs are DNS-compatible names and cannot contain dots\n  // so there's no danger of collisions.\n  let database = databaseId.projectId;\n\n  if (!databaseId.isDefaultDatabase) {\n    database += '.' + databaseId.database;\n  }\n\n  return 'firestore/' + persistenceKey + '/' + database + '/';\n}\n\nfunction indexedDbClearPersistence(_x4) {\n  return _indexedDbClearPersistence.apply(this, arguments);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Compares two array for equality using comparator. The method computes the\r\n * intersection and invokes `onAdd` for every element that is in `after` but not\r\n * `before`. `onRemove` is invoked for every element in `before` but missing\r\n * from `after`.\r\n *\r\n * The method creates a copy of both `before` and `after` and runs in O(n log\r\n * n), where n is the size of the two lists.\r\n *\r\n * @param before - The elements that exist in the original array.\r\n * @param after - The elements to diff against the original array.\r\n * @param comparator - The comparator for the elements in before and after.\r\n * @param onAdd - A function to invoke for every element that is part of `\r\n * after` but not `before`.\r\n * @param onRemove - A function to invoke for every element that is part of\r\n * `before` but not `after`.\r\n */\n\n\nfunction _indexedDbClearPersistence() {\n  _indexedDbClearPersistence = _asyncToGenerator(function* (persistenceKey) {\n    if (!SimpleDb.isAvailable()) {\n      return Promise.resolve();\n    }\n\n    const dbName = persistenceKey + MAIN_DATABASE;\n    yield SimpleDb.delete(dbName);\n  });\n  return _indexedDbClearPersistence.apply(this, arguments);\n}\n\nfunction diffArrays(before, after, comparator, onAdd, onRemove) {\n  before = [...before];\n  after = [...after];\n  before.sort(comparator);\n  after.sort(comparator);\n  const bLen = before.length;\n  const aLen = after.length;\n  let a = 0;\n  let b = 0;\n\n  while (a < aLen && b < bLen) {\n    const cmp = comparator(before[b], after[a]);\n\n    if (cmp < 0) {\n      // The element was removed if the next element in our ordered\n      // walkthrough is only in `before`.\n      onRemove(before[b++]);\n    } else if (cmp > 0) {\n      // The element was added if the next element in our ordered walkthrough\n      // is only in `after`.\n      onAdd(after[a++]);\n    } else {\n      a++;\n      b++;\n    }\n  }\n\n  while (a < aLen) {\n    onAdd(after[a++]);\n  }\n\n  while (b < bLen) {\n    onRemove(before[b++]);\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$b = 'LocalStore';\n/**\r\n * The maximum time to leave a resume token buffered without writing it out.\r\n * This value is arbitrary: it's long enough to avoid several writes\r\n * (possibly indefinitely if updates come more frequently than this) but\r\n * short enough that restarting after crashing will still have a pretty\r\n * recent resume token.\r\n */\n\nconst RESUME_TOKEN_MAX_AGE_MICROS = 5 * 60 * 1e6;\n/**\r\n * Implements `LocalStore` interface.\r\n *\r\n * Note: some field defined in this class might have public access level, but\r\n * the class is not exported so they are only accessible from this module.\r\n * This is useful to implement optional features (like bundles) in free\r\n * functions, such that they are tree-shakeable.\r\n */\n\nclass LocalStoreImpl {\n  constructor(\n  /** Manages our in-memory or durable persistence. */\n  persistence, queryEngine, initialUser, serializer) {\n    this.persistence = persistence;\n    this.queryEngine = queryEngine;\n    this.serializer = serializer;\n    /**\r\n     * Maps a targetID to data about its target.\r\n     *\r\n     * PORTING NOTE: We are using an immutable data structure on Web to make re-runs\r\n     * of `applyRemoteEvent()` idempotent.\r\n     */\n\n    this.targetDataByTarget = new SortedMap(primitiveComparator);\n    /** Maps a target to its targetID. */\n    // TODO(wuandy): Evaluate if TargetId can be part of Target.\n\n    this.targetIdByTarget = new ObjectMap(t => canonifyTarget(t), targetEquals);\n    /**\r\n     * A per collection group index of the last read time processed by\r\n     * `getNewDocumentChanges()`.\r\n     *\r\n     * PORTING NOTE: This is only used for multi-tab synchronization.\r\n     */\n\n    this.collectionGroupReadTime = new Map();\n    this.remoteDocuments = persistence.getRemoteDocumentCache();\n    this.targetCache = persistence.getTargetCache();\n    this.bundleCache = persistence.getBundleCache();\n    this.initializeUserComponents(initialUser);\n  }\n\n  initializeUserComponents(user) {\n    // TODO(indexing): Add spec tests that test these components change after a\n    // user change\n    this.documentOverlayCache = this.persistence.getDocumentOverlayCache(user);\n    this.indexManager = this.persistence.getIndexManager(user);\n    this.mutationQueue = this.persistence.getMutationQueue(user, this.indexManager);\n    this.localDocuments = new LocalDocumentsView(this.remoteDocuments, this.mutationQueue, this.documentOverlayCache, this.indexManager);\n    this.remoteDocuments.setIndexManager(this.indexManager);\n    this.queryEngine.initialize(this.localDocuments, this.indexManager);\n  }\n\n  collectGarbage(garbageCollector) {\n    return this.persistence.runTransaction('Collect garbage', 'readwrite-primary', txn => garbageCollector.collect(txn, this.targetDataByTarget));\n  }\n\n}\n\nfunction newLocalStore(\n/** Manages our in-memory or durable persistence. */\npersistence, queryEngine, initialUser, serializer) {\n  return new LocalStoreImpl(persistence, queryEngine, initialUser, serializer);\n}\n/**\r\n * Tells the LocalStore that the currently authenticated user has changed.\r\n *\r\n * In response the local store switches the mutation queue to the new user and\r\n * returns any resulting document changes.\r\n */\n// PORTING NOTE: Android and iOS only return the documents affected by the\n// change.\n\n\nfunction localStoreHandleUserChange(_x5, _x6) {\n  return _localStoreHandleUserChange.apply(this, arguments);\n}\n/* Accepts locally generated Mutations and commit them to storage. */\n\n\nfunction _localStoreHandleUserChange() {\n  _localStoreHandleUserChange = _asyncToGenerator(function* (localStore, user) {\n    const localStoreImpl = debugCast(localStore);\n    const result = yield localStoreImpl.persistence.runTransaction('Handle user change', 'readonly', txn => {\n      // Swap out the mutation queue, grabbing the pending mutation batches\n      // before and after.\n      let oldBatches;\n      return localStoreImpl.mutationQueue.getAllMutationBatches(txn).next(promisedOldBatches => {\n        oldBatches = promisedOldBatches;\n        localStoreImpl.initializeUserComponents(user);\n        return localStoreImpl.mutationQueue.getAllMutationBatches(txn);\n      }).next(newBatches => {\n        const removedBatchIds = [];\n        const addedBatchIds = []; // Union the old/new changed keys.\n\n        let changedKeys = documentKeySet();\n\n        for (const batch of oldBatches) {\n          removedBatchIds.push(batch.batchId);\n\n          for (const mutation of batch.mutations) {\n            changedKeys = changedKeys.add(mutation.key);\n          }\n        }\n\n        for (const batch of newBatches) {\n          addedBatchIds.push(batch.batchId);\n\n          for (const mutation of batch.mutations) {\n            changedKeys = changedKeys.add(mutation.key);\n          }\n        } // Return the set of all (potentially) changed documents and the list\n        // of mutation batch IDs that were affected by change.\n\n\n        return localStoreImpl.localDocuments.getDocuments(txn, changedKeys).next(affectedDocuments => {\n          return {\n            affectedDocuments,\n            removedBatchIds,\n            addedBatchIds\n          };\n        });\n      });\n    });\n    return result;\n  });\n  return _localStoreHandleUserChange.apply(this, arguments);\n}\n\nfunction localStoreWriteLocally(localStore, mutations) {\n  const localStoreImpl = debugCast(localStore);\n  const localWriteTime = Timestamp.now();\n  const keys = mutations.reduce((keys, m) => keys.add(m.key), documentKeySet());\n  let overlayedDocuments;\n  let mutationBatch;\n  return localStoreImpl.persistence.runTransaction('Locally write mutations', 'readwrite', txn => {\n    // Figure out which keys do not have a remote version in the cache, this\n    // is needed to create the right overlay mutation: if no remote version\n    // presents, we do not need to create overlays as patch mutations.\n    // TODO(Overlay): Is there a better way to determine this? Using the\n    //  document version does not work because local mutations set them back\n    //  to 0.\n    let remoteDocs = mutableDocumentMap();\n    let docsWithoutRemoteVersion = documentKeySet();\n    return localStoreImpl.remoteDocuments.getEntries(txn, keys).next(docs => {\n      remoteDocs = docs;\n      remoteDocs.forEach((key, doc) => {\n        if (!doc.isValidDocument()) {\n          docsWithoutRemoteVersion = docsWithoutRemoteVersion.add(key);\n        }\n      });\n    }).next(() => {\n      // Load and apply all existing mutations. This lets us compute the\n      // current base state for all non-idempotent transforms before applying\n      // any additional user-provided writes.\n      return localStoreImpl.localDocuments.getOverlayedDocuments(txn, remoteDocs);\n    }).next(docs => {\n      overlayedDocuments = docs; // For non-idempotent mutations (such as `FieldValue.increment()`),\n      // we record the base state in a separate patch mutation. This is\n      // later used to guarantee consistent values and prevents flicker\n      // even if the backend sends us an update that already includes our\n      // transform.\n\n      const baseMutations = [];\n\n      for (const mutation of mutations) {\n        const baseValue = mutationExtractBaseValue(mutation, overlayedDocuments.get(mutation.key).overlayedDocument);\n\n        if (baseValue != null) {\n          // NOTE: The base state should only be applied if there's some\n          // existing document to override, so use a Precondition of\n          // exists=true\n          baseMutations.push(new PatchMutation(mutation.key, baseValue, extractFieldMask(baseValue.value.mapValue), Precondition.exists(true)));\n        }\n      }\n\n      return localStoreImpl.mutationQueue.addMutationBatch(txn, localWriteTime, baseMutations, mutations);\n    }).next(batch => {\n      mutationBatch = batch;\n      const overlays = batch.applyToLocalDocumentSet(overlayedDocuments, docsWithoutRemoteVersion);\n      return localStoreImpl.documentOverlayCache.saveOverlays(txn, batch.batchId, overlays);\n    });\n  }).then(() => ({\n    batchId: mutationBatch.batchId,\n    changes: convertOverlayedDocumentMapToDocumentMap(overlayedDocuments)\n  }));\n}\n/**\r\n * Acknowledges the given batch.\r\n *\r\n * On the happy path when a batch is acknowledged, the local store will\r\n *\r\n *  + remove the batch from the mutation queue;\r\n *  + apply the changes to the remote document cache;\r\n *  + recalculate the latency compensated view implied by those changes (there\r\n *    may be mutations in the queue that affect the documents but haven't been\r\n *    acknowledged yet); and\r\n *  + give the changed documents back the sync engine\r\n *\r\n * @returns The resulting (modified) documents.\r\n */\n\n\nfunction localStoreAcknowledgeBatch(localStore, batchResult) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Acknowledge batch', 'readwrite-primary', txn => {\n    const affected = batchResult.batch.keys();\n    const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\n      trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\n\n    });\n    return applyWriteToRemoteDocuments(localStoreImpl, txn, batchResult, documentBuffer).next(() => documentBuffer.apply(txn)).next(() => localStoreImpl.mutationQueue.performConsistencyCheck(txn)).next(() => localStoreImpl.documentOverlayCache.removeOverlaysForBatchId(txn, affected, batchResult.batch.batchId)).next(() => localStoreImpl.localDocuments.recalculateAndSaveOverlaysForDocumentKeys(txn, getKeysWithTransformResults(batchResult))).next(() => localStoreImpl.localDocuments.getDocuments(txn, affected));\n  });\n}\n\nfunction getKeysWithTransformResults(batchResult) {\n  let result = documentKeySet();\n\n  for (let i = 0; i < batchResult.mutationResults.length; ++i) {\n    const mutationResult = batchResult.mutationResults[i];\n\n    if (mutationResult.transformResults.length > 0) {\n      result = result.add(batchResult.batch.mutations[i].key);\n    }\n  }\n\n  return result;\n}\n/**\r\n * Removes mutations from the MutationQueue for the specified batch;\r\n * LocalDocuments will be recalculated.\r\n *\r\n * @returns The resulting modified documents.\r\n */\n\n\nfunction localStoreRejectBatch(localStore, batchId) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Reject batch', 'readwrite-primary', txn => {\n    let affectedKeys;\n    return localStoreImpl.mutationQueue.lookupMutationBatch(txn, batchId).next(batch => {\n      hardAssert(batch !== null);\n      affectedKeys = batch.keys();\n      return localStoreImpl.mutationQueue.removeMutationBatch(txn, batch);\n    }).next(() => localStoreImpl.mutationQueue.performConsistencyCheck(txn)).next(() => localStoreImpl.documentOverlayCache.removeOverlaysForBatchId(txn, affectedKeys, batchId)).next(() => localStoreImpl.localDocuments.recalculateAndSaveOverlaysForDocumentKeys(txn, affectedKeys)).next(() => localStoreImpl.localDocuments.getDocuments(txn, affectedKeys));\n  });\n}\n/**\r\n * Returns the largest (latest) batch id in mutation queue that is pending\r\n * server response.\r\n *\r\n * Returns `BATCHID_UNKNOWN` if the queue is empty.\r\n */\n\n\nfunction localStoreGetHighestUnacknowledgedBatchId(localStore) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Get highest unacknowledged batch id', 'readonly', txn => localStoreImpl.mutationQueue.getHighestUnacknowledgedBatchId(txn));\n}\n/**\r\n * Returns the last consistent snapshot processed (used by the RemoteStore to\r\n * determine whether to buffer incoming snapshots from the backend).\r\n */\n\n\nfunction localStoreGetLastRemoteSnapshotVersion(localStore) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Get last remote snapshot version', 'readonly', txn => localStoreImpl.targetCache.getLastRemoteSnapshotVersion(txn));\n}\n/**\r\n * Updates the \"ground-state\" (remote) documents. We assume that the remote\r\n * event reflects any write batches that have been acknowledged or rejected\r\n * (i.e. we do not re-apply local mutations to updates from this event).\r\n *\r\n * LocalDocuments are re-calculated if there are remaining mutations in the\r\n * queue.\r\n */\n\n\nfunction localStoreApplyRemoteEventToLocalCache(localStore, remoteEvent) {\n  const localStoreImpl = debugCast(localStore);\n  const remoteVersion = remoteEvent.snapshotVersion;\n  let newTargetDataByTargetMap = localStoreImpl.targetDataByTarget;\n  return localStoreImpl.persistence.runTransaction('Apply remote event', 'readwrite-primary', txn => {\n    const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\n      trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\n\n    }); // Reset newTargetDataByTargetMap in case this transaction gets re-run.\n\n    newTargetDataByTargetMap = localStoreImpl.targetDataByTarget;\n    const promises = [];\n    remoteEvent.targetChanges.forEach((change, targetId) => {\n      const oldTargetData = newTargetDataByTargetMap.get(targetId);\n\n      if (!oldTargetData) {\n        return;\n      } // Only update the remote keys if the target is still active. This\n      // ensures that we can persist the updated target data along with\n      // the updated assignment.\n\n\n      promises.push(localStoreImpl.targetCache.removeMatchingKeys(txn, change.removedDocuments, targetId).next(() => {\n        return localStoreImpl.targetCache.addMatchingKeys(txn, change.addedDocuments, targetId);\n      }));\n      let newTargetData = oldTargetData.withSequenceNumber(txn.currentSequenceNumber);\n\n      if (remoteEvent.targetMismatches.get(targetId) !== null) {\n        newTargetData = newTargetData.withResumeToken(ByteString.EMPTY_BYTE_STRING, SnapshotVersion.min()).withLastLimboFreeSnapshotVersion(SnapshotVersion.min());\n      } else if (change.resumeToken.approximateByteSize() > 0) {\n        newTargetData = newTargetData.withResumeToken(change.resumeToken, remoteVersion);\n      }\n\n      newTargetDataByTargetMap = newTargetDataByTargetMap.insert(targetId, newTargetData); // Update the target data if there are target changes (or if\n      // sufficient time has passed since the last update).\n\n      if (shouldPersistTargetData(oldTargetData, newTargetData, change)) {\n        promises.push(localStoreImpl.targetCache.updateTargetData(txn, newTargetData));\n      }\n    });\n    let changedDocs = mutableDocumentMap();\n    let existenceChangedKeys = documentKeySet();\n    remoteEvent.documentUpdates.forEach(key => {\n      if (remoteEvent.resolvedLimboDocuments.has(key)) {\n        promises.push(localStoreImpl.persistence.referenceDelegate.updateLimboDocument(txn, key));\n      }\n    }); // Each loop iteration only affects its \"own\" doc, so it's safe to get all\n    // the remote documents in advance in a single call.\n\n    promises.push(populateDocumentChangeBuffer(txn, documentBuffer, remoteEvent.documentUpdates).next(result => {\n      changedDocs = result.changedDocuments;\n      existenceChangedKeys = result.existenceChangedKeys;\n    })); // HACK: The only reason we allow a null snapshot version is so that we\n    // can synthesize remote events when we get permission denied errors while\n    // trying to resolve the state of a locally cached document that is in\n    // limbo.\n\n    if (!remoteVersion.isEqual(SnapshotVersion.min())) {\n      const updateRemoteVersion = localStoreImpl.targetCache.getLastRemoteSnapshotVersion(txn).next(lastRemoteSnapshotVersion => {\n        return localStoreImpl.targetCache.setTargetsMetadata(txn, txn.currentSequenceNumber, remoteVersion);\n      });\n      promises.push(updateRemoteVersion);\n    }\n\n    return PersistencePromise.waitFor(promises).next(() => documentBuffer.apply(txn)).next(() => localStoreImpl.localDocuments.getLocalViewOfDocuments(txn, changedDocs, existenceChangedKeys)).next(() => changedDocs);\n  }).then(changedDocs => {\n    localStoreImpl.targetDataByTarget = newTargetDataByTargetMap;\n    return changedDocs;\n  });\n}\n/**\r\n * Populates document change buffer with documents from backend or a bundle.\r\n * Returns the document changes resulting from applying those documents, and\r\n * also a set of documents whose existence state are changed as a result.\r\n *\r\n * @param txn - Transaction to use to read existing documents from storage.\r\n * @param documentBuffer - Document buffer to collect the resulted changes to be\r\n *        applied to storage.\r\n * @param documents - Documents to be applied.\r\n */\n\n\nfunction populateDocumentChangeBuffer(txn, documentBuffer, documents) {\n  let updatedKeys = documentKeySet();\n  let existenceChangedKeys = documentKeySet();\n  documents.forEach(k => updatedKeys = updatedKeys.add(k));\n  return documentBuffer.getEntries(txn, updatedKeys).next(existingDocs => {\n    let changedDocuments = mutableDocumentMap();\n    documents.forEach((key, doc) => {\n      const existingDoc = existingDocs.get(key); // Check if see if there is a existence state change for this document.\n\n      if (doc.isFoundDocument() !== existingDoc.isFoundDocument()) {\n        existenceChangedKeys = existenceChangedKeys.add(key);\n      } // Note: The order of the steps below is important, since we want\n      // to ensure that rejected limbo resolutions (which fabricate\n      // NoDocuments with SnapshotVersion.min()) never add documents to\n      // cache.\n\n\n      if (doc.isNoDocument() && doc.version.isEqual(SnapshotVersion.min())) {\n        // NoDocuments with SnapshotVersion.min() are used in manufactured\n        // events. We remove these documents from cache since we lost\n        // access.\n        documentBuffer.removeEntry(key, doc.readTime);\n        changedDocuments = changedDocuments.insert(key, doc);\n      } else if (!existingDoc.isValidDocument() || doc.version.compareTo(existingDoc.version) > 0 || doc.version.compareTo(existingDoc.version) === 0 && existingDoc.hasPendingWrites) {\n        documentBuffer.addEntry(doc);\n        changedDocuments = changedDocuments.insert(key, doc);\n      } else {\n        logDebug(LOG_TAG$b, 'Ignoring outdated watch update for ', key, '. Current version:', existingDoc.version, ' Watch version:', doc.version);\n      }\n    });\n    return {\n      changedDocuments,\n      existenceChangedKeys\n    };\n  });\n}\n/**\r\n * Returns true if the newTargetData should be persisted during an update of\r\n * an active target. TargetData should always be persisted when a target is\r\n * being released and should not call this function.\r\n *\r\n * While the target is active, TargetData updates can be omitted when nothing\r\n * about the target has changed except metadata like the resume token or\r\n * snapshot version. Occasionally it's worth the extra write to prevent these\r\n * values from getting too stale after a crash, but this doesn't have to be\r\n * too frequent.\r\n */\n\n\nfunction shouldPersistTargetData(oldTargetData, newTargetData, change) {\n  // Always persist target data if we don't already have a resume token.\n  if (oldTargetData.resumeToken.approximateByteSize() === 0) {\n    return true;\n  } // Don't allow resume token changes to be buffered indefinitely. This\n  // allows us to be reasonably up-to-date after a crash and avoids needing\n  // to loop over all active queries on shutdown. Especially in the browser\n  // we may not get time to do anything interesting while the current tab is\n  // closing.\n\n\n  const timeDelta = newTargetData.snapshotVersion.toMicroseconds() - oldTargetData.snapshotVersion.toMicroseconds();\n\n  if (timeDelta >= RESUME_TOKEN_MAX_AGE_MICROS) {\n    return true;\n  } // Otherwise if the only thing that has changed about a target is its resume\n  // token it's not worth persisting. Note that the RemoteStore keeps an\n  // in-memory view of the currently active targets which includes the current\n  // resume token, so stream failure or user changes will still use an\n  // up-to-date resume token regardless of what we do here.\n\n\n  const changes = change.addedDocuments.size + change.modifiedDocuments.size + change.removedDocuments.size;\n  return changes > 0;\n}\n/**\r\n * Notifies local store of the changed views to locally pin documents.\r\n */\n\n\nfunction localStoreNotifyLocalViewChanges(_x7, _x8) {\n  return _localStoreNotifyLocalViewChanges.apply(this, arguments);\n}\n/**\r\n * Gets the mutation batch after the passed in batchId in the mutation queue\r\n * or null if empty.\r\n * @param afterBatchId - If provided, the batch to search after.\r\n * @returns The next mutation or null if there wasn't one.\r\n */\n\n\nfunction _localStoreNotifyLocalViewChanges() {\n  _localStoreNotifyLocalViewChanges = _asyncToGenerator(function* (localStore, viewChanges) {\n    const localStoreImpl = debugCast(localStore);\n\n    try {\n      yield localStoreImpl.persistence.runTransaction('notifyLocalViewChanges', 'readwrite', txn => {\n        return PersistencePromise.forEach(viewChanges, viewChange => {\n          return PersistencePromise.forEach(viewChange.addedKeys, key => localStoreImpl.persistence.referenceDelegate.addReference(txn, viewChange.targetId, key)).next(() => PersistencePromise.forEach(viewChange.removedKeys, key => localStoreImpl.persistence.referenceDelegate.removeReference(txn, viewChange.targetId, key)));\n        });\n      });\n    } catch (e) {\n      if (isIndexedDbTransactionError(e)) {\n        // If `notifyLocalViewChanges` fails, we did not advance the sequence\n        // number for the documents that were included in this transaction.\n        // This might trigger them to be deleted earlier than they otherwise\n        // would have, but it should not invalidate the integrity of the data.\n        logDebug(LOG_TAG$b, 'Failed to update sequence numbers: ' + e);\n      } else {\n        throw e;\n      }\n    }\n\n    for (const viewChange of viewChanges) {\n      const targetId = viewChange.targetId;\n\n      if (!viewChange.fromCache) {\n        const targetData = localStoreImpl.targetDataByTarget.get(targetId); // Advance the last limbo free snapshot version\n\n        const lastLimboFreeSnapshotVersion = targetData.snapshotVersion;\n        const updatedTargetData = targetData.withLastLimboFreeSnapshotVersion(lastLimboFreeSnapshotVersion);\n        localStoreImpl.targetDataByTarget = localStoreImpl.targetDataByTarget.insert(targetId, updatedTargetData); // TODO(b/272564316): Apply the optimization done on other platforms.\n        // This is a problem for web because saving the updated targetData from\n        // non-primary client conflicts with what primary client saved.\n      }\n    }\n  });\n  return _localStoreNotifyLocalViewChanges.apply(this, arguments);\n}\n\nfunction localStoreGetNextMutationBatch(localStore, afterBatchId) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Get next mutation batch', 'readonly', txn => {\n    if (afterBatchId === undefined) {\n      afterBatchId = BATCHID_UNKNOWN;\n    }\n\n    return localStoreImpl.mutationQueue.getNextMutationBatchAfterBatchId(txn, afterBatchId);\n  });\n}\n/**\r\n * Reads the current value of a Document with a given key or null if not\r\n * found - used for testing.\r\n */\n\n\nfunction localStoreReadDocument(localStore, key) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('read document', 'readonly', txn => localStoreImpl.localDocuments.getDocument(txn, key));\n}\n/**\r\n * Assigns the given target an internal ID so that its results can be pinned so\r\n * they don't get GC'd. A target must be allocated in the local store before\r\n * the store can be used to manage its view.\r\n *\r\n * Allocating an already allocated `Target` will return the existing `TargetData`\r\n * for that `Target`.\r\n */\n\n\nfunction localStoreAllocateTarget(localStore, target) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Allocate target', 'readwrite', txn => {\n    let targetData;\n    return localStoreImpl.targetCache.getTargetData(txn, target).next(cached => {\n      if (cached) {\n        // This target has been listened to previously, so reuse the\n        // previous targetID.\n        // TODO(mcg): freshen last accessed date?\n        targetData = cached;\n        return PersistencePromise.resolve(targetData);\n      } else {\n        return localStoreImpl.targetCache.allocateTargetId(txn).next(targetId => {\n          targetData = new TargetData(target, targetId, \"TargetPurposeListen\"\n          /* TargetPurpose.Listen */\n          , txn.currentSequenceNumber);\n          return localStoreImpl.targetCache.addTargetData(txn, targetData).next(() => targetData);\n        });\n      }\n    });\n  }).then(targetData => {\n    // If Multi-Tab is enabled, the existing target data may be newer than\n    // the in-memory data\n    const cachedTargetData = localStoreImpl.targetDataByTarget.get(targetData.targetId);\n\n    if (cachedTargetData === null || targetData.snapshotVersion.compareTo(cachedTargetData.snapshotVersion) > 0) {\n      localStoreImpl.targetDataByTarget = localStoreImpl.targetDataByTarget.insert(targetData.targetId, targetData);\n      localStoreImpl.targetIdByTarget.set(target, targetData.targetId);\n    }\n\n    return targetData;\n  });\n}\n/**\r\n * Returns the TargetData as seen by the LocalStore, including updates that may\r\n * have not yet been persisted to the TargetCache.\r\n */\n// Visible for testing.\n\n\nfunction localStoreGetTargetData(localStore, transaction, target) {\n  const localStoreImpl = debugCast(localStore);\n  const targetId = localStoreImpl.targetIdByTarget.get(target);\n\n  if (targetId !== undefined) {\n    return PersistencePromise.resolve(localStoreImpl.targetDataByTarget.get(targetId));\n  } else {\n    return localStoreImpl.targetCache.getTargetData(transaction, target);\n  }\n}\n/**\r\n * Unpins all the documents associated with the given target. If\r\n * `keepPersistedTargetData` is set to false and Eager GC enabled, the method\r\n * directly removes the associated target data from the target cache.\r\n *\r\n * Releasing a non-existing `Target` is a no-op.\r\n */\n// PORTING NOTE: `keepPersistedTargetData` is multi-tab only.\n\n\nfunction localStoreReleaseTarget(_x9, _x10, _x11) {\n  return _localStoreReleaseTarget.apply(this, arguments);\n}\n/**\r\n * Runs the specified query against the local store and returns the results,\r\n * potentially taking advantage of query data from previous executions (such\r\n * as the set of remote keys).\r\n *\r\n * @param usePreviousResults - Whether results from previous executions can\r\n * be used to optimize this query execution.\r\n */\n\n\nfunction _localStoreReleaseTarget() {\n  _localStoreReleaseTarget = _asyncToGenerator(function* (localStore, targetId, keepPersistedTargetData) {\n    const localStoreImpl = debugCast(localStore);\n    const targetData = localStoreImpl.targetDataByTarget.get(targetId);\n    const mode = keepPersistedTargetData ? 'readwrite' : 'readwrite-primary';\n\n    try {\n      if (!keepPersistedTargetData) {\n        yield localStoreImpl.persistence.runTransaction('Release target', mode, txn => {\n          return localStoreImpl.persistence.referenceDelegate.removeTarget(txn, targetData);\n        });\n      }\n    } catch (e) {\n      if (isIndexedDbTransactionError(e)) {\n        // All `releaseTarget` does is record the final metadata state for the\n        // target, but we've been recording this periodically during target\n        // activity. If we lose this write this could cause a very slight\n        // difference in the order of target deletion during GC, but we\n        // don't define exact LRU semantics so this is acceptable.\n        logDebug(LOG_TAG$b, `Failed to update sequence numbers for target ${targetId}: ${e}`);\n      } else {\n        throw e;\n      }\n    }\n\n    localStoreImpl.targetDataByTarget = localStoreImpl.targetDataByTarget.remove(targetId);\n    localStoreImpl.targetIdByTarget.delete(targetData.target);\n  });\n  return _localStoreReleaseTarget.apply(this, arguments);\n}\n\nfunction localStoreExecuteQuery(localStore, query, usePreviousResults) {\n  const localStoreImpl = debugCast(localStore);\n  let lastLimboFreeSnapshotVersion = SnapshotVersion.min();\n  let remoteKeys = documentKeySet();\n  return localStoreImpl.persistence.runTransaction('Execute query', 'readonly', txn => {\n    return localStoreGetTargetData(localStoreImpl, txn, queryToTarget(query)).next(targetData => {\n      if (targetData) {\n        lastLimboFreeSnapshotVersion = targetData.lastLimboFreeSnapshotVersion;\n        return localStoreImpl.targetCache.getMatchingKeysForTargetId(txn, targetData.targetId).next(result => {\n          remoteKeys = result;\n        });\n      }\n    }).next(() => localStoreImpl.queryEngine.getDocumentsMatchingQuery(txn, query, usePreviousResults ? lastLimboFreeSnapshotVersion : SnapshotVersion.min(), usePreviousResults ? remoteKeys : documentKeySet())).next(documents => {\n      setMaxReadTime(localStoreImpl, queryCollectionGroup(query), documents);\n      return {\n        documents,\n        remoteKeys\n      };\n    });\n  });\n}\n\nfunction applyWriteToRemoteDocuments(localStoreImpl, txn, batchResult, documentBuffer) {\n  const batch = batchResult.batch;\n  const docKeys = batch.keys();\n  let promiseChain = PersistencePromise.resolve();\n  docKeys.forEach(docKey => {\n    promiseChain = promiseChain.next(() => documentBuffer.getEntry(txn, docKey)).next(doc => {\n      const ackVersion = batchResult.docVersions.get(docKey);\n      hardAssert(ackVersion !== null);\n\n      if (doc.version.compareTo(ackVersion) < 0) {\n        batch.applyToRemoteDocument(doc, batchResult);\n\n        if (doc.isValidDocument()) {\n          // We use the commitVersion as the readTime rather than the\n          // document's updateTime since the updateTime is not advanced\n          // for updates that do not modify the underlying document.\n          doc.setReadTime(batchResult.commitVersion);\n          documentBuffer.addEntry(doc);\n        }\n      }\n    });\n  });\n  return promiseChain.next(() => localStoreImpl.mutationQueue.removeMutationBatch(txn, batch));\n}\n/** Returns the local view of the documents affected by a mutation batch. */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction localStoreLookupMutationDocuments(localStore, batchId) {\n  const localStoreImpl = debugCast(localStore);\n  const mutationQueueImpl = debugCast(localStoreImpl.mutationQueue);\n  return localStoreImpl.persistence.runTransaction('Lookup mutation documents', 'readonly', txn => {\n    return mutationQueueImpl.lookupMutationKeys(txn, batchId).next(keys => {\n      if (keys) {\n        return localStoreImpl.localDocuments.getDocuments(txn, keys);\n      } else {\n        return PersistencePromise.resolve(null);\n      }\n    });\n  });\n} // PORTING NOTE: Multi-Tab only.\n\n\nfunction localStoreRemoveCachedMutationBatchMetadata(localStore, batchId) {\n  const mutationQueueImpl = debugCast(debugCast(localStore, LocalStoreImpl).mutationQueue);\n  mutationQueueImpl.removeCachedMutationKeys(batchId);\n} // PORTING NOTE: Multi-Tab only.\n\n\nfunction localStoreGetActiveClients(localStore) {\n  const persistenceImpl = debugCast(debugCast(localStore, LocalStoreImpl).persistence);\n  return persistenceImpl.getActiveClients();\n} // PORTING NOTE: Multi-Tab only.\n\n\nfunction localStoreGetCachedTarget(localStore, targetId) {\n  const localStoreImpl = debugCast(localStore);\n  const targetCacheImpl = debugCast(localStoreImpl.targetCache);\n  const cachedTargetData = localStoreImpl.targetDataByTarget.get(targetId);\n\n  if (cachedTargetData) {\n    return Promise.resolve(cachedTargetData.target);\n  } else {\n    return localStoreImpl.persistence.runTransaction('Get target data', 'readonly', txn => {\n      return targetCacheImpl.getTargetDataForTarget(txn, targetId).next(targetData => targetData ? targetData.target : null);\n    });\n  }\n}\n/**\r\n * Returns the set of documents that have been updated since the last call.\r\n * If this is the first call, returns the set of changes since client\r\n * initialization. Further invocations will return document that have changed\r\n * since the prior call.\r\n */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction localStoreGetNewDocumentChanges(localStore, collectionGroup) {\n  const localStoreImpl = debugCast(localStore); // Get the current maximum read time for the collection. This should always\n  // exist, but to reduce the chance for regressions we default to\n  // SnapshotVersion.Min()\n  // TODO(indexing): Consider removing the default value.\n\n  const readTime = localStoreImpl.collectionGroupReadTime.get(collectionGroup) || SnapshotVersion.min();\n  return localStoreImpl.persistence.runTransaction('Get new document changes', 'readonly', txn => localStoreImpl.remoteDocuments.getAllFromCollectionGroup(txn, collectionGroup, newIndexOffsetSuccessorFromReadTime(readTime, INITIAL_LARGEST_BATCH_ID),\n  /* limit= */\n  Number.MAX_SAFE_INTEGER)).then(changedDocs => {\n    setMaxReadTime(localStoreImpl, collectionGroup, changedDocs);\n    return changedDocs;\n  });\n}\n/** Sets the collection group's maximum read time from the given documents. */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction setMaxReadTime(localStoreImpl, collectionGroup, changedDocs) {\n  let readTime = localStoreImpl.collectionGroupReadTime.get(collectionGroup) || SnapshotVersion.min();\n  changedDocs.forEach((_, doc) => {\n    if (doc.readTime.compareTo(readTime) > 0) {\n      readTime = doc.readTime;\n    }\n  });\n  localStoreImpl.collectionGroupReadTime.set(collectionGroup, readTime);\n}\n/**\r\n * Creates a new target using the given bundle name, which will be used to\r\n * hold the keys of all documents from the bundle in query-document mappings.\r\n * This ensures that the loaded documents do not get garbage collected\r\n * right away.\r\n */\n\n\nfunction umbrellaTarget(bundleName) {\n  // It is OK that the path used for the query is not valid, because this will\n  // not be read and queried.\n  return queryToTarget(newQueryForPath(ResourcePath.fromString(`__bundle__/docs/${bundleName}`)));\n}\n/**\r\n * Applies the documents from a bundle to the \"ground-state\" (remote)\r\n * documents.\r\n *\r\n * LocalDocuments are re-calculated if there are remaining mutations in the\r\n * queue.\r\n */\n\n\nfunction localStoreApplyBundledDocuments(_x12, _x13, _x14, _x15) {\n  return _localStoreApplyBundledDocuments.apply(this, arguments);\n}\n/**\r\n * Returns a promise of a boolean to indicate if the given bundle has already\r\n * been loaded and the create time is newer than the current loading bundle.\r\n */\n\n\nfunction _localStoreApplyBundledDocuments() {\n  _localStoreApplyBundledDocuments = _asyncToGenerator(function* (localStore, bundleConverter, documents, bundleName) {\n    const localStoreImpl = debugCast(localStore);\n    let documentKeys = documentKeySet();\n    let documentMap = mutableDocumentMap();\n\n    for (const bundleDoc of documents) {\n      const documentKey = bundleConverter.toDocumentKey(bundleDoc.metadata.name);\n\n      if (bundleDoc.document) {\n        documentKeys = documentKeys.add(documentKey);\n      }\n\n      const doc = bundleConverter.toMutableDocument(bundleDoc);\n      doc.setReadTime(bundleConverter.toSnapshotVersion(bundleDoc.metadata.readTime));\n      documentMap = documentMap.insert(documentKey, doc);\n    }\n\n    const documentBuffer = localStoreImpl.remoteDocuments.newChangeBuffer({\n      trackRemovals: true // Make sure document removals show up in `getNewDocumentChanges()`\n\n    }); // Allocates a target to hold all document keys from the bundle, such that\n    // they will not get garbage collected right away.\n\n    const umbrellaTargetData = yield localStoreAllocateTarget(localStoreImpl, umbrellaTarget(bundleName));\n    return localStoreImpl.persistence.runTransaction('Apply bundle documents', 'readwrite', txn => {\n      return populateDocumentChangeBuffer(txn, documentBuffer, documentMap).next(documentChangeResult => {\n        documentBuffer.apply(txn);\n        return documentChangeResult;\n      }).next(documentChangeResult => {\n        return localStoreImpl.targetCache.removeMatchingKeysForTargetId(txn, umbrellaTargetData.targetId).next(() => localStoreImpl.targetCache.addMatchingKeys(txn, documentKeys, umbrellaTargetData.targetId)).next(() => localStoreImpl.localDocuments.getLocalViewOfDocuments(txn, documentChangeResult.changedDocuments, documentChangeResult.existenceChangedKeys)).next(() => documentChangeResult.changedDocuments);\n      });\n    });\n  });\n  return _localStoreApplyBundledDocuments.apply(this, arguments);\n}\n\nfunction localStoreHasNewerBundle(localStore, bundleMetadata) {\n  const localStoreImpl = debugCast(localStore);\n  const currentReadTime = fromVersion(bundleMetadata.createTime);\n  return localStoreImpl.persistence.runTransaction('hasNewerBundle', 'readonly', transaction => {\n    return localStoreImpl.bundleCache.getBundleMetadata(transaction, bundleMetadata.id);\n  }).then(cached => {\n    return !!cached && cached.createTime.compareTo(currentReadTime) >= 0;\n  });\n}\n/**\r\n * Saves the given `BundleMetadata` to local persistence.\r\n */\n\n\nfunction localStoreSaveBundle(localStore, bundleMetadata) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Save bundle', 'readwrite', transaction => {\n    return localStoreImpl.bundleCache.saveBundleMetadata(transaction, bundleMetadata);\n  });\n}\n/**\r\n * Returns a promise of a `NamedQuery` associated with given query name. Promise\r\n * resolves to undefined if no persisted data can be found.\r\n */\n\n\nfunction localStoreGetNamedQuery(localStore, queryName) {\n  const localStoreImpl = debugCast(localStore);\n  return localStoreImpl.persistence.runTransaction('Get named query', 'readonly', transaction => localStoreImpl.bundleCache.getNamedQuery(transaction, queryName));\n}\n/**\r\n * Saves the given `NamedQuery` to local persistence.\r\n */\n\n\nfunction localStoreSaveNamedQuery(_x16, _x17) {\n  return _localStoreSaveNamedQuery.apply(this, arguments);\n}\n\nfunction _localStoreSaveNamedQuery() {\n  _localStoreSaveNamedQuery = _asyncToGenerator(function* (localStore, query, documents = documentKeySet()) {\n    // Allocate a target for the named query such that it can be resumed\n    // from associated read time if users use it to listen.\n    // NOTE: this also means if no corresponding target exists, the new target\n    // will remain active and will not get collected, unless users happen to\n    // unlisten the query somehow.\n    const allocated = yield localStoreAllocateTarget(localStore, queryToTarget(fromBundledQuery(query.bundledQuery)));\n    const localStoreImpl = debugCast(localStore);\n    return localStoreImpl.persistence.runTransaction('Save named query', 'readwrite', transaction => {\n      const readTime = fromVersion(query.readTime); // Simply save the query itself if it is older than what the SDK already\n      // has.\n\n      if (allocated.snapshotVersion.compareTo(readTime) >= 0) {\n        return localStoreImpl.bundleCache.saveNamedQuery(transaction, query);\n      } // Update existing target data because the query from the bundle is newer.\n\n\n      const newTargetData = allocated.withResumeToken(ByteString.EMPTY_BYTE_STRING, readTime);\n      localStoreImpl.targetDataByTarget = localStoreImpl.targetDataByTarget.insert(newTargetData.targetId, newTargetData);\n      return localStoreImpl.targetCache.updateTargetData(transaction, newTargetData).next(() => localStoreImpl.targetCache.removeMatchingKeysForTargetId(transaction, allocated.targetId)).next(() => localStoreImpl.targetCache.addMatchingKeys(transaction, documents, allocated.targetId)).next(() => localStoreImpl.bundleCache.saveNamedQuery(transaction, query));\n    });\n  });\n  return _localStoreSaveNamedQuery.apply(this, arguments);\n}\n\nfunction localStoreConfigureFieldIndexes(_x18, _x19) {\n  return _localStoreConfigureFieldIndexes.apply(this, arguments);\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * The Firestore query engine.\r\n *\r\n * Firestore queries can be executed in three modes. The Query Engine determines\r\n * what mode to use based on what data is persisted. The mode only determines\r\n * the runtime complexity of the query - the result set is equivalent across all\r\n * implementations.\r\n *\r\n * The Query engine will use indexed-based execution if a user has configured\r\n * any index that can be used to execute query (via `setIndexConfiguration()`).\r\n * Otherwise, the engine will try to optimize the query by re-using a previously\r\n * persisted query result. If that is not possible, the query will be executed\r\n * via a full collection scan.\r\n *\r\n * Index-based execution is the default when available. The query engine\r\n * supports partial indexed execution and merges the result from the index\r\n * lookup with documents that have not yet been indexed. The index evaluation\r\n * matches the backend's format and as such, the SDK can use indexing for all\r\n * queries that the backend supports.\r\n *\r\n * If no index exists, the query engine tries to take advantage of the target\r\n * document mapping in the TargetCache. These mappings exists for all queries\r\n * that have been synced with the backend at least once and allow the query\r\n * engine to only read documents that previously matched a query plus any\r\n * documents that were edited after the query was last listened to.\r\n *\r\n * There are some cases when this optimization is not guaranteed to produce\r\n * the same results as full collection scans. In these cases, query\r\n * processing falls back to full scans. These cases are:\r\n *\r\n * - Limit queries where a document that matched the query previously no longer\r\n *   matches the query.\r\n *\r\n * - Limit queries where a document edit may cause the document to sort below\r\n *   another document that is in the local cache.\r\n *\r\n * - Queries that have never been CURRENT or free of limbo documents.\r\n */\n\n\nfunction _localStoreConfigureFieldIndexes() {\n  _localStoreConfigureFieldIndexes = _asyncToGenerator(function* (localStore, newFieldIndexes) {\n    const localStoreImpl = debugCast(localStore);\n    const indexManager = localStoreImpl.indexManager;\n    const promises = [];\n    return localStoreImpl.persistence.runTransaction('Configure indexes', 'readwrite', transaction => indexManager.getFieldIndexes(transaction).next(oldFieldIndexes => diffArrays(oldFieldIndexes, newFieldIndexes, fieldIndexSemanticComparator, fieldIndex => {\n      promises.push(indexManager.addFieldIndex(transaction, fieldIndex));\n    }, fieldIndex => {\n      promises.push(indexManager.deleteFieldIndex(transaction, fieldIndex));\n    })).next(() => PersistencePromise.waitFor(promises)));\n  });\n  return _localStoreConfigureFieldIndexes.apply(this, arguments);\n}\n\nclass QueryEngine {\n  constructor() {\n    this.initialized = false;\n  }\n  /** Sets the document view to query against. */\n\n\n  initialize(localDocuments, indexManager) {\n    this.localDocumentsView = localDocuments;\n    this.indexManager = indexManager;\n    this.initialized = true;\n  }\n  /** Returns all local documents matching the specified query. */\n\n\n  getDocumentsMatchingQuery(transaction, query, lastLimboFreeSnapshotVersion, remoteKeys) {\n    return this.performQueryUsingIndex(transaction, query).next(result => result ? result : this.performQueryUsingRemoteKeys(transaction, query, remoteKeys, lastLimboFreeSnapshotVersion)).next(result => result ? result : this.executeFullCollectionScan(transaction, query));\n  }\n  /**\r\n   * Performs an indexed query that evaluates the query based on a collection's\r\n   * persisted index values. Returns `null` if an index is not available.\r\n   */\n\n\n  performQueryUsingIndex(transaction, query) {\n    if (queryMatchesAllDocuments(query)) {\n      // Queries that match all documents don't benefit from using\n      // key-based lookups. It is more efficient to scan all documents in a\n      // collection, rather than to perform individual lookups.\n      return PersistencePromise.resolve(null);\n    }\n\n    let target = queryToTarget(query);\n    return this.indexManager.getIndexType(transaction, target).next(indexType => {\n      if (indexType === 0\n      /* IndexType.NONE */\n      ) {\n        // The target cannot be served from any index.\n        return null;\n      }\n\n      if (query.limit !== null && indexType === 1\n      /* IndexType.PARTIAL */\n      ) {\n        // We cannot apply a limit for targets that are served using a partial\n        // index. If a partial index will be used to serve the target, the\n        // query may return a superset of documents that match the target\n        // (e.g. if the index doesn't include all the target's filters), or\n        // may return the correct set of documents in the wrong order (e.g. if\n        // the index doesn't include a segment for one of the orderBys).\n        // Therefore, a limit should not be applied in such cases.\n        query = queryWithLimit(query, null, \"F\"\n        /* LimitType.First */\n        );\n        target = queryToTarget(query);\n      }\n\n      return this.indexManager.getDocumentsMatchingTarget(transaction, target).next(keys => {\n        const sortedKeys = documentKeySet(...keys);\n        return this.localDocumentsView.getDocuments(transaction, sortedKeys).next(indexedDocuments => {\n          return this.indexManager.getMinOffset(transaction, target).next(offset => {\n            const previousResults = this.applyQuery(query, indexedDocuments);\n\n            if (this.needsRefill(query, previousResults, sortedKeys, offset.readTime)) {\n              // A limit query whose boundaries change due to local\n              // edits can be re-run against the cache by excluding the\n              // limit. This ensures that all documents that match the\n              // query's filters are included in the result set. The SDK\n              // can then apply the limit once all local edits are\n              // incorporated.\n              return this.performQueryUsingIndex(transaction, queryWithLimit(query, null, \"F\"\n              /* LimitType.First */\n              ));\n            }\n\n            return this.appendRemainingResults(transaction, previousResults, query, offset);\n          });\n        });\n      });\n    });\n  }\n  /**\r\n   * Performs a query based on the target's persisted query mapping. Returns\r\n   * `null` if the mapping is not available or cannot be used.\r\n   */\n\n\n  performQueryUsingRemoteKeys(transaction, query, remoteKeys, lastLimboFreeSnapshotVersion) {\n    if (queryMatchesAllDocuments(query)) {\n      // Queries that match all documents don't benefit from using\n      // key-based lookups. It is more efficient to scan all documents in a\n      // collection, rather than to perform individual lookups.\n      return this.executeFullCollectionScan(transaction, query);\n    } // Queries that have never seen a snapshot without limbo free documents\n    // should also be run as a full collection scan.\n\n\n    if (lastLimboFreeSnapshotVersion.isEqual(SnapshotVersion.min())) {\n      return this.executeFullCollectionScan(transaction, query);\n    }\n\n    return this.localDocumentsView.getDocuments(transaction, remoteKeys).next(documents => {\n      const previousResults = this.applyQuery(query, documents);\n\n      if (this.needsRefill(query, previousResults, remoteKeys, lastLimboFreeSnapshotVersion)) {\n        return this.executeFullCollectionScan(transaction, query);\n      }\n\n      if (getLogLevel() <= LogLevel.DEBUG) {\n        logDebug('QueryEngine', 'Re-using previous result from %s to execute query: %s', lastLimboFreeSnapshotVersion.toString(), stringifyQuery(query));\n      } // Retrieve all results for documents that were updated since the last\n      // limbo-document free remote snapshot.\n\n\n      return this.appendRemainingResults(transaction, previousResults, query, newIndexOffsetSuccessorFromReadTime(lastLimboFreeSnapshotVersion, INITIAL_LARGEST_BATCH_ID));\n    });\n  }\n  /** Applies the query filter and sorting to the provided documents.  */\n\n\n  applyQuery(query, documents) {\n    // Sort the documents and re-apply the query filter since previously\n    // matching documents do not necessarily still match the query.\n    let queryResults = new SortedSet(newQueryComparator(query));\n    documents.forEach((_, maybeDoc) => {\n      if (queryMatches(query, maybeDoc)) {\n        queryResults = queryResults.add(maybeDoc);\n      }\n    });\n    return queryResults;\n  }\n  /**\r\n   * Determines if a limit query needs to be refilled from cache, making it\r\n   * ineligible for index-free execution.\r\n   *\r\n   * @param query - The query.\r\n   * @param sortedPreviousResults - The documents that matched the query when it\r\n   * was last synchronized, sorted by the query's comparator.\r\n   * @param remoteKeys - The document keys that matched the query at the last\r\n   * snapshot.\r\n   * @param limboFreeSnapshotVersion - The version of the snapshot when the\r\n   * query was last synchronized.\r\n   */\n\n\n  needsRefill(query, sortedPreviousResults, remoteKeys, limboFreeSnapshotVersion) {\n    if (query.limit === null) {\n      // Queries without limits do not need to be refilled.\n      return false;\n    }\n\n    if (remoteKeys.size !== sortedPreviousResults.size) {\n      // The query needs to be refilled if a previously matching document no\n      // longer matches.\n      return true;\n    } // Limit queries are not eligible for index-free query execution if there is\n    // a potential that an older document from cache now sorts before a document\n    // that was previously part of the limit. This, however, can only happen if\n    // the document at the edge of the limit goes out of limit.\n    // If a document that is not the limit boundary sorts differently,\n    // the boundary of the limit itself did not change and documents from cache\n    // will continue to be \"rejected\" by this boundary. Therefore, we can ignore\n    // any modifications that don't affect the last document.\n\n\n    const docAtLimitEdge = query.limitType === \"F\"\n    /* LimitType.First */\n    ? sortedPreviousResults.last() : sortedPreviousResults.first();\n\n    if (!docAtLimitEdge) {\n      // We don't need to refill the query if there were already no documents.\n      return false;\n    }\n\n    return docAtLimitEdge.hasPendingWrites || docAtLimitEdge.version.compareTo(limboFreeSnapshotVersion) > 0;\n  }\n\n  executeFullCollectionScan(transaction, query) {\n    if (getLogLevel() <= LogLevel.DEBUG) {\n      logDebug('QueryEngine', 'Using full collection scan to execute query:', stringifyQuery(query));\n    }\n\n    return this.localDocumentsView.getDocumentsMatchingQuery(transaction, query, IndexOffset.min());\n  }\n  /**\r\n   * Combines the results from an indexed execution with the remaining documents\r\n   * that have not yet been indexed.\r\n   */\n\n\n  appendRemainingResults(transaction, indexedResults, query, offset) {\n    // Retrieve all results for documents that were updated since the offset.\n    return this.localDocumentsView.getDocumentsMatchingQuery(transaction, query, offset).next(remainingResults => {\n      // Merge with existing results\n      indexedResults.forEach(d => {\n        remainingResults = remainingResults.insert(d.key, d);\n      });\n      return remainingResults;\n    });\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// The format of the LocalStorage key that stores the client state is:\n//     firestore_clients_<persistence_prefix>_<instance_key>\n\n\nconst CLIENT_STATE_KEY_PREFIX = 'firestore_clients';\n/** Assembles the key for a client state in WebStorage */\n\nfunction createWebStorageClientStateKey(persistenceKey, clientId) {\n  return `${CLIENT_STATE_KEY_PREFIX}_${persistenceKey}_${clientId}`;\n} // The format of the WebStorage key that stores the mutation state is:\n//     firestore_mutations_<persistence_prefix>_<batch_id>\n//     (for unauthenticated users)\n// or: firestore_mutations_<persistence_prefix>_<batch_id>_<user_uid>\n//\n// 'user_uid' is last to avoid needing to escape '_' characters that it might\n// contain.\n\n\nconst MUTATION_BATCH_KEY_PREFIX = 'firestore_mutations';\n/** Assembles the key for a mutation batch in WebStorage */\n\nfunction createWebStorageMutationBatchKey(persistenceKey, user, batchId) {\n  let mutationKey = `${MUTATION_BATCH_KEY_PREFIX}_${persistenceKey}_${batchId}`;\n\n  if (user.isAuthenticated()) {\n    mutationKey += `_${user.uid}`;\n  }\n\n  return mutationKey;\n} // The format of the WebStorage key that stores a query target's metadata is:\n//     firestore_targets_<persistence_prefix>_<target_id>\n\n\nconst QUERY_TARGET_KEY_PREFIX = 'firestore_targets';\n/** Assembles the key for a query state in WebStorage */\n\nfunction createWebStorageQueryTargetMetadataKey(persistenceKey, targetId) {\n  return `${QUERY_TARGET_KEY_PREFIX}_${persistenceKey}_${targetId}`;\n} // The WebStorage prefix that stores the primary tab's online state. The\n// format of the key is:\n//     firestore_online_state_<persistence_prefix>\n\n\nconst ONLINE_STATE_KEY_PREFIX = 'firestore_online_state';\n/** Assembles the key for the online state of the primary tab. */\n\nfunction createWebStorageOnlineStateKey(persistenceKey) {\n  return `${ONLINE_STATE_KEY_PREFIX}_${persistenceKey}`;\n} // The WebStorage prefix that plays as a event to indicate the remote documents\n// might have changed due to some secondary tabs loading a bundle.\n// format of the key is:\n//     firestore_bundle_loaded_v2_<persistenceKey>\n// The version ending with \"v2\" stores the list of modified collection groups.\n\n\nconst BUNDLE_LOADED_KEY_PREFIX = 'firestore_bundle_loaded_v2';\n\nfunction createBundleLoadedKey(persistenceKey) {\n  return `${BUNDLE_LOADED_KEY_PREFIX}_${persistenceKey}`;\n} // The WebStorage key prefix for the key that stores the last sequence number allocated. The key\n// looks like 'firestore_sequence_number_<persistence_prefix>'.\n\n\nconst SEQUENCE_NUMBER_KEY_PREFIX = 'firestore_sequence_number';\n/** Assembles the key for the current sequence number. */\n\nfunction createWebStorageSequenceNumberKey(persistenceKey) {\n  return `${SEQUENCE_NUMBER_KEY_PREFIX}_${persistenceKey}`;\n}\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$a = 'SharedClientState';\n/**\r\n * Holds the state of a mutation batch, including its user ID, batch ID and\r\n * whether the batch is 'pending', 'acknowledged' or 'rejected'.\r\n */\n// Visible for testing\n\nclass MutationMetadata {\n  constructor(user, batchId, state, error) {\n    this.user = user;\n    this.batchId = batchId;\n    this.state = state;\n    this.error = error;\n  }\n  /**\r\n   * Parses a MutationMetadata from its JSON representation in WebStorage.\r\n   * Logs a warning and returns null if the format of the data is not valid.\r\n   */\n\n\n  static fromWebStorageEntry(user, batchId, value) {\n    const mutationBatch = JSON.parse(value);\n    let validData = typeof mutationBatch === 'object' && ['pending', 'acknowledged', 'rejected'].indexOf(mutationBatch.state) !== -1 && (mutationBatch.error === undefined || typeof mutationBatch.error === 'object');\n    let firestoreError = undefined;\n\n    if (validData && mutationBatch.error) {\n      validData = typeof mutationBatch.error.message === 'string' && typeof mutationBatch.error.code === 'string';\n\n      if (validData) {\n        firestoreError = new FirestoreError(mutationBatch.error.code, mutationBatch.error.message);\n      }\n    }\n\n    if (validData) {\n      return new MutationMetadata(user, batchId, mutationBatch.state, firestoreError);\n    } else {\n      logError(LOG_TAG$a, `Failed to parse mutation state for ID '${batchId}': ${value}`);\n      return null;\n    }\n  }\n\n  toWebStorageJSON() {\n    const batchMetadata = {\n      state: this.state,\n      updateTimeMs: Date.now() // Modify the existing value to trigger update.\n\n    };\n\n    if (this.error) {\n      batchMetadata.error = {\n        code: this.error.code,\n        message: this.error.message\n      };\n    }\n\n    return JSON.stringify(batchMetadata);\n  }\n\n}\n/**\r\n * Holds the state of a query target, including its target ID and whether the\r\n * target is 'not-current', 'current' or 'rejected'.\r\n */\n// Visible for testing\n\n\nclass QueryTargetMetadata {\n  constructor(targetId, state, error) {\n    this.targetId = targetId;\n    this.state = state;\n    this.error = error;\n  }\n  /**\r\n   * Parses a QueryTargetMetadata from its JSON representation in WebStorage.\r\n   * Logs a warning and returns null if the format of the data is not valid.\r\n   */\n\n\n  static fromWebStorageEntry(targetId, value) {\n    const targetState = JSON.parse(value);\n    let validData = typeof targetState === 'object' && ['not-current', 'current', 'rejected'].indexOf(targetState.state) !== -1 && (targetState.error === undefined || typeof targetState.error === 'object');\n    let firestoreError = undefined;\n\n    if (validData && targetState.error) {\n      validData = typeof targetState.error.message === 'string' && typeof targetState.error.code === 'string';\n\n      if (validData) {\n        firestoreError = new FirestoreError(targetState.error.code, targetState.error.message);\n      }\n    }\n\n    if (validData) {\n      return new QueryTargetMetadata(targetId, targetState.state, firestoreError);\n    } else {\n      logError(LOG_TAG$a, `Failed to parse target state for ID '${targetId}': ${value}`);\n      return null;\n    }\n  }\n\n  toWebStorageJSON() {\n    const targetState = {\n      state: this.state,\n      updateTimeMs: Date.now() // Modify the existing value to trigger update.\n\n    };\n\n    if (this.error) {\n      targetState.error = {\n        code: this.error.code,\n        message: this.error.message\n      };\n    }\n\n    return JSON.stringify(targetState);\n  }\n\n}\n/**\r\n * This class represents the immutable ClientState for a client read from\r\n * WebStorage, containing the list of active query targets.\r\n */\n\n\nclass RemoteClientState {\n  constructor(clientId, activeTargetIds) {\n    this.clientId = clientId;\n    this.activeTargetIds = activeTargetIds;\n  }\n  /**\r\n   * Parses a RemoteClientState from the JSON representation in WebStorage.\r\n   * Logs a warning and returns null if the format of the data is not valid.\r\n   */\n\n\n  static fromWebStorageEntry(clientId, value) {\n    const clientState = JSON.parse(value);\n    let validData = typeof clientState === 'object' && clientState.activeTargetIds instanceof Array;\n    let activeTargetIdsSet = targetIdSet();\n\n    for (let i = 0; validData && i < clientState.activeTargetIds.length; ++i) {\n      validData = isSafeInteger(clientState.activeTargetIds[i]);\n      activeTargetIdsSet = activeTargetIdsSet.add(clientState.activeTargetIds[i]);\n    }\n\n    if (validData) {\n      return new RemoteClientState(clientId, activeTargetIdsSet);\n    } else {\n      logError(LOG_TAG$a, `Failed to parse client data for instance '${clientId}': ${value}`);\n      return null;\n    }\n  }\n\n}\n/**\r\n * This class represents the online state for all clients participating in\r\n * multi-tab. The online state is only written to by the primary client, and\r\n * used in secondary clients to update their query views.\r\n */\n\n\nclass SharedOnlineState {\n  constructor(clientId, onlineState) {\n    this.clientId = clientId;\n    this.onlineState = onlineState;\n  }\n  /**\r\n   * Parses a SharedOnlineState from its JSON representation in WebStorage.\r\n   * Logs a warning and returns null if the format of the data is not valid.\r\n   */\n\n\n  static fromWebStorageEntry(value) {\n    const onlineState = JSON.parse(value);\n    const validData = typeof onlineState === 'object' && ['Unknown', 'Online', 'Offline'].indexOf(onlineState.onlineState) !== -1 && typeof onlineState.clientId === 'string';\n\n    if (validData) {\n      return new SharedOnlineState(onlineState.clientId, onlineState.onlineState);\n    } else {\n      logError(LOG_TAG$a, `Failed to parse online state: ${value}`);\n      return null;\n    }\n  }\n\n}\n/**\r\n * Metadata state of the local client. Unlike `RemoteClientState`, this class is\r\n * mutable and keeps track of all pending mutations, which allows us to\r\n * update the range of pending mutation batch IDs as new mutations are added or\r\n * removed.\r\n *\r\n * The data in `LocalClientState` is not read from WebStorage and instead\r\n * updated via its instance methods. The updated state can be serialized via\r\n * `toWebStorageJSON()`.\r\n */\n// Visible for testing.\n\n\nclass LocalClientState {\n  constructor() {\n    this.activeTargetIds = targetIdSet();\n  }\n\n  addQueryTarget(targetId) {\n    this.activeTargetIds = this.activeTargetIds.add(targetId);\n  }\n\n  removeQueryTarget(targetId) {\n    this.activeTargetIds = this.activeTargetIds.delete(targetId);\n  }\n  /**\r\n   * Converts this entry into a JSON-encoded format we can use for WebStorage.\r\n   * Does not encode `clientId` as it is part of the key in WebStorage.\r\n   */\n\n\n  toWebStorageJSON() {\n    const data = {\n      activeTargetIds: this.activeTargetIds.toArray(),\n      updateTimeMs: Date.now() // Modify the existing value to trigger update.\n\n    };\n    return JSON.stringify(data);\n  }\n\n}\n/**\r\n * `WebStorageSharedClientState` uses WebStorage (window.localStorage) as the\r\n * backing store for the SharedClientState. It keeps track of all active\r\n * clients and supports modifications of the local client's data.\r\n */\n\n\nclass WebStorageSharedClientState {\n  constructor(window, queue, persistenceKey, localClientId, initialUser) {\n    this.window = window;\n    this.queue = queue;\n    this.persistenceKey = persistenceKey;\n    this.localClientId = localClientId;\n    this.syncEngine = null;\n    this.onlineStateHandler = null;\n    this.sequenceNumberHandler = null;\n    this.storageListener = this.handleWebStorageEvent.bind(this);\n    this.activeClients = new SortedMap(primitiveComparator);\n    this.started = false;\n    /**\r\n     * Captures WebStorage events that occur before `start()` is called. These\r\n     * events are replayed once `WebStorageSharedClientState` is started.\r\n     */\n\n    this.earlyEvents = []; // Escape the special characters mentioned here:\n    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/Regular_Expressions\n\n    const escapedPersistenceKey = persistenceKey.replace(/[.*+?^${}()|[\\]\\\\]/g, '\\\\$&');\n    this.storage = this.window.localStorage;\n    this.currentUser = initialUser;\n    this.localClientStorageKey = createWebStorageClientStateKey(this.persistenceKey, this.localClientId);\n    this.sequenceNumberKey = createWebStorageSequenceNumberKey(this.persistenceKey);\n    this.activeClients = this.activeClients.insert(this.localClientId, new LocalClientState());\n    this.clientStateKeyRe = new RegExp(`^${CLIENT_STATE_KEY_PREFIX}_${escapedPersistenceKey}_([^_]*)$`);\n    this.mutationBatchKeyRe = new RegExp(`^${MUTATION_BATCH_KEY_PREFIX}_${escapedPersistenceKey}_(\\\\d+)(?:_(.*))?$`);\n    this.queryTargetKeyRe = new RegExp(`^${QUERY_TARGET_KEY_PREFIX}_${escapedPersistenceKey}_(\\\\d+)$`);\n    this.onlineStateKey = createWebStorageOnlineStateKey(this.persistenceKey);\n    this.bundleLoadedKey = createBundleLoadedKey(this.persistenceKey); // Rather than adding the storage observer during start(), we add the\n    // storage observer during initialization. This ensures that we collect\n    // events before other components populate their initial state (during their\n    // respective start() calls). Otherwise, we might for example miss a\n    // mutation that is added after LocalStore's start() processed the existing\n    // mutations but before we observe WebStorage events.\n\n    this.window.addEventListener('storage', this.storageListener);\n  }\n  /** Returns 'true' if WebStorage is available in the current environment. */\n\n\n  static isAvailable(window) {\n    return !!(window && window.localStorage);\n  }\n\n  start() {\n    var _this11 = this;\n\n    return _asyncToGenerator(function* () {\n      // Retrieve the list of existing clients to backfill the data in\n      // SharedClientState.\n      const existingClients = yield _this11.syncEngine.getActiveClients();\n\n      for (const clientId of existingClients) {\n        if (clientId === _this11.localClientId) {\n          continue;\n        }\n\n        const storageItem = _this11.getItem(createWebStorageClientStateKey(_this11.persistenceKey, clientId));\n\n        if (storageItem) {\n          const clientState = RemoteClientState.fromWebStorageEntry(clientId, storageItem);\n\n          if (clientState) {\n            _this11.activeClients = _this11.activeClients.insert(clientState.clientId, clientState);\n          }\n        }\n      }\n\n      _this11.persistClientState(); // Check if there is an existing online state and call the callback handler\n      // if applicable.\n\n\n      const onlineStateJSON = _this11.storage.getItem(_this11.onlineStateKey);\n\n      if (onlineStateJSON) {\n        const onlineState = _this11.fromWebStorageOnlineState(onlineStateJSON);\n\n        if (onlineState) {\n          _this11.handleOnlineStateEvent(onlineState);\n        }\n      }\n\n      for (const event of _this11.earlyEvents) {\n        _this11.handleWebStorageEvent(event);\n      }\n\n      _this11.earlyEvents = []; // Register a window unload hook to remove the client metadata entry from\n      // WebStorage even if `shutdown()` was not called.\n\n      _this11.window.addEventListener('pagehide', () => _this11.shutdown());\n\n      _this11.started = true;\n    })();\n  }\n\n  writeSequenceNumber(sequenceNumber) {\n    this.setItem(this.sequenceNumberKey, JSON.stringify(sequenceNumber));\n  }\n\n  getAllActiveQueryTargets() {\n    return this.extractActiveQueryTargets(this.activeClients);\n  }\n\n  isActiveQueryTarget(targetId) {\n    let found = false;\n    this.activeClients.forEach((key, value) => {\n      if (value.activeTargetIds.has(targetId)) {\n        found = true;\n      }\n    });\n    return found;\n  }\n\n  addPendingMutation(batchId) {\n    this.persistMutationState(batchId, 'pending');\n  }\n\n  updateMutationState(batchId, state, error) {\n    this.persistMutationState(batchId, state, error); // Once a final mutation result is observed by other clients, they no longer\n    // access the mutation's metadata entry. Since WebStorage replays events\n    // in order, it is safe to delete the entry right after updating it.\n\n    this.removeMutationState(batchId);\n  }\n\n  addLocalQueryTarget(targetId) {\n    let queryState = 'not-current'; // Lookup an existing query state if the target ID was already registered\n    // by another tab\n\n    if (this.isActiveQueryTarget(targetId)) {\n      const storageItem = this.storage.getItem(createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId));\n\n      if (storageItem) {\n        const metadata = QueryTargetMetadata.fromWebStorageEntry(targetId, storageItem);\n\n        if (metadata) {\n          queryState = metadata.state;\n        }\n      }\n    }\n\n    this.localClientState.addQueryTarget(targetId);\n    this.persistClientState();\n    return queryState;\n  }\n\n  removeLocalQueryTarget(targetId) {\n    this.localClientState.removeQueryTarget(targetId);\n    this.persistClientState();\n  }\n\n  isLocalQueryTarget(targetId) {\n    return this.localClientState.activeTargetIds.has(targetId);\n  }\n\n  clearQueryState(targetId) {\n    this.removeItem(createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId));\n  }\n\n  updateQueryState(targetId, state, error) {\n    this.persistQueryTargetState(targetId, state, error);\n  }\n\n  handleUserChange(user, removedBatchIds, addedBatchIds) {\n    removedBatchIds.forEach(batchId => {\n      this.removeMutationState(batchId);\n    });\n    this.currentUser = user;\n    addedBatchIds.forEach(batchId => {\n      this.addPendingMutation(batchId);\n    });\n  }\n\n  setOnlineState(onlineState) {\n    this.persistOnlineState(onlineState);\n  }\n\n  notifyBundleLoaded(collectionGroups) {\n    this.persistBundleLoadedState(collectionGroups);\n  }\n\n  shutdown() {\n    if (this.started) {\n      this.window.removeEventListener('storage', this.storageListener);\n      this.removeItem(this.localClientStorageKey);\n      this.started = false;\n    }\n  }\n\n  getItem(key) {\n    const value = this.storage.getItem(key);\n    logDebug(LOG_TAG$a, 'READ', key, value);\n    return value;\n  }\n\n  setItem(key, value) {\n    logDebug(LOG_TAG$a, 'SET', key, value);\n    this.storage.setItem(key, value);\n  }\n\n  removeItem(key) {\n    logDebug(LOG_TAG$a, 'REMOVE', key);\n    this.storage.removeItem(key);\n  }\n\n  handleWebStorageEvent(event) {\n    var _this12 = this;\n\n    // Note: The function is typed to take Event to be interface-compatible with\n    // `Window.addEventListener`.\n    const storageEvent = event;\n\n    if (storageEvent.storageArea === this.storage) {\n      logDebug(LOG_TAG$a, 'EVENT', storageEvent.key, storageEvent.newValue);\n\n      if (storageEvent.key === this.localClientStorageKey) {\n        logError('Received WebStorage notification for local change. Another client might have ' + 'garbage-collected our state');\n        return;\n      }\n\n      this.queue.enqueueRetryable( /*#__PURE__*/_asyncToGenerator(function* () {\n        if (!_this12.started) {\n          _this12.earlyEvents.push(storageEvent);\n\n          return;\n        }\n\n        if (storageEvent.key === null) {\n          return;\n        }\n\n        if (_this12.clientStateKeyRe.test(storageEvent.key)) {\n          if (storageEvent.newValue != null) {\n            const clientState = _this12.fromWebStorageClientState(storageEvent.key, storageEvent.newValue);\n\n            if (clientState) {\n              return _this12.handleClientStateEvent(clientState.clientId, clientState);\n            }\n          } else {\n            const clientId = _this12.fromWebStorageClientStateKey(storageEvent.key);\n\n            return _this12.handleClientStateEvent(clientId, null);\n          }\n        } else if (_this12.mutationBatchKeyRe.test(storageEvent.key)) {\n          if (storageEvent.newValue !== null) {\n            const mutationMetadata = _this12.fromWebStorageMutationMetadata(storageEvent.key, storageEvent.newValue);\n\n            if (mutationMetadata) {\n              return _this12.handleMutationBatchEvent(mutationMetadata);\n            }\n          }\n        } else if (_this12.queryTargetKeyRe.test(storageEvent.key)) {\n          if (storageEvent.newValue !== null) {\n            const queryTargetMetadata = _this12.fromWebStorageQueryTargetMetadata(storageEvent.key, storageEvent.newValue);\n\n            if (queryTargetMetadata) {\n              return _this12.handleQueryTargetEvent(queryTargetMetadata);\n            }\n          }\n        } else if (storageEvent.key === _this12.onlineStateKey) {\n          if (storageEvent.newValue !== null) {\n            const onlineState = _this12.fromWebStorageOnlineState(storageEvent.newValue);\n\n            if (onlineState) {\n              return _this12.handleOnlineStateEvent(onlineState);\n            }\n          }\n        } else if (storageEvent.key === _this12.sequenceNumberKey) {\n          const sequenceNumber = fromWebStorageSequenceNumber(storageEvent.newValue);\n\n          if (sequenceNumber !== ListenSequence.INVALID) {\n            _this12.sequenceNumberHandler(sequenceNumber);\n          }\n        } else if (storageEvent.key === _this12.bundleLoadedKey) {\n          const collectionGroups = _this12.fromWebStoreBundleLoadedState(storageEvent.newValue);\n\n          yield Promise.all(collectionGroups.map(cg => _this12.syncEngine.synchronizeWithChangedDocuments(cg)));\n        }\n      }));\n    }\n  }\n\n  get localClientState() {\n    return this.activeClients.get(this.localClientId);\n  }\n\n  persistClientState() {\n    this.setItem(this.localClientStorageKey, this.localClientState.toWebStorageJSON());\n  }\n\n  persistMutationState(batchId, state, error) {\n    const mutationState = new MutationMetadata(this.currentUser, batchId, state, error);\n    const mutationKey = createWebStorageMutationBatchKey(this.persistenceKey, this.currentUser, batchId);\n    this.setItem(mutationKey, mutationState.toWebStorageJSON());\n  }\n\n  removeMutationState(batchId) {\n    const mutationKey = createWebStorageMutationBatchKey(this.persistenceKey, this.currentUser, batchId);\n    this.removeItem(mutationKey);\n  }\n\n  persistOnlineState(onlineState) {\n    const entry = {\n      clientId: this.localClientId,\n      onlineState\n    };\n    this.storage.setItem(this.onlineStateKey, JSON.stringify(entry));\n  }\n\n  persistQueryTargetState(targetId, state, error) {\n    const targetKey = createWebStorageQueryTargetMetadataKey(this.persistenceKey, targetId);\n    const targetMetadata = new QueryTargetMetadata(targetId, state, error);\n    this.setItem(targetKey, targetMetadata.toWebStorageJSON());\n  }\n\n  persistBundleLoadedState(collectionGroups) {\n    const json = JSON.stringify(Array.from(collectionGroups));\n    this.setItem(this.bundleLoadedKey, json);\n  }\n  /**\r\n   * Parses a client state key in WebStorage. Returns null if the key does not\r\n   * match the expected key format.\r\n   */\n\n\n  fromWebStorageClientStateKey(key) {\n    const match = this.clientStateKeyRe.exec(key);\n    return match ? match[1] : null;\n  }\n  /**\r\n   * Parses a client state in WebStorage. Returns 'null' if the value could not\r\n   * be parsed.\r\n   */\n\n\n  fromWebStorageClientState(key, value) {\n    const clientId = this.fromWebStorageClientStateKey(key);\n    return RemoteClientState.fromWebStorageEntry(clientId, value);\n  }\n  /**\r\n   * Parses a mutation batch state in WebStorage. Returns 'null' if the value\r\n   * could not be parsed.\r\n   */\n\n\n  fromWebStorageMutationMetadata(key, value) {\n    const match = this.mutationBatchKeyRe.exec(key);\n    const batchId = Number(match[1]);\n    const userId = match[2] !== undefined ? match[2] : null;\n    return MutationMetadata.fromWebStorageEntry(new User(userId), batchId, value);\n  }\n  /**\r\n   * Parses a query target state from WebStorage. Returns 'null' if the value\r\n   * could not be parsed.\r\n   */\n\n\n  fromWebStorageQueryTargetMetadata(key, value) {\n    const match = this.queryTargetKeyRe.exec(key);\n    const targetId = Number(match[1]);\n    return QueryTargetMetadata.fromWebStorageEntry(targetId, value);\n  }\n  /**\r\n   * Parses an online state from WebStorage. Returns 'null' if the value\r\n   * could not be parsed.\r\n   */\n\n\n  fromWebStorageOnlineState(value) {\n    return SharedOnlineState.fromWebStorageEntry(value);\n  }\n\n  fromWebStoreBundleLoadedState(value) {\n    return JSON.parse(value);\n  }\n\n  handleMutationBatchEvent(mutationBatch) {\n    var _this13 = this;\n\n    return _asyncToGenerator(function* () {\n      if (mutationBatch.user.uid !== _this13.currentUser.uid) {\n        logDebug(LOG_TAG$a, `Ignoring mutation for non-active user ${mutationBatch.user.uid}`);\n        return;\n      }\n\n      return _this13.syncEngine.applyBatchState(mutationBatch.batchId, mutationBatch.state, mutationBatch.error);\n    })();\n  }\n\n  handleQueryTargetEvent(targetMetadata) {\n    return this.syncEngine.applyTargetState(targetMetadata.targetId, targetMetadata.state, targetMetadata.error);\n  }\n\n  handleClientStateEvent(clientId, clientState) {\n    const updatedClients = clientState ? this.activeClients.insert(clientId, clientState) : this.activeClients.remove(clientId);\n    const existingTargets = this.extractActiveQueryTargets(this.activeClients);\n    const newTargets = this.extractActiveQueryTargets(updatedClients);\n    const addedTargets = [];\n    const removedTargets = [];\n    newTargets.forEach(targetId => {\n      if (!existingTargets.has(targetId)) {\n        addedTargets.push(targetId);\n      }\n    });\n    existingTargets.forEach(targetId => {\n      if (!newTargets.has(targetId)) {\n        removedTargets.push(targetId);\n      }\n    });\n    return this.syncEngine.applyActiveTargetsChange(addedTargets, removedTargets).then(() => {\n      this.activeClients = updatedClients;\n    });\n  }\n\n  handleOnlineStateEvent(onlineState) {\n    // We check whether the client that wrote this online state is still active\n    // by comparing its client ID to the list of clients kept active in\n    // IndexedDb. If a client does not update their IndexedDb client state\n    // within 5 seconds, it is considered inactive and we don't emit an online\n    // state event.\n    if (this.activeClients.get(onlineState.clientId)) {\n      this.onlineStateHandler(onlineState.onlineState);\n    }\n  }\n\n  extractActiveQueryTargets(clients) {\n    let activeTargets = targetIdSet();\n    clients.forEach((kev, value) => {\n      activeTargets = activeTargets.unionWith(value.activeTargetIds);\n    });\n    return activeTargets;\n  }\n\n}\n\nfunction fromWebStorageSequenceNumber(seqString) {\n  let sequenceNumber = ListenSequence.INVALID;\n\n  if (seqString != null) {\n    try {\n      const parsed = JSON.parse(seqString);\n      hardAssert(typeof parsed === 'number');\n      sequenceNumber = parsed;\n    } catch (e) {\n      logError(LOG_TAG$a, 'Failed to read sequence number from WebStorage', e);\n    }\n  }\n\n  return sequenceNumber;\n}\n/**\r\n * `MemorySharedClientState` is a simple implementation of SharedClientState for\r\n * clients using memory persistence. The state in this class remains fully\r\n * isolated and no synchronization is performed.\r\n */\n\n\nclass MemorySharedClientState {\n  constructor() {\n    this.localState = new LocalClientState();\n    this.queryState = {};\n    this.onlineStateHandler = null;\n    this.sequenceNumberHandler = null;\n  }\n\n  addPendingMutation(batchId) {// No op.\n  }\n\n  updateMutationState(batchId, state, error) {// No op.\n  }\n\n  addLocalQueryTarget(targetId) {\n    this.localState.addQueryTarget(targetId);\n    return this.queryState[targetId] || 'not-current';\n  }\n\n  updateQueryState(targetId, state, error) {\n    this.queryState[targetId] = state;\n  }\n\n  removeLocalQueryTarget(targetId) {\n    this.localState.removeQueryTarget(targetId);\n  }\n\n  isLocalQueryTarget(targetId) {\n    return this.localState.activeTargetIds.has(targetId);\n  }\n\n  clearQueryState(targetId) {\n    delete this.queryState[targetId];\n  }\n\n  getAllActiveQueryTargets() {\n    return this.localState.activeTargetIds;\n  }\n\n  isActiveQueryTarget(targetId) {\n    return this.localState.activeTargetIds.has(targetId);\n  }\n\n  start() {\n    this.localState = new LocalClientState();\n    return Promise.resolve();\n  }\n\n  handleUserChange(user, removedBatchIds, addedBatchIds) {// No op.\n  }\n\n  setOnlineState(onlineState) {// No op.\n  }\n\n  shutdown() {}\n\n  writeSequenceNumber(sequenceNumber) {}\n\n  notifyBundleLoaded(collectionGroups) {// No op.\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass NoopConnectivityMonitor {\n  addCallback(callback) {// No-op.\n  }\n\n  shutdown() {// No-op.\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Provides a simple helper class that implements the Stream interface to\r\n * bridge to other implementations that are streams but do not implement the\r\n * interface. The stream callbacks are invoked with the callOn... methods.\r\n */\n\n\nclass StreamBridge {\n  constructor(args) {\n    this.sendFn = args.sendFn;\n    this.closeFn = args.closeFn;\n  }\n\n  onOpen(callback) {\n    this.wrappedOnOpen = callback;\n  }\n\n  onClose(callback) {\n    this.wrappedOnClose = callback;\n  }\n\n  onMessage(callback) {\n    this.wrappedOnMessage = callback;\n  }\n\n  close() {\n    this.closeFn();\n  }\n\n  send(msg) {\n    this.sendFn(msg);\n  }\n\n  callOnOpen() {\n    this.wrappedOnOpen();\n  }\n\n  callOnClose(err) {\n    this.wrappedOnClose(err);\n  }\n\n  callOnMessage(msg) {\n    this.wrappedOnMessage(msg);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * The value returned from the most recent invocation of\r\n * `generateUniqueDebugId()`, or null if it has never been invoked.\r\n */\n\n\nlet lastUniqueDebugId = null;\n/**\r\n * Generates and returns an initial value for `lastUniqueDebugId`.\r\n *\r\n * The returned value is randomly selected from a range of integers that are\r\n * represented as 8 hexadecimal digits. This means that (within reason) any\r\n * numbers generated by incrementing the returned number by 1 will also be\r\n * represented by 8 hexadecimal digits. This leads to all \"IDs\" having the same\r\n * length when converted to a hexadecimal string, making reading logs containing\r\n * these IDs easier to follow. And since the return value is randomly selected\r\n * it will help to differentiate between logs from different executions.\r\n */\n\nfunction generateInitialUniqueDebugId() {\n  const minResult = 0x10000000;\n  const maxResult = 0x90000000;\n  const resultRange = maxResult - minResult;\n  const resultOffset = Math.round(resultRange * Math.random());\n  return minResult + resultOffset;\n}\n/**\r\n * Generates and returns a unique ID as a hexadecimal string.\r\n *\r\n * The returned ID is intended to be used in debug logging messages to help\r\n * correlate log messages that may be spatially separated in the logs, but\r\n * logically related. For example, a network connection could include the same\r\n * \"debug ID\" string in all of its log messages to help trace a specific\r\n * connection over time.\r\n *\r\n * @return the 10-character generated ID (e.g. \"0xa1b2c3d4\").\r\n */\n\n\nfunction generateUniqueDebugId() {\n  if (lastUniqueDebugId === null) {\n    lastUniqueDebugId = generateInitialUniqueDebugId();\n  } else {\n    lastUniqueDebugId++;\n  }\n\n  return '0x' + lastUniqueDebugId.toString(16);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/*\r\n * Utilities for dealing with node.js-style APIs. See nodePromise for more\r\n * details.\r\n */\n\n/**\r\n * Creates a node-style callback that resolves or rejects a new Promise. The\r\n * callback is passed to the given action which can then use the callback as\r\n * a parameter to a node-style function.\r\n *\r\n * The intent is to directly bridge a node-style function (which takes a\r\n * callback) into a Promise without manually converting between the node-style\r\n * callback and the promise at each call.\r\n *\r\n * In effect it allows you to convert:\r\n *\r\n * @example\r\n * new Promise((resolve: (value?: fs.Stats) => void,\r\n *              reject: (error?: any) => void) => {\r\n *   fs.stat(path, (error?: any, stat?: fs.Stats) => {\r\n *     if (error) {\r\n *       reject(error);\r\n *     } else {\r\n *       resolve(stat);\r\n *     }\r\n *   });\r\n * });\r\n *\r\n * Into\r\n * @example\r\n * nodePromise((callback: NodeCallback<fs.Stats>) => {\r\n *   fs.stat(path, callback);\r\n * });\r\n *\r\n * @param action - a function that takes a node-style callback as an argument\r\n *     and then uses that callback to invoke some node-style API.\r\n * @returns a new Promise which will be rejected if the callback is given the\r\n *     first Error parameter or will resolve to the value given otherwise.\r\n */\n\n\nfunction nodePromise(action) {\n  return new Promise((resolve, reject) => {\n    action((error, value) => {\n      if (error) {\n        reject(error);\n      } else {\n        resolve(value);\n      }\n    });\n  });\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// TODO: Fetch runtime version from grpc-js/package.json instead\n// when there's a cleaner way to dynamic require JSON in both Node ESM and CJS\n\n\nconst grpcVersion = '1.7.3';\nconst LOG_TAG$9 = 'GrpcConnection';\nconst X_GOOG_API_CLIENT_VALUE = `gl-node/${process.versions.node} fire/${SDK_VERSION} grpc/${grpcVersion}`;\n\nfunction createMetadata(databasePath, authToken, appCheckToken, appId) {\n  hardAssert(authToken === null || authToken.type === 'OAuth');\n  const metadata = new grpc.Metadata();\n\n  if (authToken) {\n    authToken.headers.forEach((value, key) => metadata.set(key, value));\n  }\n\n  if (appCheckToken) {\n    appCheckToken.headers.forEach((value, key) => metadata.set(key, value));\n  }\n\n  if (appId) {\n    metadata.set('X-Firebase-GMPID', appId);\n  }\n\n  metadata.set('X-Goog-Api-Client', X_GOOG_API_CLIENT_VALUE); // These headers are used to improve routing and project isolation by the\n  // backend.\n  // TODO(b/199767712): We are keeping 'Google-Cloud-Resource-Prefix' until Emulators can be\n  // released with cl/428820046. Currently blocked because Emulators are now built with Java\n  // 11 from Google3.\n\n  metadata.set('Google-Cloud-Resource-Prefix', databasePath);\n  metadata.set('x-goog-request-params', databasePath);\n  return metadata;\n}\n/**\r\n * A Connection implemented by GRPC-Node.\r\n */\n\n\nclass GrpcConnection {\n  constructor(protos, databaseInfo) {\n    this.databaseInfo = databaseInfo; // We cache stubs for the most-recently-used token.\n\n    this.cachedStub = null; // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n    this.firestore = protos['google']['firestore']['v1'];\n    this.databasePath = `projects/${databaseInfo.databaseId.projectId}/databases/${databaseInfo.databaseId.database}`;\n  }\n\n  get shouldResourcePathBeIncludedInRequest() {\n    // Both `invokeRPC()` and `invokeStreamingRPC()` ignore their `path` arguments, and expect\n    // the \"path\" to be part of the given `request`.\n    return true;\n  }\n\n  ensureActiveStub() {\n    if (!this.cachedStub) {\n      logDebug(LOG_TAG$9, 'Creating Firestore stub.');\n      const credentials = this.databaseInfo.ssl ? grpc.credentials.createSsl() : grpc.credentials.createInsecure();\n      this.cachedStub = new this.firestore.Firestore(this.databaseInfo.host, credentials);\n    }\n\n    return this.cachedStub;\n  }\n\n  invokeRPC(rpcName, path, request, authToken, appCheckToken) {\n    const streamId = generateUniqueDebugId();\n    const stub = this.ensureActiveStub();\n    const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\n    const jsonRequest = Object.assign({\n      database: this.databasePath\n    }, request);\n    return nodePromise(callback => {\n      logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} invoked with request:`, request);\n      return stub[rpcName](jsonRequest, metadata, (grpcError, value) => {\n        if (grpcError) {\n          logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} failed with error:`, grpcError);\n          callback(new FirestoreError(mapCodeFromRpcCode(grpcError.code), grpcError.message));\n        } else {\n          logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} completed with response:`, value);\n          callback(undefined, value);\n        }\n      });\n    });\n  }\n\n  invokeStreamingRPC(rpcName, path, request, authToken, appCheckToken, expectedResponseCount) {\n    const streamId = generateUniqueDebugId();\n    const results = [];\n    const responseDeferred = new Deferred();\n    logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} invoked (streaming) with request:`, request);\n    const stub = this.ensureActiveStub();\n    const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\n    const jsonRequest = Object.assign(Object.assign({}, request), {\n      database: this.databasePath\n    });\n    const stream = stub[rpcName](jsonRequest, metadata);\n    let callbackFired = false;\n    stream.on('data', response => {\n      logDebug(LOG_TAG$9, `RPC ${rpcName} ${streamId} received result:`, response);\n      results.push(response);\n\n      if (expectedResponseCount !== undefined && results.length === expectedResponseCount) {\n        callbackFired = true;\n        responseDeferred.resolve(results);\n      }\n    });\n    stream.on('end', () => {\n      logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} completed.`);\n\n      if (!callbackFired) {\n        callbackFired = true;\n        responseDeferred.resolve(results);\n      }\n    });\n    stream.on('error', grpcError => {\n      logDebug(LOG_TAG$9, `RPC '${rpcName}' ${streamId} failed with error:`, grpcError);\n      const code = mapCodeFromRpcCode(grpcError.code);\n      responseDeferred.reject(new FirestoreError(code, grpcError.message));\n    });\n    return responseDeferred.promise;\n  } // TODO(mikelehen): This \"method\" is a monster. Should be refactored.\n\n\n  openStream(rpcName, authToken, appCheckToken) {\n    const streamId = generateUniqueDebugId();\n    const stub = this.ensureActiveStub();\n    const metadata = createMetadata(this.databasePath, authToken, appCheckToken, this.databaseInfo.appId);\n    const grpcStream = stub[rpcName](metadata);\n    let closed = false;\n\n    const close = err => {\n      if (!closed) {\n        closed = true;\n        stream.callOnClose(err);\n        grpcStream.end();\n      }\n    };\n\n    const stream = new StreamBridge({\n      sendFn: msg => {\n        if (!closed) {\n          logDebug(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} sending:`, msg);\n\n          try {\n            grpcStream.write(msg);\n          } catch (e) {\n            // This probably means we didn't conform to the proto.  Make sure to\n            // log the message we sent.\n            logError('Failure sending:', msg);\n            logError('Error:', e);\n            throw e;\n          }\n        } else {\n          logDebug(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} ` + 'not sending because gRPC stream is closed:', msg);\n        }\n      },\n      closeFn: () => {\n        logDebug(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} closed locally via close().`);\n        close();\n      }\n    });\n    grpcStream.on('data', msg => {\n      if (!closed) {\n        logDebug(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} received:`, msg);\n        stream.callOnMessage(msg);\n      }\n    });\n    grpcStream.on('end', () => {\n      logDebug(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} ended.`);\n      close();\n    });\n    grpcStream.on('error', grpcError => {\n      if (!closed) {\n        logWarn(LOG_TAG$9, `RPC '${rpcName}' stream ${streamId} error. Code:`, grpcError.code, 'Message:', grpcError.message);\n        const code = mapCodeFromRpcCode(grpcError.code);\n        close(new FirestoreError(code, grpcError.message));\n      }\n    });\n    logDebug(LOG_TAG$9, `Opening RPC '${rpcName}' stream ${streamId} ` + `to ${this.databaseInfo.host}`); // TODO(dimond): Since grpc has no explicit open status (or does it?) we\n    // simulate an onOpen in the next loop after the stream had it's listeners\n    // registered\n\n    setTimeout(() => {\n      stream.callOnOpen();\n    }, 0);\n    return stream;\n  }\n\n}\n\nconst nested = {\n  google: {\n    nested: {\n      protobuf: {\n        options: {\n          csharp_namespace: \"Google.Protobuf.WellKnownTypes\",\n          go_package: \"github.com/golang/protobuf/ptypes/wrappers\",\n          java_package: \"com.google.protobuf\",\n          java_outer_classname: \"WrappersProto\",\n          java_multiple_files: true,\n          objc_class_prefix: \"GPB\",\n          cc_enable_arenas: true,\n          optimize_for: \"SPEED\"\n        },\n        nested: {\n          Timestamp: {\n            fields: {\n              seconds: {\n                type: \"int64\",\n                id: 1\n              },\n              nanos: {\n                type: \"int32\",\n                id: 2\n              }\n            }\n          },\n          FileDescriptorSet: {\n            fields: {\n              file: {\n                rule: \"repeated\",\n                type: \"FileDescriptorProto\",\n                id: 1\n              }\n            }\n          },\n          FileDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              \"package\": {\n                type: \"string\",\n                id: 2\n              },\n              dependency: {\n                rule: \"repeated\",\n                type: \"string\",\n                id: 3\n              },\n              publicDependency: {\n                rule: \"repeated\",\n                type: \"int32\",\n                id: 10,\n                options: {\n                  packed: false\n                }\n              },\n              weakDependency: {\n                rule: \"repeated\",\n                type: \"int32\",\n                id: 11,\n                options: {\n                  packed: false\n                }\n              },\n              messageType: {\n                rule: \"repeated\",\n                type: \"DescriptorProto\",\n                id: 4\n              },\n              enumType: {\n                rule: \"repeated\",\n                type: \"EnumDescriptorProto\",\n                id: 5\n              },\n              service: {\n                rule: \"repeated\",\n                type: \"ServiceDescriptorProto\",\n                id: 6\n              },\n              extension: {\n                rule: \"repeated\",\n                type: \"FieldDescriptorProto\",\n                id: 7\n              },\n              options: {\n                type: \"FileOptions\",\n                id: 8\n              },\n              sourceCodeInfo: {\n                type: \"SourceCodeInfo\",\n                id: 9\n              },\n              syntax: {\n                type: \"string\",\n                id: 12\n              }\n            }\n          },\n          DescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              field: {\n                rule: \"repeated\",\n                type: \"FieldDescriptorProto\",\n                id: 2\n              },\n              extension: {\n                rule: \"repeated\",\n                type: \"FieldDescriptorProto\",\n                id: 6\n              },\n              nestedType: {\n                rule: \"repeated\",\n                type: \"DescriptorProto\",\n                id: 3\n              },\n              enumType: {\n                rule: \"repeated\",\n                type: \"EnumDescriptorProto\",\n                id: 4\n              },\n              extensionRange: {\n                rule: \"repeated\",\n                type: \"ExtensionRange\",\n                id: 5\n              },\n              oneofDecl: {\n                rule: \"repeated\",\n                type: \"OneofDescriptorProto\",\n                id: 8\n              },\n              options: {\n                type: \"MessageOptions\",\n                id: 7\n              },\n              reservedRange: {\n                rule: \"repeated\",\n                type: \"ReservedRange\",\n                id: 9\n              },\n              reservedName: {\n                rule: \"repeated\",\n                type: \"string\",\n                id: 10\n              }\n            },\n            nested: {\n              ExtensionRange: {\n                fields: {\n                  start: {\n                    type: \"int32\",\n                    id: 1\n                  },\n                  end: {\n                    type: \"int32\",\n                    id: 2\n                  }\n                }\n              },\n              ReservedRange: {\n                fields: {\n                  start: {\n                    type: \"int32\",\n                    id: 1\n                  },\n                  end: {\n                    type: \"int32\",\n                    id: 2\n                  }\n                }\n              }\n            }\n          },\n          FieldDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              number: {\n                type: \"int32\",\n                id: 3\n              },\n              label: {\n                type: \"Label\",\n                id: 4\n              },\n              type: {\n                type: \"Type\",\n                id: 5\n              },\n              typeName: {\n                type: \"string\",\n                id: 6\n              },\n              extendee: {\n                type: \"string\",\n                id: 2\n              },\n              defaultValue: {\n                type: \"string\",\n                id: 7\n              },\n              oneofIndex: {\n                type: \"int32\",\n                id: 9\n              },\n              jsonName: {\n                type: \"string\",\n                id: 10\n              },\n              options: {\n                type: \"FieldOptions\",\n                id: 8\n              }\n            },\n            nested: {\n              Type: {\n                values: {\n                  TYPE_DOUBLE: 1,\n                  TYPE_FLOAT: 2,\n                  TYPE_INT64: 3,\n                  TYPE_UINT64: 4,\n                  TYPE_INT32: 5,\n                  TYPE_FIXED64: 6,\n                  TYPE_FIXED32: 7,\n                  TYPE_BOOL: 8,\n                  TYPE_STRING: 9,\n                  TYPE_GROUP: 10,\n                  TYPE_MESSAGE: 11,\n                  TYPE_BYTES: 12,\n                  TYPE_UINT32: 13,\n                  TYPE_ENUM: 14,\n                  TYPE_SFIXED32: 15,\n                  TYPE_SFIXED64: 16,\n                  TYPE_SINT32: 17,\n                  TYPE_SINT64: 18\n                }\n              },\n              Label: {\n                values: {\n                  LABEL_OPTIONAL: 1,\n                  LABEL_REQUIRED: 2,\n                  LABEL_REPEATED: 3\n                }\n              }\n            }\n          },\n          OneofDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              options: {\n                type: \"OneofOptions\",\n                id: 2\n              }\n            }\n          },\n          EnumDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              value: {\n                rule: \"repeated\",\n                type: \"EnumValueDescriptorProto\",\n                id: 2\n              },\n              options: {\n                type: \"EnumOptions\",\n                id: 3\n              }\n            }\n          },\n          EnumValueDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              number: {\n                type: \"int32\",\n                id: 2\n              },\n              options: {\n                type: \"EnumValueOptions\",\n                id: 3\n              }\n            }\n          },\n          ServiceDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              method: {\n                rule: \"repeated\",\n                type: \"MethodDescriptorProto\",\n                id: 2\n              },\n              options: {\n                type: \"ServiceOptions\",\n                id: 3\n              }\n            }\n          },\n          MethodDescriptorProto: {\n            fields: {\n              name: {\n                type: \"string\",\n                id: 1\n              },\n              inputType: {\n                type: \"string\",\n                id: 2\n              },\n              outputType: {\n                type: \"string\",\n                id: 3\n              },\n              options: {\n                type: \"MethodOptions\",\n                id: 4\n              },\n              clientStreaming: {\n                type: \"bool\",\n                id: 5\n              },\n              serverStreaming: {\n                type: \"bool\",\n                id: 6\n              }\n            }\n          },\n          FileOptions: {\n            fields: {\n              javaPackage: {\n                type: \"string\",\n                id: 1\n              },\n              javaOuterClassname: {\n                type: \"string\",\n                id: 8\n              },\n              javaMultipleFiles: {\n                type: \"bool\",\n                id: 10\n              },\n              javaGenerateEqualsAndHash: {\n                type: \"bool\",\n                id: 20,\n                options: {\n                  deprecated: true\n                }\n              },\n              javaStringCheckUtf8: {\n                type: \"bool\",\n                id: 27\n              },\n              optimizeFor: {\n                type: \"OptimizeMode\",\n                id: 9,\n                options: {\n                  \"default\": \"SPEED\"\n                }\n              },\n              goPackage: {\n                type: \"string\",\n                id: 11\n              },\n              ccGenericServices: {\n                type: \"bool\",\n                id: 16\n              },\n              javaGenericServices: {\n                type: \"bool\",\n                id: 17\n              },\n              pyGenericServices: {\n                type: \"bool\",\n                id: 18\n              },\n              deprecated: {\n                type: \"bool\",\n                id: 23\n              },\n              ccEnableArenas: {\n                type: \"bool\",\n                id: 31\n              },\n              objcClassPrefix: {\n                type: \"string\",\n                id: 36\n              },\n              csharpNamespace: {\n                type: \"string\",\n                id: 37\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]],\n            reserved: [[38, 38]],\n            nested: {\n              OptimizeMode: {\n                values: {\n                  SPEED: 1,\n                  CODE_SIZE: 2,\n                  LITE_RUNTIME: 3\n                }\n              }\n            }\n          },\n          MessageOptions: {\n            fields: {\n              messageSetWireFormat: {\n                type: \"bool\",\n                id: 1\n              },\n              noStandardDescriptorAccessor: {\n                type: \"bool\",\n                id: 2\n              },\n              deprecated: {\n                type: \"bool\",\n                id: 3\n              },\n              mapEntry: {\n                type: \"bool\",\n                id: 7\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]],\n            reserved: [[8, 8]]\n          },\n          FieldOptions: {\n            fields: {\n              ctype: {\n                type: \"CType\",\n                id: 1,\n                options: {\n                  \"default\": \"STRING\"\n                }\n              },\n              packed: {\n                type: \"bool\",\n                id: 2\n              },\n              jstype: {\n                type: \"JSType\",\n                id: 6,\n                options: {\n                  \"default\": \"JS_NORMAL\"\n                }\n              },\n              lazy: {\n                type: \"bool\",\n                id: 5\n              },\n              deprecated: {\n                type: \"bool\",\n                id: 3\n              },\n              weak: {\n                type: \"bool\",\n                id: 10\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]],\n            reserved: [[4, 4]],\n            nested: {\n              CType: {\n                values: {\n                  STRING: 0,\n                  CORD: 1,\n                  STRING_PIECE: 2\n                }\n              },\n              JSType: {\n                values: {\n                  JS_NORMAL: 0,\n                  JS_STRING: 1,\n                  JS_NUMBER: 2\n                }\n              }\n            }\n          },\n          OneofOptions: {\n            fields: {\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]]\n          },\n          EnumOptions: {\n            fields: {\n              allowAlias: {\n                type: \"bool\",\n                id: 2\n              },\n              deprecated: {\n                type: \"bool\",\n                id: 3\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]]\n          },\n          EnumValueOptions: {\n            fields: {\n              deprecated: {\n                type: \"bool\",\n                id: 1\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]]\n          },\n          ServiceOptions: {\n            fields: {\n              deprecated: {\n                type: \"bool\",\n                id: 33\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]]\n          },\n          MethodOptions: {\n            fields: {\n              deprecated: {\n                type: \"bool\",\n                id: 33\n              },\n              uninterpretedOption: {\n                rule: \"repeated\",\n                type: \"UninterpretedOption\",\n                id: 999\n              }\n            },\n            extensions: [[1000, 536870911]]\n          },\n          UninterpretedOption: {\n            fields: {\n              name: {\n                rule: \"repeated\",\n                type: \"NamePart\",\n                id: 2\n              },\n              identifierValue: {\n                type: \"string\",\n                id: 3\n              },\n              positiveIntValue: {\n                type: \"uint64\",\n                id: 4\n              },\n              negativeIntValue: {\n                type: \"int64\",\n                id: 5\n              },\n              doubleValue: {\n                type: \"double\",\n                id: 6\n              },\n              stringValue: {\n                type: \"bytes\",\n                id: 7\n              },\n              aggregateValue: {\n                type: \"string\",\n                id: 8\n              }\n            },\n            nested: {\n              NamePart: {\n                fields: {\n                  namePart: {\n                    rule: \"required\",\n                    type: \"string\",\n                    id: 1\n                  },\n                  isExtension: {\n                    rule: \"required\",\n                    type: \"bool\",\n                    id: 2\n                  }\n                }\n              }\n            }\n          },\n          SourceCodeInfo: {\n            fields: {\n              location: {\n                rule: \"repeated\",\n                type: \"Location\",\n                id: 1\n              }\n            },\n            nested: {\n              Location: {\n                fields: {\n                  path: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 1\n                  },\n                  span: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 2\n                  },\n                  leadingComments: {\n                    type: \"string\",\n                    id: 3\n                  },\n                  trailingComments: {\n                    type: \"string\",\n                    id: 4\n                  },\n                  leadingDetachedComments: {\n                    rule: \"repeated\",\n                    type: \"string\",\n                    id: 6\n                  }\n                }\n              }\n            }\n          },\n          GeneratedCodeInfo: {\n            fields: {\n              annotation: {\n                rule: \"repeated\",\n                type: \"Annotation\",\n                id: 1\n              }\n            },\n            nested: {\n              Annotation: {\n                fields: {\n                  path: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 1\n                  },\n                  sourceFile: {\n                    type: \"string\",\n                    id: 2\n                  },\n                  begin: {\n                    type: \"int32\",\n                    id: 3\n                  },\n                  end: {\n                    type: \"int32\",\n                    id: 4\n                  }\n                }\n              }\n            }\n          },\n          Struct: {\n            fields: {\n              fields: {\n                keyType: \"string\",\n                type: \"Value\",\n                id: 1\n              }\n            }\n          },\n          Value: {\n            oneofs: {\n              kind: {\n                oneof: [\"nullValue\", \"numberValue\", \"stringValue\", \"boolValue\", \"structValue\", \"listValue\"]\n              }\n            },\n            fields: {\n              nullValue: {\n                type: \"NullValue\",\n                id: 1\n              },\n              numberValue: {\n                type: \"double\",\n                id: 2\n              },\n              stringValue: {\n                type: \"string\",\n                id: 3\n              },\n              boolValue: {\n                type: \"bool\",\n                id: 4\n              },\n              structValue: {\n                type: \"Struct\",\n                id: 5\n              },\n              listValue: {\n                type: \"ListValue\",\n                id: 6\n              }\n            }\n          },\n          NullValue: {\n            values: {\n              NULL_VALUE: 0\n            }\n          },\n          ListValue: {\n            fields: {\n              values: {\n                rule: \"repeated\",\n                type: \"Value\",\n                id: 1\n              }\n            }\n          },\n          Empty: {\n            fields: {}\n          },\n          DoubleValue: {\n            fields: {\n              value: {\n                type: \"double\",\n                id: 1\n              }\n            }\n          },\n          FloatValue: {\n            fields: {\n              value: {\n                type: \"float\",\n                id: 1\n              }\n            }\n          },\n          Int64Value: {\n            fields: {\n              value: {\n                type: \"int64\",\n                id: 1\n              }\n            }\n          },\n          UInt64Value: {\n            fields: {\n              value: {\n                type: \"uint64\",\n                id: 1\n              }\n            }\n          },\n          Int32Value: {\n            fields: {\n              value: {\n                type: \"int32\",\n                id: 1\n              }\n            }\n          },\n          UInt32Value: {\n            fields: {\n              value: {\n                type: \"uint32\",\n                id: 1\n              }\n            }\n          },\n          BoolValue: {\n            fields: {\n              value: {\n                type: \"bool\",\n                id: 1\n              }\n            }\n          },\n          StringValue: {\n            fields: {\n              value: {\n                type: \"string\",\n                id: 1\n              }\n            }\n          },\n          BytesValue: {\n            fields: {\n              value: {\n                type: \"bytes\",\n                id: 1\n              }\n            }\n          },\n          Any: {\n            fields: {\n              typeUrl: {\n                type: \"string\",\n                id: 1\n              },\n              value: {\n                type: \"bytes\",\n                id: 2\n              }\n            }\n          }\n        }\n      },\n      firestore: {\n        nested: {\n          v1: {\n            options: {\n              csharp_namespace: \"Google.Cloud.Firestore.V1\",\n              go_package: \"google.golang.org/genproto/googleapis/firestore/v1;firestore\",\n              java_multiple_files: true,\n              java_outer_classname: \"WriteProto\",\n              java_package: \"com.google.firestore.v1\",\n              objc_class_prefix: \"GCFS\",\n              php_namespace: \"Google\\\\Cloud\\\\Firestore\\\\V1\",\n              ruby_package: \"Google::Cloud::Firestore::V1\"\n            },\n            nested: {\n              AggregationResult: {\n                fields: {\n                  aggregateFields: {\n                    keyType: \"string\",\n                    type: \"Value\",\n                    id: 2\n                  }\n                }\n              },\n              BitSequence: {\n                fields: {\n                  bitmap: {\n                    type: \"bytes\",\n                    id: 1\n                  },\n                  padding: {\n                    type: \"int32\",\n                    id: 2\n                  }\n                }\n              },\n              BloomFilter: {\n                fields: {\n                  bits: {\n                    type: \"BitSequence\",\n                    id: 1\n                  },\n                  hashCount: {\n                    type: \"int32\",\n                    id: 2\n                  }\n                }\n              },\n              DocumentMask: {\n                fields: {\n                  fieldPaths: {\n                    rule: \"repeated\",\n                    type: \"string\",\n                    id: 1\n                  }\n                }\n              },\n              Precondition: {\n                oneofs: {\n                  conditionType: {\n                    oneof: [\"exists\", \"updateTime\"]\n                  }\n                },\n                fields: {\n                  exists: {\n                    type: \"bool\",\n                    id: 1\n                  },\n                  updateTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 2\n                  }\n                }\n              },\n              TransactionOptions: {\n                oneofs: {\n                  mode: {\n                    oneof: [\"readOnly\", \"readWrite\"]\n                  }\n                },\n                fields: {\n                  readOnly: {\n                    type: \"ReadOnly\",\n                    id: 2\n                  },\n                  readWrite: {\n                    type: \"ReadWrite\",\n                    id: 3\n                  }\n                },\n                nested: {\n                  ReadWrite: {\n                    fields: {\n                      retryTransaction: {\n                        type: \"bytes\",\n                        id: 1\n                      }\n                    }\n                  },\n                  ReadOnly: {\n                    oneofs: {\n                      consistencySelector: {\n                        oneof: [\"readTime\"]\n                      }\n                    },\n                    fields: {\n                      readTime: {\n                        type: \"google.protobuf.Timestamp\",\n                        id: 2\n                      }\n                    }\n                  }\n                }\n              },\n              Document: {\n                fields: {\n                  name: {\n                    type: \"string\",\n                    id: 1\n                  },\n                  fields: {\n                    keyType: \"string\",\n                    type: \"Value\",\n                    id: 2\n                  },\n                  createTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 3\n                  },\n                  updateTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 4\n                  }\n                }\n              },\n              Value: {\n                oneofs: {\n                  valueType: {\n                    oneof: [\"nullValue\", \"booleanValue\", \"integerValue\", \"doubleValue\", \"timestampValue\", \"stringValue\", \"bytesValue\", \"referenceValue\", \"geoPointValue\", \"arrayValue\", \"mapValue\"]\n                  }\n                },\n                fields: {\n                  nullValue: {\n                    type: \"google.protobuf.NullValue\",\n                    id: 11\n                  },\n                  booleanValue: {\n                    type: \"bool\",\n                    id: 1\n                  },\n                  integerValue: {\n                    type: \"int64\",\n                    id: 2\n                  },\n                  doubleValue: {\n                    type: \"double\",\n                    id: 3\n                  },\n                  timestampValue: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 10\n                  },\n                  stringValue: {\n                    type: \"string\",\n                    id: 17\n                  },\n                  bytesValue: {\n                    type: \"bytes\",\n                    id: 18\n                  },\n                  referenceValue: {\n                    type: \"string\",\n                    id: 5\n                  },\n                  geoPointValue: {\n                    type: \"google.type.LatLng\",\n                    id: 8\n                  },\n                  arrayValue: {\n                    type: \"ArrayValue\",\n                    id: 9\n                  },\n                  mapValue: {\n                    type: \"MapValue\",\n                    id: 6\n                  }\n                }\n              },\n              ArrayValue: {\n                fields: {\n                  values: {\n                    rule: \"repeated\",\n                    type: \"Value\",\n                    id: 1\n                  }\n                }\n              },\n              MapValue: {\n                fields: {\n                  fields: {\n                    keyType: \"string\",\n                    type: \"Value\",\n                    id: 1\n                  }\n                }\n              },\n              Firestore: {\n                options: {\n                  \"(google.api.default_host)\": \"firestore.googleapis.com\",\n                  \"(google.api.oauth_scopes)\": \"https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/datastore\"\n                },\n                methods: {\n                  GetDocument: {\n                    requestType: \"GetDocumentRequest\",\n                    responseType: \"Document\",\n                    options: {\n                      \"(google.api.http).get\": \"/v1/{name=projects/*/databases/*/documents/*/**}\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        get: \"/v1/{name=projects/*/databases/*/documents/*/**}\"\n                      }\n                    }]\n                  },\n                  ListDocuments: {\n                    requestType: \"ListDocumentsRequest\",\n                    responseType: \"ListDocumentsResponse\",\n                    options: {\n                      \"(google.api.http).get\": \"/v1/{parent=projects/*/databases/*/documents/*/**}/{collection_id}\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        get: \"/v1/{parent=projects/*/databases/*/documents/*/**}/{collection_id}\"\n                      }\n                    }]\n                  },\n                  UpdateDocument: {\n                    requestType: \"UpdateDocumentRequest\",\n                    responseType: \"Document\",\n                    options: {\n                      \"(google.api.http).patch\": \"/v1/{document.name=projects/*/databases/*/documents/*/**}\",\n                      \"(google.api.http).body\": \"document\",\n                      \"(google.api.method_signature)\": \"document,update_mask\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        patch: \"/v1/{document.name=projects/*/databases/*/documents/*/**}\",\n                        body: \"document\"\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"document,update_mask\"\n                    }]\n                  },\n                  DeleteDocument: {\n                    requestType: \"DeleteDocumentRequest\",\n                    responseType: \"google.protobuf.Empty\",\n                    options: {\n                      \"(google.api.http).delete\": \"/v1/{name=projects/*/databases/*/documents/*/**}\",\n                      \"(google.api.method_signature)\": \"name\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        \"delete\": \"/v1/{name=projects/*/databases/*/documents/*/**}\"\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"name\"\n                    }]\n                  },\n                  BatchGetDocuments: {\n                    requestType: \"BatchGetDocumentsRequest\",\n                    responseType: \"BatchGetDocumentsResponse\",\n                    responseStream: true,\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:batchGet\",\n                      \"(google.api.http).body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:batchGet\",\n                        body: \"*\"\n                      }\n                    }]\n                  },\n                  BeginTransaction: {\n                    requestType: \"BeginTransactionRequest\",\n                    responseType: \"BeginTransactionResponse\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:beginTransaction\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.method_signature)\": \"database\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:beginTransaction\",\n                        body: \"*\"\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"database\"\n                    }]\n                  },\n                  Commit: {\n                    requestType: \"CommitRequest\",\n                    responseType: \"CommitResponse\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:commit\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.method_signature)\": \"database,writes\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:commit\",\n                        body: \"*\"\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"database,writes\"\n                    }]\n                  },\n                  Rollback: {\n                    requestType: \"RollbackRequest\",\n                    responseType: \"google.protobuf.Empty\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:rollback\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.method_signature)\": \"database,transaction\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:rollback\",\n                        body: \"*\"\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"database,transaction\"\n                    }]\n                  },\n                  RunQuery: {\n                    requestType: \"RunQueryRequest\",\n                    responseType: \"RunQueryResponse\",\n                    responseStream: true,\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{parent=projects/*/databases/*/documents}:runQuery\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.http).additional_bindings.post\": \"/v1/{parent=projects/*/databases/*/documents/*/**}:runQuery\",\n                      \"(google.api.http).additional_bindings.body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{parent=projects/*/databases/*/documents}:runQuery\",\n                        body: \"*\",\n                        additional_bindings: {\n                          post: \"/v1/{parent=projects/*/databases/*/documents/*/**}:runQuery\",\n                          body: \"*\"\n                        }\n                      }\n                    }]\n                  },\n                  RunAggregationQuery: {\n                    requestType: \"RunAggregationQueryRequest\",\n                    responseType: \"RunAggregationQueryResponse\",\n                    responseStream: true,\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{parent=projects/*/databases/*/documents}:runAggregationQuery\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.http).additional_bindings.post\": \"/v1/{parent=projects/*/databases/*/documents/*/**}:runAggregationQuery\",\n                      \"(google.api.http).additional_bindings.body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{parent=projects/*/databases/*/documents}:runAggregationQuery\",\n                        body: \"*\",\n                        additional_bindings: {\n                          post: \"/v1/{parent=projects/*/databases/*/documents/*/**}:runAggregationQuery\",\n                          body: \"*\"\n                        }\n                      }\n                    }]\n                  },\n                  PartitionQuery: {\n                    requestType: \"PartitionQueryRequest\",\n                    responseType: \"PartitionQueryResponse\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{parent=projects/*/databases/*/documents}:partitionQuery\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.http).additional_bindings.post\": \"/v1/{parent=projects/*/databases/*/documents/*/**}:partitionQuery\",\n                      \"(google.api.http).additional_bindings.body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{parent=projects/*/databases/*/documents}:partitionQuery\",\n                        body: \"*\",\n                        additional_bindings: {\n                          post: \"/v1/{parent=projects/*/databases/*/documents/*/**}:partitionQuery\",\n                          body: \"*\"\n                        }\n                      }\n                    }]\n                  },\n                  Write: {\n                    requestType: \"WriteRequest\",\n                    requestStream: true,\n                    responseType: \"WriteResponse\",\n                    responseStream: true,\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:write\",\n                      \"(google.api.http).body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:write\",\n                        body: \"*\"\n                      }\n                    }]\n                  },\n                  Listen: {\n                    requestType: \"ListenRequest\",\n                    requestStream: true,\n                    responseType: \"ListenResponse\",\n                    responseStream: true,\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:listen\",\n                      \"(google.api.http).body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:listen\",\n                        body: \"*\"\n                      }\n                    }]\n                  },\n                  ListCollectionIds: {\n                    requestType: \"ListCollectionIdsRequest\",\n                    responseType: \"ListCollectionIdsResponse\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{parent=projects/*/databases/*/documents}:listCollectionIds\",\n                      \"(google.api.http).body\": \"*\",\n                      \"(google.api.http).additional_bindings.post\": \"/v1/{parent=projects/*/databases/*/documents/*/**}:listCollectionIds\",\n                      \"(google.api.http).additional_bindings.body\": \"*\",\n                      \"(google.api.method_signature)\": \"parent\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{parent=projects/*/databases/*/documents}:listCollectionIds\",\n                        body: \"*\",\n                        additional_bindings: {\n                          post: \"/v1/{parent=projects/*/databases/*/documents/*/**}:listCollectionIds\",\n                          body: \"*\"\n                        }\n                      }\n                    }, {\n                      \"(google.api.method_signature)\": \"parent\"\n                    }]\n                  },\n                  BatchWrite: {\n                    requestType: \"BatchWriteRequest\",\n                    responseType: \"BatchWriteResponse\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{database=projects/*/databases/*}/documents:batchWrite\",\n                      \"(google.api.http).body\": \"*\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{database=projects/*/databases/*}/documents:batchWrite\",\n                        body: \"*\"\n                      }\n                    }]\n                  },\n                  CreateDocument: {\n                    requestType: \"CreateDocumentRequest\",\n                    responseType: \"Document\",\n                    options: {\n                      \"(google.api.http).post\": \"/v1/{parent=projects/*/databases/*/documents/**}/{collection_id}\",\n                      \"(google.api.http).body\": \"document\"\n                    },\n                    parsedOptions: [{\n                      \"(google.api.http)\": {\n                        post: \"/v1/{parent=projects/*/databases/*/documents/**}/{collection_id}\",\n                        body: \"document\"\n                      }\n                    }]\n                  }\n                }\n              },\n              GetDocumentRequest: {\n                oneofs: {\n                  consistencySelector: {\n                    oneof: [\"transaction\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  name: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  mask: {\n                    type: \"DocumentMask\",\n                    id: 2\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 3\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 5\n                  }\n                }\n              },\n              ListDocumentsRequest: {\n                oneofs: {\n                  consistencySelector: {\n                    oneof: [\"transaction\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  collectionId: {\n                    type: \"string\",\n                    id: 2,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  pageSize: {\n                    type: \"int32\",\n                    id: 3\n                  },\n                  pageToken: {\n                    type: \"string\",\n                    id: 4\n                  },\n                  orderBy: {\n                    type: \"string\",\n                    id: 6\n                  },\n                  mask: {\n                    type: \"DocumentMask\",\n                    id: 7\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 8\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 10\n                  },\n                  showMissing: {\n                    type: \"bool\",\n                    id: 12\n                  }\n                }\n              },\n              ListDocumentsResponse: {\n                fields: {\n                  documents: {\n                    rule: \"repeated\",\n                    type: \"Document\",\n                    id: 1\n                  },\n                  nextPageToken: {\n                    type: \"string\",\n                    id: 2\n                  }\n                }\n              },\n              CreateDocumentRequest: {\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  collectionId: {\n                    type: \"string\",\n                    id: 2,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  documentId: {\n                    type: \"string\",\n                    id: 3\n                  },\n                  document: {\n                    type: \"Document\",\n                    id: 4,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  mask: {\n                    type: \"DocumentMask\",\n                    id: 5\n                  }\n                }\n              },\n              UpdateDocumentRequest: {\n                fields: {\n                  document: {\n                    type: \"Document\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  updateMask: {\n                    type: \"DocumentMask\",\n                    id: 2\n                  },\n                  mask: {\n                    type: \"DocumentMask\",\n                    id: 3\n                  },\n                  currentDocument: {\n                    type: \"Precondition\",\n                    id: 4\n                  }\n                }\n              },\n              DeleteDocumentRequest: {\n                fields: {\n                  name: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  currentDocument: {\n                    type: \"Precondition\",\n                    id: 2\n                  }\n                }\n              },\n              BatchGetDocumentsRequest: {\n                oneofs: {\n                  consistencySelector: {\n                    oneof: [\"transaction\", \"newTransaction\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  documents: {\n                    rule: \"repeated\",\n                    type: \"string\",\n                    id: 2\n                  },\n                  mask: {\n                    type: \"DocumentMask\",\n                    id: 3\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 4\n                  },\n                  newTransaction: {\n                    type: \"TransactionOptions\",\n                    id: 5\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 7\n                  }\n                }\n              },\n              BatchGetDocumentsResponse: {\n                oneofs: {\n                  result: {\n                    oneof: [\"found\", \"missing\"]\n                  }\n                },\n                fields: {\n                  found: {\n                    type: \"Document\",\n                    id: 1\n                  },\n                  missing: {\n                    type: \"string\",\n                    id: 2\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 3\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 4\n                  }\n                }\n              },\n              BeginTransactionRequest: {\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  options: {\n                    type: \"TransactionOptions\",\n                    id: 2\n                  }\n                }\n              },\n              BeginTransactionResponse: {\n                fields: {\n                  transaction: {\n                    type: \"bytes\",\n                    id: 1\n                  }\n                }\n              },\n              CommitRequest: {\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  writes: {\n                    rule: \"repeated\",\n                    type: \"Write\",\n                    id: 2\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 3\n                  }\n                }\n              },\n              CommitResponse: {\n                fields: {\n                  writeResults: {\n                    rule: \"repeated\",\n                    type: \"WriteResult\",\n                    id: 1\n                  },\n                  commitTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 2\n                  }\n                }\n              },\n              RollbackRequest: {\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 2,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  }\n                }\n              },\n              RunQueryRequest: {\n                oneofs: {\n                  queryType: {\n                    oneof: [\"structuredQuery\"]\n                  },\n                  consistencySelector: {\n                    oneof: [\"transaction\", \"newTransaction\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  structuredQuery: {\n                    type: \"StructuredQuery\",\n                    id: 2\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 5\n                  },\n                  newTransaction: {\n                    type: \"TransactionOptions\",\n                    id: 6\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 7\n                  }\n                }\n              },\n              RunQueryResponse: {\n                fields: {\n                  transaction: {\n                    type: \"bytes\",\n                    id: 2\n                  },\n                  document: {\n                    type: \"Document\",\n                    id: 1\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 3\n                  },\n                  skippedResults: {\n                    type: \"int32\",\n                    id: 4\n                  }\n                }\n              },\n              RunAggregationQueryRequest: {\n                oneofs: {\n                  queryType: {\n                    oneof: [\"structuredAggregationQuery\"]\n                  },\n                  consistencySelector: {\n                    oneof: [\"transaction\", \"newTransaction\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  structuredAggregationQuery: {\n                    type: \"StructuredAggregationQuery\",\n                    id: 2\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 4\n                  },\n                  newTransaction: {\n                    type: \"TransactionOptions\",\n                    id: 5\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 6\n                  }\n                }\n              },\n              RunAggregationQueryResponse: {\n                fields: {\n                  result: {\n                    type: \"AggregationResult\",\n                    id: 1\n                  },\n                  transaction: {\n                    type: \"bytes\",\n                    id: 2\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 3\n                  }\n                }\n              },\n              PartitionQueryRequest: {\n                oneofs: {\n                  queryType: {\n                    oneof: [\"structuredQuery\"]\n                  }\n                },\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  structuredQuery: {\n                    type: \"StructuredQuery\",\n                    id: 2\n                  },\n                  partitionCount: {\n                    type: \"int64\",\n                    id: 3\n                  },\n                  pageToken: {\n                    type: \"string\",\n                    id: 4\n                  },\n                  pageSize: {\n                    type: \"int32\",\n                    id: 5\n                  }\n                }\n              },\n              PartitionQueryResponse: {\n                fields: {\n                  partitions: {\n                    rule: \"repeated\",\n                    type: \"Cursor\",\n                    id: 1\n                  },\n                  nextPageToken: {\n                    type: \"string\",\n                    id: 2\n                  }\n                }\n              },\n              WriteRequest: {\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  streamId: {\n                    type: \"string\",\n                    id: 2\n                  },\n                  writes: {\n                    rule: \"repeated\",\n                    type: \"Write\",\n                    id: 3\n                  },\n                  streamToken: {\n                    type: \"bytes\",\n                    id: 4\n                  },\n                  labels: {\n                    keyType: \"string\",\n                    type: \"string\",\n                    id: 5\n                  }\n                }\n              },\n              WriteResponse: {\n                fields: {\n                  streamId: {\n                    type: \"string\",\n                    id: 1\n                  },\n                  streamToken: {\n                    type: \"bytes\",\n                    id: 2\n                  },\n                  writeResults: {\n                    rule: \"repeated\",\n                    type: \"WriteResult\",\n                    id: 3\n                  },\n                  commitTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 4\n                  }\n                }\n              },\n              ListenRequest: {\n                oneofs: {\n                  targetChange: {\n                    oneof: [\"addTarget\", \"removeTarget\"]\n                  }\n                },\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  addTarget: {\n                    type: \"Target\",\n                    id: 2\n                  },\n                  removeTarget: {\n                    type: \"int32\",\n                    id: 3\n                  },\n                  labels: {\n                    keyType: \"string\",\n                    type: \"string\",\n                    id: 4\n                  }\n                }\n              },\n              ListenResponse: {\n                oneofs: {\n                  responseType: {\n                    oneof: [\"targetChange\", \"documentChange\", \"documentDelete\", \"documentRemove\", \"filter\"]\n                  }\n                },\n                fields: {\n                  targetChange: {\n                    type: \"TargetChange\",\n                    id: 2\n                  },\n                  documentChange: {\n                    type: \"DocumentChange\",\n                    id: 3\n                  },\n                  documentDelete: {\n                    type: \"DocumentDelete\",\n                    id: 4\n                  },\n                  documentRemove: {\n                    type: \"DocumentRemove\",\n                    id: 6\n                  },\n                  filter: {\n                    type: \"ExistenceFilter\",\n                    id: 5\n                  }\n                }\n              },\n              Target: {\n                oneofs: {\n                  targetType: {\n                    oneof: [\"query\", \"documents\"]\n                  },\n                  resumeType: {\n                    oneof: [\"resumeToken\", \"readTime\"]\n                  }\n                },\n                fields: {\n                  query: {\n                    type: \"QueryTarget\",\n                    id: 2\n                  },\n                  documents: {\n                    type: \"DocumentsTarget\",\n                    id: 3\n                  },\n                  resumeToken: {\n                    type: \"bytes\",\n                    id: 4\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 11\n                  },\n                  targetId: {\n                    type: \"int32\",\n                    id: 5\n                  },\n                  once: {\n                    type: \"bool\",\n                    id: 6\n                  },\n                  expectedCount: {\n                    type: \"google.protobuf.Int32Value\",\n                    id: 12\n                  }\n                },\n                nested: {\n                  DocumentsTarget: {\n                    fields: {\n                      documents: {\n                        rule: \"repeated\",\n                        type: \"string\",\n                        id: 2\n                      }\n                    }\n                  },\n                  QueryTarget: {\n                    oneofs: {\n                      queryType: {\n                        oneof: [\"structuredQuery\"]\n                      }\n                    },\n                    fields: {\n                      parent: {\n                        type: \"string\",\n                        id: 1\n                      },\n                      structuredQuery: {\n                        type: \"StructuredQuery\",\n                        id: 2\n                      }\n                    }\n                  }\n                }\n              },\n              TargetChange: {\n                fields: {\n                  targetChangeType: {\n                    type: \"TargetChangeType\",\n                    id: 1\n                  },\n                  targetIds: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 2\n                  },\n                  cause: {\n                    type: \"google.rpc.Status\",\n                    id: 3\n                  },\n                  resumeToken: {\n                    type: \"bytes\",\n                    id: 4\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 6\n                  }\n                },\n                nested: {\n                  TargetChangeType: {\n                    values: {\n                      NO_CHANGE: 0,\n                      ADD: 1,\n                      REMOVE: 2,\n                      CURRENT: 3,\n                      RESET: 4\n                    }\n                  }\n                }\n              },\n              ListCollectionIdsRequest: {\n                fields: {\n                  parent: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  pageSize: {\n                    type: \"int32\",\n                    id: 2\n                  },\n                  pageToken: {\n                    type: \"string\",\n                    id: 3\n                  }\n                }\n              },\n              ListCollectionIdsResponse: {\n                fields: {\n                  collectionIds: {\n                    rule: \"repeated\",\n                    type: \"string\",\n                    id: 1\n                  },\n                  nextPageToken: {\n                    type: \"string\",\n                    id: 2\n                  }\n                }\n              },\n              BatchWriteRequest: {\n                fields: {\n                  database: {\n                    type: \"string\",\n                    id: 1,\n                    options: {\n                      \"(google.api.field_behavior)\": \"REQUIRED\"\n                    }\n                  },\n                  writes: {\n                    rule: \"repeated\",\n                    type: \"Write\",\n                    id: 2\n                  },\n                  labels: {\n                    keyType: \"string\",\n                    type: \"string\",\n                    id: 3\n                  }\n                }\n              },\n              BatchWriteResponse: {\n                fields: {\n                  writeResults: {\n                    rule: \"repeated\",\n                    type: \"WriteResult\",\n                    id: 1\n                  },\n                  status: {\n                    rule: \"repeated\",\n                    type: \"google.rpc.Status\",\n                    id: 2\n                  }\n                }\n              },\n              StructuredQuery: {\n                fields: {\n                  select: {\n                    type: \"Projection\",\n                    id: 1\n                  },\n                  from: {\n                    rule: \"repeated\",\n                    type: \"CollectionSelector\",\n                    id: 2\n                  },\n                  where: {\n                    type: \"Filter\",\n                    id: 3\n                  },\n                  orderBy: {\n                    rule: \"repeated\",\n                    type: \"Order\",\n                    id: 4\n                  },\n                  startAt: {\n                    type: \"Cursor\",\n                    id: 7\n                  },\n                  endAt: {\n                    type: \"Cursor\",\n                    id: 8\n                  },\n                  offset: {\n                    type: \"int32\",\n                    id: 6\n                  },\n                  limit: {\n                    type: \"google.protobuf.Int32Value\",\n                    id: 5\n                  }\n                },\n                nested: {\n                  CollectionSelector: {\n                    fields: {\n                      collectionId: {\n                        type: \"string\",\n                        id: 2\n                      },\n                      allDescendants: {\n                        type: \"bool\",\n                        id: 3\n                      }\n                    }\n                  },\n                  Filter: {\n                    oneofs: {\n                      filterType: {\n                        oneof: [\"compositeFilter\", \"fieldFilter\", \"unaryFilter\"]\n                      }\n                    },\n                    fields: {\n                      compositeFilter: {\n                        type: \"CompositeFilter\",\n                        id: 1\n                      },\n                      fieldFilter: {\n                        type: \"FieldFilter\",\n                        id: 2\n                      },\n                      unaryFilter: {\n                        type: \"UnaryFilter\",\n                        id: 3\n                      }\n                    }\n                  },\n                  CompositeFilter: {\n                    fields: {\n                      op: {\n                        type: \"Operator\",\n                        id: 1\n                      },\n                      filters: {\n                        rule: \"repeated\",\n                        type: \"Filter\",\n                        id: 2\n                      }\n                    },\n                    nested: {\n                      Operator: {\n                        values: {\n                          OPERATOR_UNSPECIFIED: 0,\n                          AND: 1,\n                          OR: 2\n                        }\n                      }\n                    }\n                  },\n                  FieldFilter: {\n                    fields: {\n                      field: {\n                        type: \"FieldReference\",\n                        id: 1\n                      },\n                      op: {\n                        type: \"Operator\",\n                        id: 2\n                      },\n                      value: {\n                        type: \"Value\",\n                        id: 3\n                      }\n                    },\n                    nested: {\n                      Operator: {\n                        values: {\n                          OPERATOR_UNSPECIFIED: 0,\n                          LESS_THAN: 1,\n                          LESS_THAN_OR_EQUAL: 2,\n                          GREATER_THAN: 3,\n                          GREATER_THAN_OR_EQUAL: 4,\n                          EQUAL: 5,\n                          NOT_EQUAL: 6,\n                          ARRAY_CONTAINS: 7,\n                          IN: 8,\n                          ARRAY_CONTAINS_ANY: 9,\n                          NOT_IN: 10\n                        }\n                      }\n                    }\n                  },\n                  UnaryFilter: {\n                    oneofs: {\n                      operandType: {\n                        oneof: [\"field\"]\n                      }\n                    },\n                    fields: {\n                      op: {\n                        type: \"Operator\",\n                        id: 1\n                      },\n                      field: {\n                        type: \"FieldReference\",\n                        id: 2\n                      }\n                    },\n                    nested: {\n                      Operator: {\n                        values: {\n                          OPERATOR_UNSPECIFIED: 0,\n                          IS_NAN: 2,\n                          IS_NULL: 3,\n                          IS_NOT_NAN: 4,\n                          IS_NOT_NULL: 5\n                        }\n                      }\n                    }\n                  },\n                  Order: {\n                    fields: {\n                      field: {\n                        type: \"FieldReference\",\n                        id: 1\n                      },\n                      direction: {\n                        type: \"Direction\",\n                        id: 2\n                      }\n                    }\n                  },\n                  FieldReference: {\n                    fields: {\n                      fieldPath: {\n                        type: \"string\",\n                        id: 2\n                      }\n                    }\n                  },\n                  Projection: {\n                    fields: {\n                      fields: {\n                        rule: \"repeated\",\n                        type: \"FieldReference\",\n                        id: 2\n                      }\n                    }\n                  },\n                  Direction: {\n                    values: {\n                      DIRECTION_UNSPECIFIED: 0,\n                      ASCENDING: 1,\n                      DESCENDING: 2\n                    }\n                  }\n                }\n              },\n              StructuredAggregationQuery: {\n                oneofs: {\n                  queryType: {\n                    oneof: [\"structuredQuery\"]\n                  }\n                },\n                fields: {\n                  structuredQuery: {\n                    type: \"StructuredQuery\",\n                    id: 1\n                  },\n                  aggregations: {\n                    rule: \"repeated\",\n                    type: \"Aggregation\",\n                    id: 3\n                  }\n                },\n                nested: {\n                  Aggregation: {\n                    oneofs: {\n                      operator: {\n                        oneof: [\"count\", \"sum\", \"avg\"]\n                      }\n                    },\n                    fields: {\n                      count: {\n                        type: \"Count\",\n                        id: 1\n                      },\n                      sum: {\n                        type: \"Sum\",\n                        id: 2\n                      },\n                      avg: {\n                        type: \"Avg\",\n                        id: 3\n                      },\n                      alias: {\n                        type: \"string\",\n                        id: 7\n                      }\n                    },\n                    nested: {\n                      Count: {\n                        fields: {\n                          upTo: {\n                            type: \"google.protobuf.Int64Value\",\n                            id: 1\n                          }\n                        }\n                      },\n                      Sum: {\n                        fields: {\n                          field: {\n                            type: \"FieldReference\",\n                            id: 1\n                          }\n                        }\n                      },\n                      Avg: {\n                        fields: {\n                          field: {\n                            type: \"FieldReference\",\n                            id: 1\n                          }\n                        }\n                      }\n                    }\n                  }\n                }\n              },\n              Cursor: {\n                fields: {\n                  values: {\n                    rule: \"repeated\",\n                    type: \"Value\",\n                    id: 1\n                  },\n                  before: {\n                    type: \"bool\",\n                    id: 2\n                  }\n                }\n              },\n              Write: {\n                oneofs: {\n                  operation: {\n                    oneof: [\"update\", \"delete\", \"verify\", \"transform\"]\n                  }\n                },\n                fields: {\n                  update: {\n                    type: \"Document\",\n                    id: 1\n                  },\n                  \"delete\": {\n                    type: \"string\",\n                    id: 2\n                  },\n                  verify: {\n                    type: \"string\",\n                    id: 5\n                  },\n                  transform: {\n                    type: \"DocumentTransform\",\n                    id: 6\n                  },\n                  updateMask: {\n                    type: \"DocumentMask\",\n                    id: 3\n                  },\n                  updateTransforms: {\n                    rule: \"repeated\",\n                    type: \"DocumentTransform.FieldTransform\",\n                    id: 7\n                  },\n                  currentDocument: {\n                    type: \"Precondition\",\n                    id: 4\n                  }\n                }\n              },\n              DocumentTransform: {\n                fields: {\n                  document: {\n                    type: \"string\",\n                    id: 1\n                  },\n                  fieldTransforms: {\n                    rule: \"repeated\",\n                    type: \"FieldTransform\",\n                    id: 2\n                  }\n                },\n                nested: {\n                  FieldTransform: {\n                    oneofs: {\n                      transformType: {\n                        oneof: [\"setToServerValue\", \"increment\", \"maximum\", \"minimum\", \"appendMissingElements\", \"removeAllFromArray\"]\n                      }\n                    },\n                    fields: {\n                      fieldPath: {\n                        type: \"string\",\n                        id: 1\n                      },\n                      setToServerValue: {\n                        type: \"ServerValue\",\n                        id: 2\n                      },\n                      increment: {\n                        type: \"Value\",\n                        id: 3\n                      },\n                      maximum: {\n                        type: \"Value\",\n                        id: 4\n                      },\n                      minimum: {\n                        type: \"Value\",\n                        id: 5\n                      },\n                      appendMissingElements: {\n                        type: \"ArrayValue\",\n                        id: 6\n                      },\n                      removeAllFromArray: {\n                        type: \"ArrayValue\",\n                        id: 7\n                      }\n                    },\n                    nested: {\n                      ServerValue: {\n                        values: {\n                          SERVER_VALUE_UNSPECIFIED: 0,\n                          REQUEST_TIME: 1\n                        }\n                      }\n                    }\n                  }\n                }\n              },\n              WriteResult: {\n                fields: {\n                  updateTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 1\n                  },\n                  transformResults: {\n                    rule: \"repeated\",\n                    type: \"Value\",\n                    id: 2\n                  }\n                }\n              },\n              DocumentChange: {\n                fields: {\n                  document: {\n                    type: \"Document\",\n                    id: 1\n                  },\n                  targetIds: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 5\n                  },\n                  removedTargetIds: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 6\n                  }\n                }\n              },\n              DocumentDelete: {\n                fields: {\n                  document: {\n                    type: \"string\",\n                    id: 1\n                  },\n                  removedTargetIds: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 6\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 4\n                  }\n                }\n              },\n              DocumentRemove: {\n                fields: {\n                  document: {\n                    type: \"string\",\n                    id: 1\n                  },\n                  removedTargetIds: {\n                    rule: \"repeated\",\n                    type: \"int32\",\n                    id: 2\n                  },\n                  readTime: {\n                    type: \"google.protobuf.Timestamp\",\n                    id: 4\n                  }\n                }\n              },\n              ExistenceFilter: {\n                fields: {\n                  targetId: {\n                    type: \"int32\",\n                    id: 1\n                  },\n                  count: {\n                    type: \"int32\",\n                    id: 2\n                  },\n                  unchangedNames: {\n                    type: \"BloomFilter\",\n                    id: 3\n                  }\n                }\n              }\n            }\n          }\n        }\n      },\n      api: {\n        options: {\n          go_package: \"google.golang.org/genproto/googleapis/api/annotations;annotations\",\n          java_multiple_files: true,\n          java_outer_classname: \"HttpProto\",\n          java_package: \"com.google.api\",\n          objc_class_prefix: \"GAPI\",\n          cc_enable_arenas: true\n        },\n        nested: {\n          http: {\n            type: \"HttpRule\",\n            id: 72295728,\n            extend: \"google.protobuf.MethodOptions\"\n          },\n          Http: {\n            fields: {\n              rules: {\n                rule: \"repeated\",\n                type: \"HttpRule\",\n                id: 1\n              }\n            }\n          },\n          HttpRule: {\n            oneofs: {\n              pattern: {\n                oneof: [\"get\", \"put\", \"post\", \"delete\", \"patch\", \"custom\"]\n              }\n            },\n            fields: {\n              get: {\n                type: \"string\",\n                id: 2\n              },\n              put: {\n                type: \"string\",\n                id: 3\n              },\n              post: {\n                type: \"string\",\n                id: 4\n              },\n              \"delete\": {\n                type: \"string\",\n                id: 5\n              },\n              patch: {\n                type: \"string\",\n                id: 6\n              },\n              custom: {\n                type: \"CustomHttpPattern\",\n                id: 8\n              },\n              selector: {\n                type: \"string\",\n                id: 1\n              },\n              body: {\n                type: \"string\",\n                id: 7\n              },\n              additionalBindings: {\n                rule: \"repeated\",\n                type: \"HttpRule\",\n                id: 11\n              }\n            }\n          },\n          CustomHttpPattern: {\n            fields: {\n              kind: {\n                type: \"string\",\n                id: 1\n              },\n              path: {\n                type: \"string\",\n                id: 2\n              }\n            }\n          },\n          methodSignature: {\n            rule: \"repeated\",\n            type: \"string\",\n            id: 1051,\n            extend: \"google.protobuf.MethodOptions\"\n          },\n          defaultHost: {\n            type: \"string\",\n            id: 1049,\n            extend: \"google.protobuf.ServiceOptions\"\n          },\n          oauthScopes: {\n            type: \"string\",\n            id: 1050,\n            extend: \"google.protobuf.ServiceOptions\"\n          },\n          fieldBehavior: {\n            rule: \"repeated\",\n            type: \"google.api.FieldBehavior\",\n            id: 1052,\n            extend: \"google.protobuf.FieldOptions\"\n          },\n          FieldBehavior: {\n            values: {\n              FIELD_BEHAVIOR_UNSPECIFIED: 0,\n              OPTIONAL: 1,\n              REQUIRED: 2,\n              OUTPUT_ONLY: 3,\n              INPUT_ONLY: 4,\n              IMMUTABLE: 5,\n              UNORDERED_LIST: 6,\n              NON_EMPTY_DEFAULT: 7\n            }\n          }\n        }\n      },\n      type: {\n        options: {\n          cc_enable_arenas: true,\n          go_package: \"google.golang.org/genproto/googleapis/type/latlng;latlng\",\n          java_multiple_files: true,\n          java_outer_classname: \"LatLngProto\",\n          java_package: \"com.google.type\",\n          objc_class_prefix: \"GTP\"\n        },\n        nested: {\n          LatLng: {\n            fields: {\n              latitude: {\n                type: \"double\",\n                id: 1\n              },\n              longitude: {\n                type: \"double\",\n                id: 2\n              }\n            }\n          }\n        }\n      },\n      rpc: {\n        options: {\n          cc_enable_arenas: true,\n          go_package: \"google.golang.org/genproto/googleapis/rpc/status;status\",\n          java_multiple_files: true,\n          java_outer_classname: \"StatusProto\",\n          java_package: \"com.google.rpc\",\n          objc_class_prefix: \"RPC\"\n        },\n        nested: {\n          Status: {\n            fields: {\n              code: {\n                type: \"int32\",\n                id: 1\n              },\n              message: {\n                type: \"string\",\n                id: 2\n              },\n              details: {\n                rule: \"repeated\",\n                type: \"google.protobuf.Any\",\n                id: 3\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n};\nvar protos = {\n  nested: nested\n};\nvar protos$1 = /*#__PURE__*/Object.freeze({\n  __proto__: null,\n  nested: nested,\n  'default': protos\n});\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Used by tests so we can match @grpc/proto-loader behavior. */\n\nconst protoLoaderOptions = {\n  longs: String,\n  enums: String,\n  defaults: true,\n  oneofs: false\n};\n/**\r\n * Loads the protocol buffer definitions for Firestore.\r\n *\r\n * @returns The GrpcObject representing our protos.\r\n */\n\nfunction loadProtos() {\n  const packageDefinition = protoLoader.fromJSON(protos$1, protoLoaderOptions);\n  return grpc.loadPackageDefinition(packageDefinition);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** Loads the GRPC stack */\n\n\nfunction newConnection(databaseInfo) {\n  const protos = loadProtos();\n  return new GrpcConnection(protos, databaseInfo);\n}\n/** Return the Platform-specific connectivity monitor. */\n\n\nfunction newConnectivityMonitor() {\n  return new NoopConnectivityMonitor();\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/** The Platform's 'window' implementation or null if not available. */\n\n\nfunction getWindow() {\n  if (process.env.USE_MOCK_PERSISTENCE === 'YES') {\n    // eslint-disable-next-line no-restricted-globals\n    return window;\n  }\n\n  return null;\n}\n/** The Platform's 'document' implementation or null if not available. */\n\n\nfunction getDocument() {\n  return null;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction newSerializer(databaseId) {\n  return new JsonProtoSerializer(databaseId,\n  /* useProto3Json= */\n  false);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$8 = 'ExponentialBackoff';\n/**\r\n * Initial backoff time in milliseconds after an error.\r\n * Set to 1s according to https://cloud.google.com/apis/design/errors.\r\n */\n\nconst DEFAULT_BACKOFF_INITIAL_DELAY_MS = 1000;\nconst DEFAULT_BACKOFF_FACTOR = 1.5;\n/** Maximum backoff time in milliseconds */\n\nconst DEFAULT_BACKOFF_MAX_DELAY_MS = 60 * 1000;\n/**\r\n * A helper for running delayed tasks following an exponential backoff curve\r\n * between attempts.\r\n *\r\n * Each delay is made up of a \"base\" delay which follows the exponential\r\n * backoff curve, and a +/- 50% \"jitter\" that is calculated and added to the\r\n * base delay. This prevents clients from accidentally synchronizing their\r\n * delays causing spikes of load to the backend.\r\n */\n\nclass ExponentialBackoff {\n  constructor(\n  /**\r\n   * The AsyncQueue to run backoff operations on.\r\n   */\n  queue,\n  /**\r\n   * The ID to use when scheduling backoff operations on the AsyncQueue.\r\n   */\n  timerId,\n  /**\r\n   * The initial delay (used as the base delay on the first retry attempt).\r\n   * Note that jitter will still be applied, so the actual delay could be as\r\n   * little as 0.5*initialDelayMs.\r\n   */\n  initialDelayMs = DEFAULT_BACKOFF_INITIAL_DELAY_MS,\n  /**\r\n   * The multiplier to use to determine the extended base delay after each\r\n   * attempt.\r\n   */\n  backoffFactor = DEFAULT_BACKOFF_FACTOR,\n  /**\r\n   * The maximum base delay after which no further backoff is performed.\r\n   * Note that jitter will still be applied, so the actual delay could be as\r\n   * much as 1.5*maxDelayMs.\r\n   */\n  maxDelayMs = DEFAULT_BACKOFF_MAX_DELAY_MS) {\n    this.queue = queue;\n    this.timerId = timerId;\n    this.initialDelayMs = initialDelayMs;\n    this.backoffFactor = backoffFactor;\n    this.maxDelayMs = maxDelayMs;\n    this.currentBaseMs = 0;\n    this.timerPromise = null;\n    /** The last backoff attempt, as epoch milliseconds. */\n\n    this.lastAttemptTime = Date.now();\n    this.reset();\n  }\n  /**\r\n   * Resets the backoff delay.\r\n   *\r\n   * The very next backoffAndWait() will have no delay. If it is called again\r\n   * (i.e. due to an error), initialDelayMs (plus jitter) will be used, and\r\n   * subsequent ones will increase according to the backoffFactor.\r\n   */\n\n\n  reset() {\n    this.currentBaseMs = 0;\n  }\n  /**\r\n   * Resets the backoff delay to the maximum delay (e.g. for use after a\r\n   * RESOURCE_EXHAUSTED error).\r\n   */\n\n\n  resetToMax() {\n    this.currentBaseMs = this.maxDelayMs;\n  }\n  /**\r\n   * Returns a promise that resolves after currentDelayMs, and increases the\r\n   * delay for any subsequent attempts. If there was a pending backoff operation\r\n   * already, it will be canceled.\r\n   */\n\n\n  backoffAndRun(op) {\n    // Cancel any pending backoff operation.\n    this.cancel(); // First schedule using the current base (which may be 0 and should be\n    // honored as such).\n\n    const desiredDelayWithJitterMs = Math.floor(this.currentBaseMs + this.jitterDelayMs()); // Guard against lastAttemptTime being in the future due to a clock change.\n\n    const delaySoFarMs = Math.max(0, Date.now() - this.lastAttemptTime); // Guard against the backoff delay already being past.\n\n    const remainingDelayMs = Math.max(0, desiredDelayWithJitterMs - delaySoFarMs);\n\n    if (remainingDelayMs > 0) {\n      logDebug(LOG_TAG$8, `Backing off for ${remainingDelayMs} ms ` + `(base delay: ${this.currentBaseMs} ms, ` + `delay with jitter: ${desiredDelayWithJitterMs} ms, ` + `last attempt: ${delaySoFarMs} ms ago)`);\n    }\n\n    this.timerPromise = this.queue.enqueueAfterDelay(this.timerId, remainingDelayMs, () => {\n      this.lastAttemptTime = Date.now();\n      return op();\n    }); // Apply backoff factor to determine next delay and ensure it is within\n    // bounds.\n\n    this.currentBaseMs *= this.backoffFactor;\n\n    if (this.currentBaseMs < this.initialDelayMs) {\n      this.currentBaseMs = this.initialDelayMs;\n    }\n\n    if (this.currentBaseMs > this.maxDelayMs) {\n      this.currentBaseMs = this.maxDelayMs;\n    }\n  }\n\n  skipBackoff() {\n    if (this.timerPromise !== null) {\n      this.timerPromise.skipDelay();\n      this.timerPromise = null;\n    }\n  }\n\n  cancel() {\n    if (this.timerPromise !== null) {\n      this.timerPromise.cancel();\n      this.timerPromise = null;\n    }\n  }\n  /** Returns a random value in the range [-currentBaseMs/2, currentBaseMs/2] */\n\n\n  jitterDelayMs() {\n    return (Math.random() - 0.5) * this.currentBaseMs;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$7 = 'PersistentStream';\n/** The time a stream stays open after it is marked idle. */\n\nconst IDLE_TIMEOUT_MS = 60 * 1000;\n/** The time a stream stays open until we consider it healthy. */\n\nconst HEALTHY_TIMEOUT_MS = 10 * 1000;\n/**\r\n * A PersistentStream is an abstract base class that represents a streaming RPC\r\n * to the Firestore backend. It's built on top of the connections own support\r\n * for streaming RPCs, and adds several critical features for our clients:\r\n *\r\n *   - Exponential backoff on failure\r\n *   - Authentication via CredentialsProvider\r\n *   - Dispatching all callbacks into the shared worker queue\r\n *   - Closing idle streams after 60 seconds of inactivity\r\n *\r\n * Subclasses of PersistentStream implement serialization of models to and\r\n * from the JSON representation of the protocol buffers for a specific\r\n * streaming RPC.\r\n *\r\n * ## Starting and Stopping\r\n *\r\n * Streaming RPCs are stateful and need to be start()ed before messages can\r\n * be sent and received. The PersistentStream will call the onOpen() function\r\n * of the listener once the stream is ready to accept requests.\r\n *\r\n * Should a start() fail, PersistentStream will call the registered onClose()\r\n * listener with a FirestoreError indicating what went wrong.\r\n *\r\n * A PersistentStream can be started and stopped repeatedly.\r\n *\r\n * Generic types:\r\n *  SendType: The type of the outgoing message of the underlying\r\n *    connection stream\r\n *  ReceiveType: The type of the incoming message of the underlying\r\n *    connection stream\r\n *  ListenerType: The type of the listener that will be used for callbacks\r\n */\n\nclass PersistentStream {\n  constructor(queue, connectionTimerId, idleTimerId, healthTimerId, connection, authCredentialsProvider, appCheckCredentialsProvider, listener) {\n    this.queue = queue;\n    this.idleTimerId = idleTimerId;\n    this.healthTimerId = healthTimerId;\n    this.connection = connection;\n    this.authCredentialsProvider = authCredentialsProvider;\n    this.appCheckCredentialsProvider = appCheckCredentialsProvider;\n    this.listener = listener;\n    this.state = 0\n    /* PersistentStreamState.Initial */\n    ;\n    /**\r\n     * A close count that's incremented every time the stream is closed; used by\r\n     * getCloseGuardedDispatcher() to invalidate callbacks that happen after\r\n     * close.\r\n     */\n\n    this.closeCount = 0;\n    this.idleTimer = null;\n    this.healthCheck = null;\n    this.stream = null;\n    this.backoff = new ExponentialBackoff(queue, connectionTimerId);\n  }\n  /**\r\n   * Returns true if start() has been called and no error has occurred. True\r\n   * indicates the stream is open or in the process of opening (which\r\n   * encompasses respecting backoff, getting auth tokens, and starting the\r\n   * actual RPC). Use isOpen() to determine if the stream is open and ready for\r\n   * outbound requests.\r\n   */\n\n\n  isStarted() {\n    return this.state === 1\n    /* PersistentStreamState.Starting */\n    || this.state === 5\n    /* PersistentStreamState.Backoff */\n    || this.isOpen();\n  }\n  /**\r\n   * Returns true if the underlying RPC is open (the onOpen() listener has been\r\n   * called) and the stream is ready for outbound requests.\r\n   */\n\n\n  isOpen() {\n    return this.state === 2\n    /* PersistentStreamState.Open */\n    || this.state === 3\n    /* PersistentStreamState.Healthy */\n    ;\n  }\n  /**\r\n   * Starts the RPC. Only allowed if isStarted() returns false. The stream is\r\n   * not immediately ready for use: onOpen() will be invoked when the RPC is\r\n   * ready for outbound requests, at which point isOpen() will return true.\r\n   *\r\n   * When start returns, isStarted() will return true.\r\n   */\n\n\n  start() {\n    if (this.state === 4\n    /* PersistentStreamState.Error */\n    ) {\n      this.performBackoff();\n      return;\n    }\n\n    this.auth();\n  }\n  /**\r\n   * Stops the RPC. This call is idempotent and allowed regardless of the\r\n   * current isStarted() state.\r\n   *\r\n   * When stop returns, isStarted() and isOpen() will both return false.\r\n   */\n\n\n  stop() {\n    var _this14 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this14.isStarted()) {\n        yield _this14.close(0\n        /* PersistentStreamState.Initial */\n        );\n      }\n    })();\n  }\n  /**\r\n   * After an error the stream will usually back off on the next attempt to\r\n   * start it. If the error warrants an immediate restart of the stream, the\r\n   * sender can use this to indicate that the receiver should not back off.\r\n   *\r\n   * Each error will call the onClose() listener. That function can decide to\r\n   * inhibit backoff if required.\r\n   */\n\n\n  inhibitBackoff() {\n    this.state = 0\n    /* PersistentStreamState.Initial */\n    ;\n    this.backoff.reset();\n  }\n  /**\r\n   * Marks this stream as idle. If no further actions are performed on the\r\n   * stream for one minute, the stream will automatically close itself and\r\n   * notify the stream's onClose() handler with Status.OK. The stream will then\r\n   * be in a !isStarted() state, requiring the caller to start the stream again\r\n   * before further use.\r\n   *\r\n   * Only streams that are in state 'Open' can be marked idle, as all other\r\n   * states imply pending network operations.\r\n   */\n\n\n  markIdle() {\n    // Starts the idle time if we are in state 'Open' and are not yet already\n    // running a timer (in which case the previous idle timeout still applies).\n    if (this.isOpen() && this.idleTimer === null) {\n      this.idleTimer = this.queue.enqueueAfterDelay(this.idleTimerId, IDLE_TIMEOUT_MS, () => this.handleIdleCloseTimer());\n    }\n  }\n  /** Sends a message to the underlying stream. */\n\n\n  sendRequest(msg) {\n    this.cancelIdleCheck();\n    this.stream.send(msg);\n  }\n  /** Called by the idle timer when the stream should close due to inactivity. */\n\n\n  handleIdleCloseTimer() {\n    var _this15 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this15.isOpen()) {\n        // When timing out an idle stream there's no reason to force the stream into backoff when\n        // it restarts so set the stream state to Initial instead of Error.\n        return _this15.close(0\n        /* PersistentStreamState.Initial */\n        );\n      }\n    })();\n  }\n  /** Marks the stream as active again. */\n\n\n  cancelIdleCheck() {\n    if (this.idleTimer) {\n      this.idleTimer.cancel();\n      this.idleTimer = null;\n    }\n  }\n  /** Cancels the health check delayed operation. */\n\n\n  cancelHealthCheck() {\n    if (this.healthCheck) {\n      this.healthCheck.cancel();\n      this.healthCheck = null;\n    }\n  }\n  /**\r\n   * Closes the stream and cleans up as necessary:\r\n   *\r\n   * * closes the underlying GRPC stream;\r\n   * * calls the onClose handler with the given 'error';\r\n   * * sets internal stream state to 'finalState';\r\n   * * adjusts the backoff timer based on the error\r\n   *\r\n   * A new stream can be opened by calling start().\r\n   *\r\n   * @param finalState - the intended state of the stream after closing.\r\n   * @param error - the error the connection was closed with.\r\n   */\n\n\n  close(finalState, error) {\n    var _this16 = this;\n\n    return _asyncToGenerator(function* () {\n      // Cancel any outstanding timers (they're guaranteed not to execute).\n      _this16.cancelIdleCheck();\n\n      _this16.cancelHealthCheck();\n\n      _this16.backoff.cancel(); // Invalidates any stream-related callbacks (e.g. from auth or the\n      // underlying stream), guaranteeing they won't execute.\n\n\n      _this16.closeCount++;\n\n      if (finalState !== 4\n      /* PersistentStreamState.Error */\n      ) {\n        // If this is an intentional close ensure we don't delay our next connection attempt.\n        _this16.backoff.reset();\n      } else if (error && error.code === Code.RESOURCE_EXHAUSTED) {\n        // Log the error. (Probably either 'quota exceeded' or 'max queue length reached'.)\n        logError(error.toString());\n        logError('Using maximum backoff delay to prevent overloading the backend.');\n\n        _this16.backoff.resetToMax();\n      } else if (error && error.code === Code.UNAUTHENTICATED && _this16.state !== 3\n      /* PersistentStreamState.Healthy */\n      ) {\n        // \"unauthenticated\" error means the token was rejected. This should rarely\n        // happen since both Auth and AppCheck ensure a sufficient TTL when we\n        // request a token. If a user manually resets their system clock this can\n        // fail, however. In this case, we should get a Code.UNAUTHENTICATED error\n        // before we received the first message and we need to invalidate the token\n        // to ensure that we fetch a new token.\n        _this16.authCredentialsProvider.invalidateToken();\n\n        _this16.appCheckCredentialsProvider.invalidateToken();\n      } // Clean up the underlying stream because we are no longer interested in events.\n\n\n      if (_this16.stream !== null) {\n        _this16.tearDown();\n\n        _this16.stream.close();\n\n        _this16.stream = null;\n      } // This state must be assigned before calling onClose() to allow the callback to\n      // inhibit backoff or otherwise manipulate the state in its non-started state.\n\n\n      _this16.state = finalState; // Notify the listener that the stream closed.\n\n      yield _this16.listener.onClose(error);\n    })();\n  }\n  /**\r\n   * Can be overridden to perform additional cleanup before the stream is closed.\r\n   * Calling super.tearDown() is not required.\r\n   */\n\n\n  tearDown() {}\n\n  auth() {\n    this.state = 1\n    /* PersistentStreamState.Starting */\n    ;\n    const dispatchIfNotClosed = this.getCloseGuardedDispatcher(this.closeCount); // TODO(mikelehen): Just use dispatchIfNotClosed, but see TODO below.\n\n    const closeCount = this.closeCount;\n    Promise.all([this.authCredentialsProvider.getToken(), this.appCheckCredentialsProvider.getToken()]).then(([authToken, appCheckToken]) => {\n      // Stream can be stopped while waiting for authentication.\n      // TODO(mikelehen): We really should just use dispatchIfNotClosed\n      // and let this dispatch onto the queue, but that opened a spec test can\n      // of worms that I don't want to deal with in this PR.\n      if (this.closeCount === closeCount) {\n        // Normally we'd have to schedule the callback on the AsyncQueue.\n        // However, the following calls are safe to be called outside the\n        // AsyncQueue since they don't chain asynchronous calls\n        this.startStream(authToken, appCheckToken);\n      }\n    }, error => {\n      dispatchIfNotClosed(() => {\n        const rpcError = new FirestoreError(Code.UNKNOWN, 'Fetching auth token failed: ' + error.message);\n        return this.handleStreamClose(rpcError);\n      });\n    });\n  }\n\n  startStream(authToken, appCheckToken) {\n    const dispatchIfNotClosed = this.getCloseGuardedDispatcher(this.closeCount);\n    this.stream = this.startRpc(authToken, appCheckToken);\n    this.stream.onOpen(() => {\n      dispatchIfNotClosed(() => {\n        this.state = 2\n        /* PersistentStreamState.Open */\n        ;\n        this.healthCheck = this.queue.enqueueAfterDelay(this.healthTimerId, HEALTHY_TIMEOUT_MS, () => {\n          if (this.isOpen()) {\n            this.state = 3\n            /* PersistentStreamState.Healthy */\n            ;\n          }\n\n          return Promise.resolve();\n        });\n        return this.listener.onOpen();\n      });\n    });\n    this.stream.onClose(error => {\n      dispatchIfNotClosed(() => {\n        return this.handleStreamClose(error);\n      });\n    });\n    this.stream.onMessage(msg => {\n      dispatchIfNotClosed(() => {\n        return this.onMessage(msg);\n      });\n    });\n  }\n\n  performBackoff() {\n    var _this17 = this;\n\n    this.state = 5\n    /* PersistentStreamState.Backoff */\n    ;\n    this.backoff.backoffAndRun( /*#__PURE__*/_asyncToGenerator(function* () {\n      _this17.state = 0\n      /* PersistentStreamState.Initial */\n      ;\n\n      _this17.start();\n    }));\n  } // Visible for tests\n\n\n  handleStreamClose(error) {\n    logDebug(LOG_TAG$7, `close with error: ${error}`);\n    this.stream = null; // In theory the stream could close cleanly, however, in our current model\n    // we never expect this to happen because if we stop a stream ourselves,\n    // this callback will never be called. To prevent cases where we retry\n    // without a backoff accidentally, we set the stream to error in all cases.\n\n    return this.close(4\n    /* PersistentStreamState.Error */\n    , error);\n  }\n  /**\r\n   * Returns a \"dispatcher\" function that dispatches operations onto the\r\n   * AsyncQueue but only runs them if closeCount remains unchanged. This allows\r\n   * us to turn auth / stream callbacks into no-ops if the stream is closed /\r\n   * re-opened, etc.\r\n   */\n\n\n  getCloseGuardedDispatcher(startCloseCount) {\n    return fn => {\n      this.queue.enqueueAndForget(() => {\n        if (this.closeCount === startCloseCount) {\n          return fn();\n        } else {\n          logDebug(LOG_TAG$7, 'stream callback skipped by getCloseGuardedDispatcher.');\n          return Promise.resolve();\n        }\n      });\n    };\n  }\n\n}\n/**\r\n * A PersistentStream that implements the Listen RPC.\r\n *\r\n * Once the Listen stream has called the onOpen() listener, any number of\r\n * listen() and unlisten() calls can be made to control what changes will be\r\n * sent from the server for ListenResponses.\r\n */\n\n\nclass PersistentListenStream extends PersistentStream {\n  constructor(queue, connection, authCredentials, appCheckCredentials, serializer, listener) {\n    super(queue, \"listen_stream_connection_backoff\"\n    /* TimerId.ListenStreamConnectionBackoff */\n    , \"listen_stream_idle\"\n    /* TimerId.ListenStreamIdle */\n    , \"health_check_timeout\"\n    /* TimerId.HealthCheckTimeout */\n    , connection, authCredentials, appCheckCredentials, listener);\n    this.serializer = serializer;\n  }\n\n  startRpc(authToken, appCheckToken) {\n    return this.connection.openStream('Listen', authToken, appCheckToken);\n  }\n\n  onMessage(watchChangeProto) {\n    // A successful response means the stream is healthy\n    this.backoff.reset();\n    const watchChange = fromWatchChange(this.serializer, watchChangeProto);\n    const snapshot = versionFromListenResponse(watchChangeProto);\n    return this.listener.onWatchChange(watchChange, snapshot);\n  }\n  /**\r\n   * Registers interest in the results of the given target. If the target\r\n   * includes a resumeToken it will be included in the request. Results that\r\n   * affect the target will be streamed back as WatchChange messages that\r\n   * reference the targetId.\r\n   */\n\n\n  watch(targetData) {\n    const request = {};\n    request.database = getEncodedDatabaseId(this.serializer);\n    request.addTarget = toTarget(this.serializer, targetData);\n    const labels = toListenRequestLabels(this.serializer, targetData);\n\n    if (labels) {\n      request.labels = labels;\n    }\n\n    this.sendRequest(request);\n  }\n  /**\r\n   * Unregisters interest in the results of the target associated with the\r\n   * given targetId.\r\n   */\n\n\n  unwatch(targetId) {\n    const request = {};\n    request.database = getEncodedDatabaseId(this.serializer);\n    request.removeTarget = targetId;\n    this.sendRequest(request);\n  }\n\n}\n/**\r\n * A Stream that implements the Write RPC.\r\n *\r\n * The Write RPC requires the caller to maintain special streamToken\r\n * state in between calls, to help the server understand which responses the\r\n * client has processed by the time the next request is made. Every response\r\n * will contain a streamToken; this value must be passed to the next\r\n * request.\r\n *\r\n * After calling start() on this stream, the next request must be a handshake,\r\n * containing whatever streamToken is on hand. Once a response to this\r\n * request is received, all pending mutations may be submitted. When\r\n * submitting multiple batches of mutations at the same time, it's\r\n * okay to use the same streamToken for the calls to writeMutations.\r\n *\r\n * TODO(b/33271235): Use proto types\r\n */\n\n\nclass PersistentWriteStream extends PersistentStream {\n  constructor(queue, connection, authCredentials, appCheckCredentials, serializer, listener) {\n    super(queue, \"write_stream_connection_backoff\"\n    /* TimerId.WriteStreamConnectionBackoff */\n    , \"write_stream_idle\"\n    /* TimerId.WriteStreamIdle */\n    , \"health_check_timeout\"\n    /* TimerId.HealthCheckTimeout */\n    , connection, authCredentials, appCheckCredentials, listener);\n    this.serializer = serializer;\n    this.handshakeComplete_ = false;\n  }\n  /**\r\n   * Tracks whether or not a handshake has been successfully exchanged and\r\n   * the stream is ready to accept mutations.\r\n   */\n\n\n  get handshakeComplete() {\n    return this.handshakeComplete_;\n  } // Override of PersistentStream.start\n\n\n  start() {\n    this.handshakeComplete_ = false;\n    this.lastStreamToken = undefined;\n    super.start();\n  }\n\n  tearDown() {\n    if (this.handshakeComplete_) {\n      this.writeMutations([]);\n    }\n  }\n\n  startRpc(authToken, appCheckToken) {\n    return this.connection.openStream('Write', authToken, appCheckToken);\n  }\n\n  onMessage(responseProto) {\n    // Always capture the last stream token.\n    hardAssert(!!responseProto.streamToken);\n    this.lastStreamToken = responseProto.streamToken;\n\n    if (!this.handshakeComplete_) {\n      // The first response is always the handshake response\n      hardAssert(!responseProto.writeResults || responseProto.writeResults.length === 0);\n      this.handshakeComplete_ = true;\n      return this.listener.onHandshakeComplete();\n    } else {\n      // A successful first write response means the stream is healthy,\n      // Note, that we could consider a successful handshake healthy, however,\n      // the write itself might be causing an error we want to back off from.\n      this.backoff.reset();\n      const results = fromWriteResults(responseProto.writeResults, responseProto.commitTime);\n      const commitVersion = fromVersion(responseProto.commitTime);\n      return this.listener.onMutationResult(commitVersion, results);\n    }\n  }\n  /**\r\n   * Sends an initial streamToken to the server, performing the handshake\r\n   * required to make the StreamingWrite RPC work. Subsequent\r\n   * calls should wait until onHandshakeComplete was called.\r\n   */\n\n\n  writeHandshake() {\n    // TODO(dimond): Support stream resumption. We intentionally do not set the\n    // stream token on the handshake, ignoring any stream token we might have.\n    const request = {};\n    request.database = getEncodedDatabaseId(this.serializer);\n    this.sendRequest(request);\n  }\n  /** Sends a group of mutations to the Firestore backend to apply. */\n\n\n  writeMutations(mutations) {\n    const request = {\n      streamToken: this.lastStreamToken,\n      writes: mutations.map(mutation => toMutation(this.serializer, mutation))\n    };\n    this.sendRequest(request);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Datastore and its related methods are a wrapper around the external Google\r\n * Cloud Datastore grpc API, which provides an interface that is more convenient\r\n * for the rest of the client SDK architecture to consume.\r\n */\n\n\nclass Datastore {}\n/**\r\n * An implementation of Datastore that exposes additional state for internal\r\n * consumption.\r\n */\n\n\nclass DatastoreImpl extends Datastore {\n  constructor(authCredentials, appCheckCredentials, connection, serializer) {\n    super();\n    this.authCredentials = authCredentials;\n    this.appCheckCredentials = appCheckCredentials;\n    this.connection = connection;\n    this.serializer = serializer;\n    this.terminated = false;\n  }\n\n  verifyInitialized() {\n    if (this.terminated) {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, 'The client has already been terminated.');\n    }\n  }\n  /** Invokes the provided RPC with auth and AppCheck tokens. */\n\n\n  invokeRPC(rpcName, path, request) {\n    this.verifyInitialized();\n    return Promise.all([this.authCredentials.getToken(), this.appCheckCredentials.getToken()]).then(([authToken, appCheckToken]) => {\n      return this.connection.invokeRPC(rpcName, path, request, authToken, appCheckToken);\n    }).catch(error => {\n      if (error.name === 'FirebaseError') {\n        if (error.code === Code.UNAUTHENTICATED) {\n          this.authCredentials.invalidateToken();\n          this.appCheckCredentials.invalidateToken();\n        }\n\n        throw error;\n      } else {\n        throw new FirestoreError(Code.UNKNOWN, error.toString());\n      }\n    });\n  }\n  /** Invokes the provided RPC with streamed results with auth and AppCheck tokens. */\n\n\n  invokeStreamingRPC(rpcName, path, request, expectedResponseCount) {\n    this.verifyInitialized();\n    return Promise.all([this.authCredentials.getToken(), this.appCheckCredentials.getToken()]).then(([authToken, appCheckToken]) => {\n      return this.connection.invokeStreamingRPC(rpcName, path, request, authToken, appCheckToken, expectedResponseCount);\n    }).catch(error => {\n      if (error.name === 'FirebaseError') {\n        if (error.code === Code.UNAUTHENTICATED) {\n          this.authCredentials.invalidateToken();\n          this.appCheckCredentials.invalidateToken();\n        }\n\n        throw error;\n      } else {\n        throw new FirestoreError(Code.UNKNOWN, error.toString());\n      }\n    });\n  }\n\n  terminate() {\n    this.terminated = true;\n  }\n\n} // TODO(firestorexp): Make sure there is only one Datastore instance per\n// firestore-exp client.\n\n\nfunction newDatastore(authCredentials, appCheckCredentials, connection, serializer) {\n  return new DatastoreImpl(authCredentials, appCheckCredentials, connection, serializer);\n}\n\nfunction invokeCommitRpc(_x20, _x21) {\n  return _invokeCommitRpc.apply(this, arguments);\n}\n\nfunction _invokeCommitRpc() {\n  _invokeCommitRpc = _asyncToGenerator(function* (datastore, mutations) {\n    const datastoreImpl = debugCast(datastore);\n    const path = getEncodedDatabaseId(datastoreImpl.serializer) + '/documents';\n    const request = {\n      writes: mutations.map(m => toMutation(datastoreImpl.serializer, m))\n    };\n    yield datastoreImpl.invokeRPC('Commit', path, request);\n  });\n  return _invokeCommitRpc.apply(this, arguments);\n}\n\nfunction invokeBatchGetDocumentsRpc(_x22, _x23) {\n  return _invokeBatchGetDocumentsRpc.apply(this, arguments);\n}\n\nfunction _invokeBatchGetDocumentsRpc() {\n  _invokeBatchGetDocumentsRpc = _asyncToGenerator(function* (datastore, keys) {\n    const datastoreImpl = debugCast(datastore);\n    const path = getEncodedDatabaseId(datastoreImpl.serializer) + '/documents';\n    const request = {\n      documents: keys.map(k => toName(datastoreImpl.serializer, k))\n    };\n    const response = yield datastoreImpl.invokeStreamingRPC('BatchGetDocuments', path, request, keys.length);\n    const docs = new Map();\n    response.forEach(proto => {\n      const doc = fromBatchGetDocumentsResponse(datastoreImpl.serializer, proto);\n      docs.set(doc.key.toString(), doc);\n    });\n    const result = [];\n    keys.forEach(key => {\n      const doc = docs.get(key.toString());\n      hardAssert(!!doc);\n      result.push(doc);\n    });\n    return result;\n  });\n  return _invokeBatchGetDocumentsRpc.apply(this, arguments);\n}\n\nfunction invokeRunAggregationQueryRpc(_x24, _x25, _x26) {\n  return _invokeRunAggregationQueryRpc.apply(this, arguments);\n}\n\nfunction _invokeRunAggregationQueryRpc() {\n  _invokeRunAggregationQueryRpc = _asyncToGenerator(function* (datastore, query, aggregates) {\n    var _a;\n\n    const datastoreImpl = debugCast(datastore);\n    const {\n      request,\n      aliasMap\n    } = toRunAggregationQueryRequest(datastoreImpl.serializer, queryToTarget(query), aggregates);\n    const parent = request.parent;\n\n    if (!datastoreImpl.connection.shouldResourcePathBeIncludedInRequest) {\n      delete request.parent;\n    }\n\n    const response = yield datastoreImpl.invokeStreamingRPC('RunAggregationQuery', parent, request,\n    /*expectedResponseCount=*/\n    1); // Omit RunAggregationQueryResponse that only contain readTimes.\n\n    const filteredResult = response.filter(proto => !!proto.result);\n    hardAssert(filteredResult.length === 1); // Remap the short-form aliases that were sent to the server\n    // to the client-side aliases. Users will access the results\n    // using the client-side alias.\n\n    const unmappedAggregateFields = (_a = filteredResult[0].result) === null || _a === void 0 ? void 0 : _a.aggregateFields;\n    const remappedFields = Object.keys(unmappedAggregateFields).reduce((accumulator, key) => {\n      accumulator[aliasMap[key]] = unmappedAggregateFields[key];\n      return accumulator;\n    }, {});\n    return remappedFields;\n  });\n  return _invokeRunAggregationQueryRpc.apply(this, arguments);\n}\n\nfunction newPersistentWriteStream(datastore, queue, listener) {\n  const datastoreImpl = debugCast(datastore);\n  datastoreImpl.verifyInitialized();\n  return new PersistentWriteStream(queue, datastoreImpl.connection, datastoreImpl.authCredentials, datastoreImpl.appCheckCredentials, datastoreImpl.serializer, listener);\n}\n\nfunction newPersistentWatchStream(datastore, queue, listener) {\n  const datastoreImpl = debugCast(datastore);\n  datastoreImpl.verifyInitialized();\n  return new PersistentListenStream(queue, datastoreImpl.connection, datastoreImpl.authCredentials, datastoreImpl.appCheckCredentials, datastoreImpl.serializer, listener);\n}\n/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$6 = 'OnlineStateTracker'; // To deal with transient failures, we allow multiple stream attempts before\n// giving up and transitioning from OnlineState.Unknown to Offline.\n// TODO(mikelehen): This used to be set to 2 as a mitigation for b/66228394.\n// @jdimond thinks that bug is sufficiently fixed so that we can set this back\n// to 1. If that works okay, we could potentially remove this logic entirely.\n\nconst MAX_WATCH_STREAM_FAILURES = 1; // To deal with stream attempts that don't succeed or fail in a timely manner,\n// we have a timeout for OnlineState to reach Online or Offline.\n// If the timeout is reached, we transition to Offline rather than waiting\n// indefinitely.\n\nconst ONLINE_STATE_TIMEOUT_MS = 10 * 1000;\n/**\r\n * A component used by the RemoteStore to track the OnlineState (that is,\r\n * whether or not the client as a whole should be considered to be online or\r\n * offline), implementing the appropriate heuristics.\r\n *\r\n * In particular, when the client is trying to connect to the backend, we\r\n * allow up to MAX_WATCH_STREAM_FAILURES within ONLINE_STATE_TIMEOUT_MS for\r\n * a connection to succeed. If we have too many failures or the timeout elapses,\r\n * then we set the OnlineState to Offline, and the client will behave as if\r\n * it is offline (get()s will return cached data, etc.).\r\n */\n\nclass OnlineStateTracker {\n  constructor(asyncQueue, onlineStateHandler) {\n    this.asyncQueue = asyncQueue;\n    this.onlineStateHandler = onlineStateHandler;\n    /** The current OnlineState. */\n\n    this.state = \"Unknown\"\n    /* OnlineState.Unknown */\n    ;\n    /**\r\n     * A count of consecutive failures to open the stream. If it reaches the\r\n     * maximum defined by MAX_WATCH_STREAM_FAILURES, we'll set the OnlineState to\r\n     * Offline.\r\n     */\n\n    this.watchStreamFailures = 0;\n    /**\r\n     * A timer that elapses after ONLINE_STATE_TIMEOUT_MS, at which point we\r\n     * transition from OnlineState.Unknown to OnlineState.Offline without waiting\r\n     * for the stream to actually fail (MAX_WATCH_STREAM_FAILURES times).\r\n     */\n\n    this.onlineStateTimer = null;\n    /**\r\n     * Whether the client should log a warning message if it fails to connect to\r\n     * the backend (initially true, cleared after a successful stream, or if we've\r\n     * logged the message already).\r\n     */\n\n    this.shouldWarnClientIsOffline = true;\n  }\n  /**\r\n   * Called by RemoteStore when a watch stream is started (including on each\r\n   * backoff attempt).\r\n   *\r\n   * If this is the first attempt, it sets the OnlineState to Unknown and starts\r\n   * the onlineStateTimer.\r\n   */\n\n\n  handleWatchStreamStart() {\n    if (this.watchStreamFailures === 0) {\n      this.setAndBroadcast(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n      this.onlineStateTimer = this.asyncQueue.enqueueAfterDelay(\"online_state_timeout\"\n      /* TimerId.OnlineStateTimeout */\n      , ONLINE_STATE_TIMEOUT_MS, () => {\n        this.onlineStateTimer = null;\n        this.logClientOfflineWarningIfNecessary(`Backend didn't respond within ${ONLINE_STATE_TIMEOUT_MS / 1000} ` + `seconds.`);\n        this.setAndBroadcast(\"Offline\"\n        /* OnlineState.Offline */\n        ); // NOTE: handleWatchStreamFailure() will continue to increment\n        // watchStreamFailures even though we are already marked Offline,\n        // but this is non-harmful.\n\n        return Promise.resolve();\n      });\n    }\n  }\n  /**\r\n   * Updates our OnlineState as appropriate after the watch stream reports a\r\n   * failure. The first failure moves us to the 'Unknown' state. We then may\r\n   * allow multiple failures (based on MAX_WATCH_STREAM_FAILURES) before we\r\n   * actually transition to the 'Offline' state.\r\n   */\n\n\n  handleWatchStreamFailure(error) {\n    if (this.state === \"Online\"\n    /* OnlineState.Online */\n    ) {\n      this.setAndBroadcast(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n    } else {\n      this.watchStreamFailures++;\n\n      if (this.watchStreamFailures >= MAX_WATCH_STREAM_FAILURES) {\n        this.clearOnlineStateTimer();\n        this.logClientOfflineWarningIfNecessary(`Connection failed ${MAX_WATCH_STREAM_FAILURES} ` + `times. Most recent error: ${error.toString()}`);\n        this.setAndBroadcast(\"Offline\"\n        /* OnlineState.Offline */\n        );\n      }\n    }\n  }\n  /**\r\n   * Explicitly sets the OnlineState to the specified state.\r\n   *\r\n   * Note that this resets our timers / failure counters, etc. used by our\r\n   * Offline heuristics, so must not be used in place of\r\n   * handleWatchStreamStart() and handleWatchStreamFailure().\r\n   */\n\n\n  set(newState) {\n    this.clearOnlineStateTimer();\n    this.watchStreamFailures = 0;\n\n    if (newState === \"Online\"\n    /* OnlineState.Online */\n    ) {\n      // We've connected to watch at least once. Don't warn the developer\n      // about being offline going forward.\n      this.shouldWarnClientIsOffline = false;\n    }\n\n    this.setAndBroadcast(newState);\n  }\n\n  setAndBroadcast(newState) {\n    if (newState !== this.state) {\n      this.state = newState;\n      this.onlineStateHandler(newState);\n    }\n  }\n\n  logClientOfflineWarningIfNecessary(details) {\n    const message = `Could not reach Cloud Firestore backend. ${details}\\n` + `This typically indicates that your device does not have a healthy ` + `Internet connection at the moment. The client will operate in offline ` + `mode until it is able to successfully connect to the backend.`;\n\n    if (this.shouldWarnClientIsOffline) {\n      logError(message);\n      this.shouldWarnClientIsOffline = false;\n    } else {\n      logDebug(LOG_TAG$6, message);\n    }\n  }\n\n  clearOnlineStateTimer() {\n    if (this.onlineStateTimer !== null) {\n      this.onlineStateTimer.cancel();\n      this.onlineStateTimer = null;\n    }\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$5 = 'RemoteStore'; // TODO(b/35853402): Negotiate this with the stream.\n\nconst MAX_PENDING_WRITES = 10;\n\nclass RemoteStoreImpl {\n  constructor(\n  /**\r\n   * The local store, used to fill the write pipeline with outbound mutations.\r\n   */\n  localStore,\n  /** The client-side proxy for interacting with the backend. */\n  datastore, asyncQueue, onlineStateHandler, connectivityMonitor) {\n    var _this18 = this;\n\n    this.localStore = localStore;\n    this.datastore = datastore;\n    this.asyncQueue = asyncQueue;\n    this.remoteSyncer = {};\n    /**\r\n     * A list of up to MAX_PENDING_WRITES writes that we have fetched from the\r\n     * LocalStore via fillWritePipeline() and have or will send to the write\r\n     * stream.\r\n     *\r\n     * Whenever writePipeline.length > 0 the RemoteStore will attempt to start or\r\n     * restart the write stream. When the stream is established the writes in the\r\n     * pipeline will be sent in order.\r\n     *\r\n     * Writes remain in writePipeline until they are acknowledged by the backend\r\n     * and thus will automatically be re-sent if the stream is interrupted /\r\n     * restarted before they're acknowledged.\r\n     *\r\n     * Write responses from the backend are linked to their originating request\r\n     * purely based on order, and so we can just shift() writes from the front of\r\n     * the writePipeline as we receive responses.\r\n     */\n\n    this.writePipeline = [];\n    /**\r\n     * A mapping of watched targets that the client cares about tracking and the\r\n     * user has explicitly called a 'listen' for this target.\r\n     *\r\n     * These targets may or may not have been sent to or acknowledged by the\r\n     * server. On re-establishing the listen stream, these targets should be sent\r\n     * to the server. The targets removed with unlistens are removed eagerly\r\n     * without waiting for confirmation from the listen stream.\r\n     */\n\n    this.listenTargets = new Map();\n    /**\r\n     * A set of reasons for why the RemoteStore may be offline. If empty, the\r\n     * RemoteStore may start its network connections.\r\n     */\n\n    this.offlineCauses = new Set();\n    /**\r\n     * Event handlers that get called when the network is disabled or enabled.\r\n     *\r\n     * PORTING NOTE: These functions are used on the Web client to create the\r\n     * underlying streams (to support tree-shakeable streams). On Android and iOS,\r\n     * the streams are created during construction of RemoteStore.\r\n     */\n\n    this.onNetworkStatusChange = [];\n    this.connectivityMonitor = connectivityMonitor;\n    this.connectivityMonitor.addCallback(_ => {\n      asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n        // Porting Note: Unlike iOS, `restartNetwork()` is called even when the\n        // network becomes unreachable as we don't have any other way to tear\n        // down our streams.\n        if (canUseNetwork(_this18)) {\n          logDebug(LOG_TAG$5, 'Restarting streams for network reachability change.');\n          yield restartNetwork(_this18);\n        }\n      }));\n    });\n    this.onlineStateTracker = new OnlineStateTracker(asyncQueue, onlineStateHandler);\n  }\n\n}\n\nfunction newRemoteStore(localStore, datastore, asyncQueue, onlineStateHandler, connectivityMonitor) {\n  return new RemoteStoreImpl(localStore, datastore, asyncQueue, onlineStateHandler, connectivityMonitor);\n}\n/** Re-enables the network. Idempotent. */\n\n\nfunction remoteStoreEnableNetwork(remoteStore) {\n  const remoteStoreImpl = debugCast(remoteStore);\n  remoteStoreImpl.offlineCauses.delete(0\n  /* OfflineCause.UserDisabled */\n  );\n  return enableNetworkInternal(remoteStoreImpl);\n}\n\nfunction enableNetworkInternal(_x27) {\n  return _enableNetworkInternal.apply(this, arguments);\n}\n/**\r\n * Temporarily disables the network. The network can be re-enabled using\r\n * enableNetwork().\r\n */\n\n\nfunction _enableNetworkInternal() {\n  _enableNetworkInternal = _asyncToGenerator(function* (remoteStoreImpl) {\n    if (canUseNetwork(remoteStoreImpl)) {\n      for (const networkStatusHandler of remoteStoreImpl.onNetworkStatusChange) {\n        yield networkStatusHandler(\n        /* enabled= */\n        true);\n      }\n    }\n  });\n  return _enableNetworkInternal.apply(this, arguments);\n}\n\nfunction remoteStoreDisableNetwork(_x28) {\n  return _remoteStoreDisableNetwork.apply(this, arguments);\n}\n\nfunction _remoteStoreDisableNetwork() {\n  _remoteStoreDisableNetwork = _asyncToGenerator(function* (remoteStore) {\n    const remoteStoreImpl = debugCast(remoteStore);\n    remoteStoreImpl.offlineCauses.add(0\n    /* OfflineCause.UserDisabled */\n    );\n    yield disableNetworkInternal(remoteStoreImpl); // Set the OnlineState to Offline so get()s return from cache, etc.\n\n    remoteStoreImpl.onlineStateTracker.set(\"Offline\"\n    /* OnlineState.Offline */\n    );\n  });\n  return _remoteStoreDisableNetwork.apply(this, arguments);\n}\n\nfunction disableNetworkInternal(_x29) {\n  return _disableNetworkInternal.apply(this, arguments);\n}\n\nfunction _disableNetworkInternal() {\n  _disableNetworkInternal = _asyncToGenerator(function* (remoteStoreImpl) {\n    for (const networkStatusHandler of remoteStoreImpl.onNetworkStatusChange) {\n      yield networkStatusHandler(\n      /* enabled= */\n      false);\n    }\n  });\n  return _disableNetworkInternal.apply(this, arguments);\n}\n\nfunction remoteStoreShutdown(_x30) {\n  return _remoteStoreShutdown.apply(this, arguments);\n}\n/**\r\n * Starts new listen for the given target. Uses resume token if provided. It\r\n * is a no-op if the target of given `TargetData` is already being listened to.\r\n */\n\n\nfunction _remoteStoreShutdown() {\n  _remoteStoreShutdown = _asyncToGenerator(function* (remoteStore) {\n    const remoteStoreImpl = debugCast(remoteStore);\n    logDebug(LOG_TAG$5, 'RemoteStore shutting down.');\n    remoteStoreImpl.offlineCauses.add(5\n    /* OfflineCause.Shutdown */\n    );\n    yield disableNetworkInternal(remoteStoreImpl);\n    remoteStoreImpl.connectivityMonitor.shutdown(); // Set the OnlineState to Unknown (rather than Offline) to avoid potentially\n    // triggering spurious listener events with cached data, etc.\n\n    remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n    /* OnlineState.Unknown */\n    );\n  });\n  return _remoteStoreShutdown.apply(this, arguments);\n}\n\nfunction remoteStoreListen(remoteStore, targetData) {\n  const remoteStoreImpl = debugCast(remoteStore);\n\n  if (remoteStoreImpl.listenTargets.has(targetData.targetId)) {\n    return;\n  } // Mark this as something the client is currently listening for.\n\n\n  remoteStoreImpl.listenTargets.set(targetData.targetId, targetData);\n\n  if (shouldStartWatchStream(remoteStoreImpl)) {\n    // The listen will be sent in onWatchStreamOpen\n    startWatchStream(remoteStoreImpl);\n  } else if (ensureWatchStream(remoteStoreImpl).isOpen()) {\n    sendWatchRequest(remoteStoreImpl, targetData);\n  }\n}\n/**\r\n * Removes the listen from server. It is a no-op if the given target id is\r\n * not being listened to.\r\n */\n\n\nfunction remoteStoreUnlisten(remoteStore, targetId) {\n  const remoteStoreImpl = debugCast(remoteStore);\n  const watchStream = ensureWatchStream(remoteStoreImpl);\n  remoteStoreImpl.listenTargets.delete(targetId);\n\n  if (watchStream.isOpen()) {\n    sendUnwatchRequest(remoteStoreImpl, targetId);\n  }\n\n  if (remoteStoreImpl.listenTargets.size === 0) {\n    if (watchStream.isOpen()) {\n      watchStream.markIdle();\n    } else if (canUseNetwork(remoteStoreImpl)) {\n      // Revert to OnlineState.Unknown if the watch stream is not open and we\n      // have no listeners, since without any listens to send we cannot\n      // confirm if the stream is healthy and upgrade to OnlineState.Online.\n      remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n    }\n  }\n}\n/**\r\n * We need to increment the the expected number of pending responses we're due\r\n * from watch so we wait for the ack to process any messages from this target.\r\n */\n\n\nfunction sendWatchRequest(remoteStoreImpl, targetData) {\n  remoteStoreImpl.watchChangeAggregator.recordPendingTargetRequest(targetData.targetId);\n\n  if (targetData.resumeToken.approximateByteSize() > 0 || targetData.snapshotVersion.compareTo(SnapshotVersion.min()) > 0) {\n    const expectedCount = remoteStoreImpl.remoteSyncer.getRemoteKeysForTarget(targetData.targetId).size;\n    targetData = targetData.withExpectedCount(expectedCount);\n  }\n\n  ensureWatchStream(remoteStoreImpl).watch(targetData);\n}\n/**\r\n * We need to increment the expected number of pending responses we're due\r\n * from watch so we wait for the removal on the server before we process any\r\n * messages from this target.\r\n */\n\n\nfunction sendUnwatchRequest(remoteStoreImpl, targetId) {\n  remoteStoreImpl.watchChangeAggregator.recordPendingTargetRequest(targetId);\n  ensureWatchStream(remoteStoreImpl).unwatch(targetId);\n}\n\nfunction startWatchStream(remoteStoreImpl) {\n  remoteStoreImpl.watchChangeAggregator = new WatchChangeAggregator({\n    getRemoteKeysForTarget: targetId => remoteStoreImpl.remoteSyncer.getRemoteKeysForTarget(targetId),\n    getTargetDataForTarget: targetId => remoteStoreImpl.listenTargets.get(targetId) || null,\n    getDatabaseId: () => remoteStoreImpl.datastore.serializer.databaseId\n  });\n  ensureWatchStream(remoteStoreImpl).start();\n  remoteStoreImpl.onlineStateTracker.handleWatchStreamStart();\n}\n/**\r\n * Returns whether the watch stream should be started because it's necessary\r\n * and has not yet been started.\r\n */\n\n\nfunction shouldStartWatchStream(remoteStoreImpl) {\n  return canUseNetwork(remoteStoreImpl) && !ensureWatchStream(remoteStoreImpl).isStarted() && remoteStoreImpl.listenTargets.size > 0;\n}\n\nfunction canUseNetwork(remoteStore) {\n  const remoteStoreImpl = debugCast(remoteStore);\n  return remoteStoreImpl.offlineCauses.size === 0;\n}\n\nfunction cleanUpWatchStreamState(remoteStoreImpl) {\n  remoteStoreImpl.watchChangeAggregator = undefined;\n}\n\nfunction onWatchStreamOpen(_x31) {\n  return _onWatchStreamOpen.apply(this, arguments);\n}\n\nfunction _onWatchStreamOpen() {\n  _onWatchStreamOpen = _asyncToGenerator(function* (remoteStoreImpl) {\n    remoteStoreImpl.listenTargets.forEach((targetData, targetId) => {\n      sendWatchRequest(remoteStoreImpl, targetData);\n    });\n  });\n  return _onWatchStreamOpen.apply(this, arguments);\n}\n\nfunction onWatchStreamClose(_x32, _x33) {\n  return _onWatchStreamClose.apply(this, arguments);\n}\n\nfunction _onWatchStreamClose() {\n  _onWatchStreamClose = _asyncToGenerator(function* (remoteStoreImpl, error) {\n    cleanUpWatchStreamState(remoteStoreImpl); // If we still need the watch stream, retry the connection.\n\n    if (shouldStartWatchStream(remoteStoreImpl)) {\n      remoteStoreImpl.onlineStateTracker.handleWatchStreamFailure(error);\n      startWatchStream(remoteStoreImpl);\n    } else {\n      // No need to restart watch stream because there are no active targets.\n      // The online state is set to unknown because there is no active attempt\n      // at establishing a connection\n      remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n    }\n  });\n  return _onWatchStreamClose.apply(this, arguments);\n}\n\nfunction onWatchStreamChange(_x34, _x35, _x36) {\n  return _onWatchStreamChange.apply(this, arguments);\n}\n/**\r\n * Recovery logic for IndexedDB errors that takes the network offline until\r\n * `op` succeeds. Retries are scheduled with backoff using\r\n * `enqueueRetryable()`. If `op()` is not provided, IndexedDB access is\r\n * validated via a generic operation.\r\n *\r\n * The returned Promise is resolved once the network is disabled and before\r\n * any retry attempt.\r\n */\n\n\nfunction _onWatchStreamChange() {\n  _onWatchStreamChange = _asyncToGenerator(function* (remoteStoreImpl, watchChange, snapshotVersion) {\n    // Mark the client as online since we got a message from the server\n    remoteStoreImpl.onlineStateTracker.set(\"Online\"\n    /* OnlineState.Online */\n    );\n\n    if (watchChange instanceof WatchTargetChange && watchChange.state === 2\n    /* WatchTargetChangeState.Removed */\n    && watchChange.cause) {\n      // There was an error on a target, don't wait for a consistent snapshot\n      // to raise events\n      try {\n        yield handleTargetError(remoteStoreImpl, watchChange);\n      } catch (e) {\n        logDebug(LOG_TAG$5, 'Failed to remove targets %s: %s ', watchChange.targetIds.join(','), e);\n        yield disableNetworkUntilRecovery(remoteStoreImpl, e);\n      }\n\n      return;\n    }\n\n    if (watchChange instanceof DocumentWatchChange) {\n      remoteStoreImpl.watchChangeAggregator.handleDocumentChange(watchChange);\n    } else if (watchChange instanceof ExistenceFilterChange) {\n      remoteStoreImpl.watchChangeAggregator.handleExistenceFilter(watchChange);\n    } else {\n      remoteStoreImpl.watchChangeAggregator.handleTargetChange(watchChange);\n    }\n\n    if (!snapshotVersion.isEqual(SnapshotVersion.min())) {\n      try {\n        const lastRemoteSnapshotVersion = yield localStoreGetLastRemoteSnapshotVersion(remoteStoreImpl.localStore);\n\n        if (snapshotVersion.compareTo(lastRemoteSnapshotVersion) >= 0) {\n          // We have received a target change with a global snapshot if the snapshot\n          // version is not equal to SnapshotVersion.min().\n          yield raiseWatchSnapshot(remoteStoreImpl, snapshotVersion);\n        }\n      } catch (e) {\n        logDebug(LOG_TAG$5, 'Failed to raise snapshot:', e);\n        yield disableNetworkUntilRecovery(remoteStoreImpl, e);\n      }\n    }\n  });\n  return _onWatchStreamChange.apply(this, arguments);\n}\n\nfunction disableNetworkUntilRecovery(_x37, _x38, _x39) {\n  return _disableNetworkUntilRecovery.apply(this, arguments);\n}\n/**\r\n * Executes `op`. If `op` fails, takes the network offline until `op`\r\n * succeeds. Returns after the first attempt.\r\n */\n\n\nfunction _disableNetworkUntilRecovery() {\n  _disableNetworkUntilRecovery = _asyncToGenerator(function* (remoteStoreImpl, e, op) {\n    if (isIndexedDbTransactionError(e)) {\n      remoteStoreImpl.offlineCauses.add(1\n      /* OfflineCause.IndexedDbFailed */\n      ); // Disable network and raise offline snapshots\n\n      yield disableNetworkInternal(remoteStoreImpl);\n      remoteStoreImpl.onlineStateTracker.set(\"Offline\"\n      /* OnlineState.Offline */\n      );\n\n      if (!op) {\n        // Use a simple read operation to determine if IndexedDB recovered.\n        // Ideally, we would expose a health check directly on SimpleDb, but\n        // RemoteStore only has access to persistence through LocalStore.\n        op = () => localStoreGetLastRemoteSnapshotVersion(remoteStoreImpl.localStore);\n      } // Probe IndexedDB periodically and re-enable network\n\n\n      remoteStoreImpl.asyncQueue.enqueueRetryable( /*#__PURE__*/_asyncToGenerator(function* () {\n        logDebug(LOG_TAG$5, 'Retrying IndexedDB access');\n        yield op();\n        remoteStoreImpl.offlineCauses.delete(1\n        /* OfflineCause.IndexedDbFailed */\n        );\n        yield enableNetworkInternal(remoteStoreImpl);\n      }));\n    } else {\n      throw e;\n    }\n  });\n  return _disableNetworkUntilRecovery.apply(this, arguments);\n}\n\nfunction executeWithRecovery(remoteStoreImpl, op) {\n  return op().catch(e => disableNetworkUntilRecovery(remoteStoreImpl, e, op));\n}\n/**\r\n * Takes a batch of changes from the Datastore, repackages them as a\r\n * RemoteEvent, and passes that on to the listener, which is typically the\r\n * SyncEngine.\r\n */\n\n\nfunction raiseWatchSnapshot(remoteStoreImpl, snapshotVersion) {\n  const remoteEvent = remoteStoreImpl.watchChangeAggregator.createRemoteEvent(snapshotVersion); // Update in-memory resume tokens. LocalStore will update the\n  // persistent view of these when applying the completed RemoteEvent.\n\n  remoteEvent.targetChanges.forEach((change, targetId) => {\n    if (change.resumeToken.approximateByteSize() > 0) {\n      const targetData = remoteStoreImpl.listenTargets.get(targetId); // A watched target might have been removed already.\n\n      if (targetData) {\n        remoteStoreImpl.listenTargets.set(targetId, targetData.withResumeToken(change.resumeToken, snapshotVersion));\n      }\n    }\n  }); // Re-establish listens for the targets that have been invalidated by\n  // existence filter mismatches.\n\n  remoteEvent.targetMismatches.forEach((targetId, targetPurpose) => {\n    const targetData = remoteStoreImpl.listenTargets.get(targetId);\n\n    if (!targetData) {\n      // A watched target might have been removed already.\n      return;\n    } // Clear the resume token for the target, since we're in a known mismatch\n    // state.\n\n\n    remoteStoreImpl.listenTargets.set(targetId, targetData.withResumeToken(ByteString.EMPTY_BYTE_STRING, targetData.snapshotVersion)); // Cause a hard reset by unwatching and rewatching immediately, but\n    // deliberately don't send a resume token so that we get a full update.\n\n    sendUnwatchRequest(remoteStoreImpl, targetId); // Mark the target we send as being on behalf of an existence filter\n    // mismatch, but don't actually retain that in listenTargets. This ensures\n    // that we flag the first re-listen this way without impacting future\n    // listens of this target (that might happen e.g. on reconnect).\n\n    const requestTargetData = new TargetData(targetData.target, targetId, targetPurpose, targetData.sequenceNumber);\n    sendWatchRequest(remoteStoreImpl, requestTargetData);\n  });\n  return remoteStoreImpl.remoteSyncer.applyRemoteEvent(remoteEvent);\n}\n/** Handles an error on a target */\n\n\nfunction handleTargetError(_x40, _x41) {\n  return _handleTargetError.apply(this, arguments);\n}\n/**\r\n * Attempts to fill our write pipeline with writes from the LocalStore.\r\n *\r\n * Called internally to bootstrap or refill the write pipeline and by\r\n * SyncEngine whenever there are new mutations to process.\r\n *\r\n * Starts the write stream if necessary.\r\n */\n\n\nfunction _handleTargetError() {\n  _handleTargetError = _asyncToGenerator(function* (remoteStoreImpl, watchChange) {\n    const error = watchChange.cause;\n\n    for (const targetId of watchChange.targetIds) {\n      // A watched target might have been removed already.\n      if (remoteStoreImpl.listenTargets.has(targetId)) {\n        yield remoteStoreImpl.remoteSyncer.rejectListen(targetId, error);\n        remoteStoreImpl.listenTargets.delete(targetId);\n        remoteStoreImpl.watchChangeAggregator.removeTarget(targetId);\n      }\n    }\n  });\n  return _handleTargetError.apply(this, arguments);\n}\n\nfunction fillWritePipeline(_x42) {\n  return _fillWritePipeline.apply(this, arguments);\n}\n/**\r\n * Returns true if we can add to the write pipeline (i.e. the network is\r\n * enabled and the write pipeline is not full).\r\n */\n\n\nfunction _fillWritePipeline() {\n  _fillWritePipeline = _asyncToGenerator(function* (remoteStore) {\n    const remoteStoreImpl = debugCast(remoteStore);\n    const writeStream = ensureWriteStream(remoteStoreImpl);\n    let lastBatchIdRetrieved = remoteStoreImpl.writePipeline.length > 0 ? remoteStoreImpl.writePipeline[remoteStoreImpl.writePipeline.length - 1].batchId : BATCHID_UNKNOWN;\n\n    while (canAddToWritePipeline(remoteStoreImpl)) {\n      try {\n        const batch = yield localStoreGetNextMutationBatch(remoteStoreImpl.localStore, lastBatchIdRetrieved);\n\n        if (batch === null) {\n          if (remoteStoreImpl.writePipeline.length === 0) {\n            writeStream.markIdle();\n          }\n\n          break;\n        } else {\n          lastBatchIdRetrieved = batch.batchId;\n          addToWritePipeline(remoteStoreImpl, batch);\n        }\n      } catch (e) {\n        yield disableNetworkUntilRecovery(remoteStoreImpl, e);\n      }\n    }\n\n    if (shouldStartWriteStream(remoteStoreImpl)) {\n      startWriteStream(remoteStoreImpl);\n    }\n  });\n  return _fillWritePipeline.apply(this, arguments);\n}\n\nfunction canAddToWritePipeline(remoteStoreImpl) {\n  return canUseNetwork(remoteStoreImpl) && remoteStoreImpl.writePipeline.length < MAX_PENDING_WRITES;\n}\n/**\r\n * Queues additional writes to be sent to the write stream, sending them\r\n * immediately if the write stream is established.\r\n */\n\n\nfunction addToWritePipeline(remoteStoreImpl, batch) {\n  remoteStoreImpl.writePipeline.push(batch);\n  const writeStream = ensureWriteStream(remoteStoreImpl);\n\n  if (writeStream.isOpen() && writeStream.handshakeComplete) {\n    writeStream.writeMutations(batch.mutations);\n  }\n}\n\nfunction shouldStartWriteStream(remoteStoreImpl) {\n  return canUseNetwork(remoteStoreImpl) && !ensureWriteStream(remoteStoreImpl).isStarted() && remoteStoreImpl.writePipeline.length > 0;\n}\n\nfunction startWriteStream(remoteStoreImpl) {\n  ensureWriteStream(remoteStoreImpl).start();\n}\n\nfunction onWriteStreamOpen(_x43) {\n  return _onWriteStreamOpen.apply(this, arguments);\n}\n\nfunction _onWriteStreamOpen() {\n  _onWriteStreamOpen = _asyncToGenerator(function* (remoteStoreImpl) {\n    ensureWriteStream(remoteStoreImpl).writeHandshake();\n  });\n  return _onWriteStreamOpen.apply(this, arguments);\n}\n\nfunction onWriteHandshakeComplete(_x44) {\n  return _onWriteHandshakeComplete.apply(this, arguments);\n}\n\nfunction _onWriteHandshakeComplete() {\n  _onWriteHandshakeComplete = _asyncToGenerator(function* (remoteStoreImpl) {\n    const writeStream = ensureWriteStream(remoteStoreImpl); // Send the write pipeline now that the stream is established.\n\n    for (const batch of remoteStoreImpl.writePipeline) {\n      writeStream.writeMutations(batch.mutations);\n    }\n  });\n  return _onWriteHandshakeComplete.apply(this, arguments);\n}\n\nfunction onMutationResult(_x45, _x46, _x47) {\n  return _onMutationResult.apply(this, arguments);\n}\n\nfunction _onMutationResult() {\n  _onMutationResult = _asyncToGenerator(function* (remoteStoreImpl, commitVersion, results) {\n    const batch = remoteStoreImpl.writePipeline.shift();\n    const success = MutationBatchResult.from(batch, commitVersion, results);\n    yield executeWithRecovery(remoteStoreImpl, () => remoteStoreImpl.remoteSyncer.applySuccessfulWrite(success)); // It's possible that with the completion of this mutation another\n    // slot has freed up.\n\n    yield fillWritePipeline(remoteStoreImpl);\n  });\n  return _onMutationResult.apply(this, arguments);\n}\n\nfunction onWriteStreamClose(_x48, _x49) {\n  return _onWriteStreamClose.apply(this, arguments);\n}\n\nfunction _onWriteStreamClose() {\n  _onWriteStreamClose = _asyncToGenerator(function* (remoteStoreImpl, error) {\n    // If the write stream closed after the write handshake completes, a write\n    // operation failed and we fail the pending operation.\n    if (error && ensureWriteStream(remoteStoreImpl).handshakeComplete) {\n      // This error affects the actual write.\n      yield handleWriteError(remoteStoreImpl, error);\n    } // The write stream might have been started by refilling the write\n    // pipeline for failed writes\n\n\n    if (shouldStartWriteStream(remoteStoreImpl)) {\n      startWriteStream(remoteStoreImpl);\n    }\n  });\n  return _onWriteStreamClose.apply(this, arguments);\n}\n\nfunction handleWriteError(_x50, _x51) {\n  return _handleWriteError.apply(this, arguments);\n}\n\nfunction _handleWriteError() {\n  _handleWriteError = _asyncToGenerator(function* (remoteStoreImpl, error) {\n    // Only handle permanent errors here. If it's transient, just let the retry\n    // logic kick in.\n    if (isPermanentWriteError(error.code)) {\n      // This was a permanent error, the request itself was the problem\n      // so it's not going to succeed if we resend it.\n      const batch = remoteStoreImpl.writePipeline.shift(); // In this case it's also unlikely that the server itself is melting\n      // down -- this was just a bad request so inhibit backoff on the next\n      // restart.\n\n      ensureWriteStream(remoteStoreImpl).inhibitBackoff();\n      yield executeWithRecovery(remoteStoreImpl, () => remoteStoreImpl.remoteSyncer.rejectFailedWrite(batch.batchId, error)); // It's possible that with the completion of this mutation\n      // another slot has freed up.\n\n      yield fillWritePipeline(remoteStoreImpl);\n    }\n  });\n  return _handleWriteError.apply(this, arguments);\n}\n\nfunction restartNetwork(_x52) {\n  return _restartNetwork.apply(this, arguments);\n}\n\nfunction _restartNetwork() {\n  _restartNetwork = _asyncToGenerator(function* (remoteStore) {\n    const remoteStoreImpl = debugCast(remoteStore);\n    remoteStoreImpl.offlineCauses.add(4\n    /* OfflineCause.ConnectivityChange */\n    );\n    yield disableNetworkInternal(remoteStoreImpl);\n    remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n    /* OnlineState.Unknown */\n    );\n    remoteStoreImpl.offlineCauses.delete(4\n    /* OfflineCause.ConnectivityChange */\n    );\n    yield enableNetworkInternal(remoteStoreImpl);\n  });\n  return _restartNetwork.apply(this, arguments);\n}\n\nfunction remoteStoreHandleCredentialChange(_x53, _x54) {\n  return _remoteStoreHandleCredentialChange.apply(this, arguments);\n}\n/**\r\n * Toggles the network state when the client gains or loses its primary lease.\r\n */\n\n\nfunction _remoteStoreHandleCredentialChange() {\n  _remoteStoreHandleCredentialChange = _asyncToGenerator(function* (remoteStore, user) {\n    const remoteStoreImpl = debugCast(remoteStore);\n    remoteStoreImpl.asyncQueue.verifyOperationInProgress();\n    logDebug(LOG_TAG$5, 'RemoteStore received new credentials');\n    const usesNetwork = canUseNetwork(remoteStoreImpl); // Tear down and re-create our network streams. This will ensure we get a\n    // fresh auth token for the new user and re-fill the write pipeline with\n    // new mutations from the LocalStore (since mutations are per-user).\n\n    remoteStoreImpl.offlineCauses.add(3\n    /* OfflineCause.CredentialChange */\n    );\n    yield disableNetworkInternal(remoteStoreImpl);\n\n    if (usesNetwork) {\n      // Don't set the network status to Unknown if we are offline.\n      remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n    }\n\n    yield remoteStoreImpl.remoteSyncer.handleCredentialChange(user);\n    remoteStoreImpl.offlineCauses.delete(3\n    /* OfflineCause.CredentialChange */\n    );\n    yield enableNetworkInternal(remoteStoreImpl);\n  });\n  return _remoteStoreHandleCredentialChange.apply(this, arguments);\n}\n\nfunction remoteStoreApplyPrimaryState(_x55, _x56) {\n  return _remoteStoreApplyPrimaryState.apply(this, arguments);\n}\n/**\r\n * If not yet initialized, registers the WatchStream and its network state\r\n * callback with `remoteStoreImpl`. Returns the existing stream if one is\r\n * already available.\r\n *\r\n * PORTING NOTE: On iOS and Android, the WatchStream gets registered on startup.\r\n * This is not done on Web to allow it to be tree-shaken.\r\n */\n\n\nfunction _remoteStoreApplyPrimaryState() {\n  _remoteStoreApplyPrimaryState = _asyncToGenerator(function* (remoteStore, isPrimary) {\n    const remoteStoreImpl = debugCast(remoteStore);\n\n    if (isPrimary) {\n      remoteStoreImpl.offlineCauses.delete(2\n      /* OfflineCause.IsSecondary */\n      );\n      yield enableNetworkInternal(remoteStoreImpl);\n    } else if (!isPrimary) {\n      remoteStoreImpl.offlineCauses.add(2\n      /* OfflineCause.IsSecondary */\n      );\n      yield disableNetworkInternal(remoteStoreImpl);\n      remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n      /* OnlineState.Unknown */\n      );\n    }\n  });\n  return _remoteStoreApplyPrimaryState.apply(this, arguments);\n}\n\nfunction ensureWatchStream(remoteStoreImpl) {\n  if (!remoteStoreImpl.watchStream) {\n    // Create stream (but note that it is not started yet).\n    remoteStoreImpl.watchStream = newPersistentWatchStream(remoteStoreImpl.datastore, remoteStoreImpl.asyncQueue, {\n      onOpen: onWatchStreamOpen.bind(null, remoteStoreImpl),\n      onClose: onWatchStreamClose.bind(null, remoteStoreImpl),\n      onWatchChange: onWatchStreamChange.bind(null, remoteStoreImpl)\n    });\n    remoteStoreImpl.onNetworkStatusChange.push( /*#__PURE__*/function () {\n      var _ref10 = _asyncToGenerator(function* (enabled) {\n        if (enabled) {\n          remoteStoreImpl.watchStream.inhibitBackoff();\n\n          if (shouldStartWatchStream(remoteStoreImpl)) {\n            startWatchStream(remoteStoreImpl);\n          } else {\n            remoteStoreImpl.onlineStateTracker.set(\"Unknown\"\n            /* OnlineState.Unknown */\n            );\n          }\n        } else {\n          yield remoteStoreImpl.watchStream.stop();\n          cleanUpWatchStreamState(remoteStoreImpl);\n        }\n      });\n\n      return function (_x57) {\n        return _ref10.apply(this, arguments);\n      };\n    }());\n  }\n\n  return remoteStoreImpl.watchStream;\n}\n/**\r\n * If not yet initialized, registers the WriteStream and its network state\r\n * callback with `remoteStoreImpl`. Returns the existing stream if one is\r\n * already available.\r\n *\r\n * PORTING NOTE: On iOS and Android, the WriteStream gets registered on startup.\r\n * This is not done on Web to allow it to be tree-shaken.\r\n */\n\n\nfunction ensureWriteStream(remoteStoreImpl) {\n  if (!remoteStoreImpl.writeStream) {\n    // Create stream (but note that it is not started yet).\n    remoteStoreImpl.writeStream = newPersistentWriteStream(remoteStoreImpl.datastore, remoteStoreImpl.asyncQueue, {\n      onOpen: onWriteStreamOpen.bind(null, remoteStoreImpl),\n      onClose: onWriteStreamClose.bind(null, remoteStoreImpl),\n      onHandshakeComplete: onWriteHandshakeComplete.bind(null, remoteStoreImpl),\n      onMutationResult: onMutationResult.bind(null, remoteStoreImpl)\n    });\n    remoteStoreImpl.onNetworkStatusChange.push( /*#__PURE__*/function () {\n      var _ref11 = _asyncToGenerator(function* (enabled) {\n        if (enabled) {\n          remoteStoreImpl.writeStream.inhibitBackoff(); // This will start the write stream if necessary.\n\n          yield fillWritePipeline(remoteStoreImpl);\n        } else {\n          yield remoteStoreImpl.writeStream.stop();\n\n          if (remoteStoreImpl.writePipeline.length > 0) {\n            logDebug(LOG_TAG$5, `Stopping write stream with ${remoteStoreImpl.writePipeline.length} pending writes`);\n            remoteStoreImpl.writePipeline = [];\n          }\n        }\n      });\n\n      return function (_x58) {\n        return _ref11.apply(this, arguments);\n      };\n    }());\n  }\n\n  return remoteStoreImpl.writeStream;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$4 = 'AsyncQueue';\n/**\r\n * Represents an operation scheduled to be run in the future on an AsyncQueue.\r\n *\r\n * It is created via DelayedOperation.createAndSchedule().\r\n *\r\n * Supports cancellation (via cancel()) and early execution (via skipDelay()).\r\n *\r\n * Note: We implement `PromiseLike` instead of `Promise`, as the `Promise` type\r\n * in newer versions of TypeScript defines `finally`, which is not available in\r\n * IE.\r\n */\n\nclass DelayedOperation {\n  constructor(asyncQueue, timerId, targetTimeMs, op, removalCallback) {\n    this.asyncQueue = asyncQueue;\n    this.timerId = timerId;\n    this.targetTimeMs = targetTimeMs;\n    this.op = op;\n    this.removalCallback = removalCallback;\n    this.deferred = new Deferred();\n    this.then = this.deferred.promise.then.bind(this.deferred.promise); // It's normal for the deferred promise to be canceled (due to cancellation)\n    // and so we attach a dummy catch callback to avoid\n    // 'UnhandledPromiseRejectionWarning' log spam.\n\n    this.deferred.promise.catch(err => {});\n  }\n  /**\r\n   * Creates and returns a DelayedOperation that has been scheduled to be\r\n   * executed on the provided asyncQueue after the provided delayMs.\r\n   *\r\n   * @param asyncQueue - The queue to schedule the operation on.\r\n   * @param id - A Timer ID identifying the type of operation this is.\r\n   * @param delayMs - The delay (ms) before the operation should be scheduled.\r\n   * @param op - The operation to run.\r\n   * @param removalCallback - A callback to be called synchronously once the\r\n   *   operation is executed or canceled, notifying the AsyncQueue to remove it\r\n   *   from its delayedOperations list.\r\n   *   PORTING NOTE: This exists to prevent making removeDelayedOperation() and\r\n   *   the DelayedOperation class public.\r\n   */\n\n\n  static createAndSchedule(asyncQueue, timerId, delayMs, op, removalCallback) {\n    const targetTime = Date.now() + delayMs;\n    const delayedOp = new DelayedOperation(asyncQueue, timerId, targetTime, op, removalCallback);\n    delayedOp.start(delayMs);\n    return delayedOp;\n  }\n  /**\r\n   * Starts the timer. This is called immediately after construction by\r\n   * createAndSchedule().\r\n   */\n\n\n  start(delayMs) {\n    this.timerHandle = setTimeout(() => this.handleDelayElapsed(), delayMs);\n  }\n  /**\r\n   * Queues the operation to run immediately (if it hasn't already been run or\r\n   * canceled).\r\n   */\n\n\n  skipDelay() {\n    return this.handleDelayElapsed();\n  }\n  /**\r\n   * Cancels the operation if it hasn't already been executed or canceled. The\r\n   * promise will be rejected.\r\n   *\r\n   * As long as the operation has not yet been run, calling cancel() provides a\r\n   * guarantee that the operation will not be run.\r\n   */\n\n\n  cancel(reason) {\n    if (this.timerHandle !== null) {\n      this.clearTimeout();\n      this.deferred.reject(new FirestoreError(Code.CANCELLED, 'Operation cancelled' + (reason ? ': ' + reason : '')));\n    }\n  }\n\n  handleDelayElapsed() {\n    this.asyncQueue.enqueueAndForget(() => {\n      if (this.timerHandle !== null) {\n        this.clearTimeout();\n        return this.op().then(result => {\n          return this.deferred.resolve(result);\n        });\n      } else {\n        return Promise.resolve();\n      }\n    });\n  }\n\n  clearTimeout() {\n    if (this.timerHandle !== null) {\n      this.removalCallback(this);\n      clearTimeout(this.timerHandle);\n      this.timerHandle = null;\n    }\n  }\n\n}\n/**\r\n * Returns a FirestoreError that can be surfaced to the user if the provided\r\n * error is an IndexedDbTransactionError. Re-throws the error otherwise.\r\n */\n\n\nfunction wrapInUserErrorIfRecoverable(e, msg) {\n  logError(LOG_TAG$4, `${msg}: ${e}`);\n\n  if (isIndexedDbTransactionError(e)) {\n    return new FirestoreError(Code.UNAVAILABLE, `${msg}: ${e}`);\n  } else {\n    throw e;\n  }\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * DocumentSet is an immutable (copy-on-write) collection that holds documents\r\n * in order specified by the provided comparator. We always add a document key\r\n * comparator on top of what is provided to guarantee document equality based on\r\n * the key.\r\n */\n\n\nclass DocumentSet {\n  /** The default ordering is by key if the comparator is omitted */\n  constructor(comp) {\n    // We are adding document key comparator to the end as it's the only\n    // guaranteed unique property of a document.\n    if (comp) {\n      this.comparator = (d1, d2) => comp(d1, d2) || DocumentKey.comparator(d1.key, d2.key);\n    } else {\n      this.comparator = (d1, d2) => DocumentKey.comparator(d1.key, d2.key);\n    }\n\n    this.keyedMap = documentMap();\n    this.sortedSet = new SortedMap(this.comparator);\n  }\n  /**\r\n   * Returns an empty copy of the existing DocumentSet, using the same\r\n   * comparator.\r\n   */\n\n\n  static emptySet(oldSet) {\n    return new DocumentSet(oldSet.comparator);\n  }\n\n  has(key) {\n    return this.keyedMap.get(key) != null;\n  }\n\n  get(key) {\n    return this.keyedMap.get(key);\n  }\n\n  first() {\n    return this.sortedSet.minKey();\n  }\n\n  last() {\n    return this.sortedSet.maxKey();\n  }\n\n  isEmpty() {\n    return this.sortedSet.isEmpty();\n  }\n  /**\r\n   * Returns the index of the provided key in the document set, or -1 if the\r\n   * document key is not present in the set;\r\n   */\n\n\n  indexOf(key) {\n    const doc = this.keyedMap.get(key);\n    return doc ? this.sortedSet.indexOf(doc) : -1;\n  }\n\n  get size() {\n    return this.sortedSet.size;\n  }\n  /** Iterates documents in order defined by \"comparator\" */\n\n\n  forEach(cb) {\n    this.sortedSet.inorderTraversal((k, v) => {\n      cb(k);\n      return false;\n    });\n  }\n  /** Inserts or updates a document with the same key */\n\n\n  add(doc) {\n    // First remove the element if we have it.\n    const set = this.delete(doc.key);\n    return set.copy(set.keyedMap.insert(doc.key, doc), set.sortedSet.insert(doc, null));\n  }\n  /** Deletes a document with a given key */\n\n\n  delete(key) {\n    const doc = this.get(key);\n\n    if (!doc) {\n      return this;\n    }\n\n    return this.copy(this.keyedMap.remove(key), this.sortedSet.remove(doc));\n  }\n\n  isEqual(other) {\n    if (!(other instanceof DocumentSet)) {\n      return false;\n    }\n\n    if (this.size !== other.size) {\n      return false;\n    }\n\n    const thisIt = this.sortedSet.getIterator();\n    const otherIt = other.sortedSet.getIterator();\n\n    while (thisIt.hasNext()) {\n      const thisDoc = thisIt.getNext().key;\n      const otherDoc = otherIt.getNext().key;\n\n      if (!thisDoc.isEqual(otherDoc)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n  toString() {\n    const docStrings = [];\n    this.forEach(doc => {\n      docStrings.push(doc.toString());\n    });\n\n    if (docStrings.length === 0) {\n      return 'DocumentSet ()';\n    } else {\n      return 'DocumentSet (\\n  ' + docStrings.join('  \\n') + '\\n)';\n    }\n  }\n\n  copy(keyedMap, sortedSet) {\n    const newSet = new DocumentSet();\n    newSet.comparator = this.comparator;\n    newSet.keyedMap = keyedMap;\n    newSet.sortedSet = sortedSet;\n    return newSet;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * DocumentChangeSet keeps track of a set of changes to docs in a query, merging\r\n * duplicate events for the same doc.\r\n */\n\n\nclass DocumentChangeSet {\n  constructor() {\n    this.changeMap = new SortedMap(DocumentKey.comparator);\n  }\n\n  track(change) {\n    const key = change.doc.key;\n    const oldChange = this.changeMap.get(key);\n\n    if (!oldChange) {\n      this.changeMap = this.changeMap.insert(key, change);\n      return;\n    } // Merge the new change with the existing change.\n\n\n    if (change.type !== 0\n    /* ChangeType.Added */\n    && oldChange.type === 3\n    /* ChangeType.Metadata */\n    ) {\n      this.changeMap = this.changeMap.insert(key, change);\n    } else if (change.type === 3\n    /* ChangeType.Metadata */\n    && oldChange.type !== 1\n    /* ChangeType.Removed */\n    ) {\n      this.changeMap = this.changeMap.insert(key, {\n        type: oldChange.type,\n        doc: change.doc\n      });\n    } else if (change.type === 2\n    /* ChangeType.Modified */\n    && oldChange.type === 2\n    /* ChangeType.Modified */\n    ) {\n      this.changeMap = this.changeMap.insert(key, {\n        type: 2\n        /* ChangeType.Modified */\n        ,\n        doc: change.doc\n      });\n    } else if (change.type === 2\n    /* ChangeType.Modified */\n    && oldChange.type === 0\n    /* ChangeType.Added */\n    ) {\n      this.changeMap = this.changeMap.insert(key, {\n        type: 0\n        /* ChangeType.Added */\n        ,\n        doc: change.doc\n      });\n    } else if (change.type === 1\n    /* ChangeType.Removed */\n    && oldChange.type === 0\n    /* ChangeType.Added */\n    ) {\n      this.changeMap = this.changeMap.remove(key);\n    } else if (change.type === 1\n    /* ChangeType.Removed */\n    && oldChange.type === 2\n    /* ChangeType.Modified */\n    ) {\n      this.changeMap = this.changeMap.insert(key, {\n        type: 1\n        /* ChangeType.Removed */\n        ,\n        doc: oldChange.doc\n      });\n    } else if (change.type === 0\n    /* ChangeType.Added */\n    && oldChange.type === 1\n    /* ChangeType.Removed */\n    ) {\n      this.changeMap = this.changeMap.insert(key, {\n        type: 2\n        /* ChangeType.Modified */\n        ,\n        doc: change.doc\n      });\n    } else {\n      // This includes these cases, which don't make sense:\n      // Added->Added\n      // Removed->Removed\n      // Modified->Added\n      // Removed->Modified\n      // Metadata->Added\n      // Removed->Metadata\n      fail();\n    }\n  }\n\n  getChanges() {\n    const changes = [];\n    this.changeMap.inorderTraversal((key, change) => {\n      changes.push(change);\n    });\n    return changes;\n  }\n\n}\n\nclass ViewSnapshot {\n  constructor(query, docs, oldDocs, docChanges, mutatedKeys, fromCache, syncStateChanged, excludesMetadataChanges, hasCachedResults) {\n    this.query = query;\n    this.docs = docs;\n    this.oldDocs = oldDocs;\n    this.docChanges = docChanges;\n    this.mutatedKeys = mutatedKeys;\n    this.fromCache = fromCache;\n    this.syncStateChanged = syncStateChanged;\n    this.excludesMetadataChanges = excludesMetadataChanges;\n    this.hasCachedResults = hasCachedResults;\n  }\n  /** Returns a view snapshot as if all documents in the snapshot were added. */\n\n\n  static fromInitialDocuments(query, documents, mutatedKeys, fromCache, hasCachedResults) {\n    const changes = [];\n    documents.forEach(doc => {\n      changes.push({\n        type: 0\n        /* ChangeType.Added */\n        ,\n        doc\n      });\n    });\n    return new ViewSnapshot(query, documents, DocumentSet.emptySet(documents), changes, mutatedKeys, fromCache,\n    /* syncStateChanged= */\n    true,\n    /* excludesMetadataChanges= */\n    false, hasCachedResults);\n  }\n\n  get hasPendingWrites() {\n    return !this.mutatedKeys.isEmpty();\n  }\n\n  isEqual(other) {\n    if (this.fromCache !== other.fromCache || this.hasCachedResults !== other.hasCachedResults || this.syncStateChanged !== other.syncStateChanged || !this.mutatedKeys.isEqual(other.mutatedKeys) || !queryEquals(this.query, other.query) || !this.docs.isEqual(other.docs) || !this.oldDocs.isEqual(other.oldDocs)) {\n      return false;\n    }\n\n    const changes = this.docChanges;\n    const otherChanges = other.docChanges;\n\n    if (changes.length !== otherChanges.length) {\n      return false;\n    }\n\n    for (let i = 0; i < changes.length; i++) {\n      if (changes[i].type !== otherChanges[i].type || !changes[i].doc.isEqual(otherChanges[i].doc)) {\n        return false;\n      }\n    }\n\n    return true;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Holds the listeners and the last received ViewSnapshot for a query being\r\n * tracked by EventManager.\r\n */\n\n\nclass QueryListenersInfo {\n  constructor() {\n    this.viewSnap = undefined;\n    this.listeners = [];\n  }\n\n}\n\nfunction newEventManager() {\n  return new EventManagerImpl();\n}\n\nclass EventManagerImpl {\n  constructor() {\n    this.queries = new ObjectMap(q => canonifyQuery(q), queryEquals);\n    this.onlineState = \"Unknown\"\n    /* OnlineState.Unknown */\n    ;\n    this.snapshotsInSyncListeners = new Set();\n  }\n\n}\n\nfunction eventManagerListen(_x59, _x60) {\n  return _eventManagerListen.apply(this, arguments);\n}\n\nfunction _eventManagerListen() {\n  _eventManagerListen = _asyncToGenerator(function* (eventManager, listener) {\n    const eventManagerImpl = debugCast(eventManager);\n    const query = listener.query;\n    let firstListen = false;\n    let queryInfo = eventManagerImpl.queries.get(query);\n\n    if (!queryInfo) {\n      firstListen = true;\n      queryInfo = new QueryListenersInfo();\n    }\n\n    if (firstListen) {\n      try {\n        queryInfo.viewSnap = yield eventManagerImpl.onListen(query);\n      } catch (e) {\n        const firestoreError = wrapInUserErrorIfRecoverable(e, `Initialization of query '${stringifyQuery(listener.query)}' failed`);\n        listener.onError(firestoreError);\n        return;\n      }\n    }\n\n    eventManagerImpl.queries.set(query, queryInfo);\n    queryInfo.listeners.push(listener); // Run global snapshot listeners if a consistent snapshot has been emitted.\n\n    listener.applyOnlineStateChange(eventManagerImpl.onlineState);\n\n    if (queryInfo.viewSnap) {\n      const raisedEvent = listener.onViewSnapshot(queryInfo.viewSnap);\n\n      if (raisedEvent) {\n        raiseSnapshotsInSyncEvent(eventManagerImpl);\n      }\n    }\n  });\n  return _eventManagerListen.apply(this, arguments);\n}\n\nfunction eventManagerUnlisten(_x61, _x62) {\n  return _eventManagerUnlisten.apply(this, arguments);\n}\n\nfunction _eventManagerUnlisten() {\n  _eventManagerUnlisten = _asyncToGenerator(function* (eventManager, listener) {\n    const eventManagerImpl = debugCast(eventManager);\n    const query = listener.query;\n    let lastListen = false;\n    const queryInfo = eventManagerImpl.queries.get(query);\n\n    if (queryInfo) {\n      const i = queryInfo.listeners.indexOf(listener);\n\n      if (i >= 0) {\n        queryInfo.listeners.splice(i, 1);\n        lastListen = queryInfo.listeners.length === 0;\n      }\n    }\n\n    if (lastListen) {\n      eventManagerImpl.queries.delete(query);\n      return eventManagerImpl.onUnlisten(query);\n    }\n  });\n  return _eventManagerUnlisten.apply(this, arguments);\n}\n\nfunction eventManagerOnWatchChange(eventManager, viewSnaps) {\n  const eventManagerImpl = debugCast(eventManager);\n  let raisedEvent = false;\n\n  for (const viewSnap of viewSnaps) {\n    const query = viewSnap.query;\n    const queryInfo = eventManagerImpl.queries.get(query);\n\n    if (queryInfo) {\n      for (const listener of queryInfo.listeners) {\n        if (listener.onViewSnapshot(viewSnap)) {\n          raisedEvent = true;\n        }\n      }\n\n      queryInfo.viewSnap = viewSnap;\n    }\n  }\n\n  if (raisedEvent) {\n    raiseSnapshotsInSyncEvent(eventManagerImpl);\n  }\n}\n\nfunction eventManagerOnWatchError(eventManager, query, error) {\n  const eventManagerImpl = debugCast(eventManager);\n  const queryInfo = eventManagerImpl.queries.get(query);\n\n  if (queryInfo) {\n    for (const listener of queryInfo.listeners) {\n      listener.onError(error);\n    }\n  } // Remove all listeners. NOTE: We don't need to call syncEngine.unlisten()\n  // after an error.\n\n\n  eventManagerImpl.queries.delete(query);\n}\n\nfunction eventManagerOnOnlineStateChange(eventManager, onlineState) {\n  const eventManagerImpl = debugCast(eventManager);\n  eventManagerImpl.onlineState = onlineState;\n  let raisedEvent = false;\n  eventManagerImpl.queries.forEach((_, queryInfo) => {\n    for (const listener of queryInfo.listeners) {\n      // Run global snapshot listeners if a consistent snapshot has been emitted.\n      if (listener.applyOnlineStateChange(onlineState)) {\n        raisedEvent = true;\n      }\n    }\n  });\n\n  if (raisedEvent) {\n    raiseSnapshotsInSyncEvent(eventManagerImpl);\n  }\n}\n\nfunction addSnapshotsInSyncListener(eventManager, observer) {\n  const eventManagerImpl = debugCast(eventManager);\n  eventManagerImpl.snapshotsInSyncListeners.add(observer); // Immediately fire an initial event, indicating all existing listeners\n  // are in-sync.\n\n  observer.next();\n}\n\nfunction removeSnapshotsInSyncListener(eventManager, observer) {\n  const eventManagerImpl = debugCast(eventManager);\n  eventManagerImpl.snapshotsInSyncListeners.delete(observer);\n} // Call all global snapshot listeners that have been set.\n\n\nfunction raiseSnapshotsInSyncEvent(eventManagerImpl) {\n  eventManagerImpl.snapshotsInSyncListeners.forEach(observer => {\n    observer.next();\n  });\n}\n/**\r\n * QueryListener takes a series of internal view snapshots and determines\r\n * when to raise the event.\r\n *\r\n * It uses an Observer to dispatch events.\r\n */\n\n\nclass QueryListener {\n  constructor(query, queryObserver, options) {\n    this.query = query;\n    this.queryObserver = queryObserver;\n    /**\r\n     * Initial snapshots (e.g. from cache) may not be propagated to the wrapped\r\n     * observer. This flag is set to true once we've actually raised an event.\r\n     */\n\n    this.raisedInitialEvent = false;\n    this.snap = null;\n    this.onlineState = \"Unknown\"\n    /* OnlineState.Unknown */\n    ;\n    this.options = options || {};\n  }\n  /**\r\n   * Applies the new ViewSnapshot to this listener, raising a user-facing event\r\n   * if applicable (depending on what changed, whether the user has opted into\r\n   * metadata-only changes, etc.). Returns true if a user-facing event was\r\n   * indeed raised.\r\n   */\n\n\n  onViewSnapshot(snap) {\n    if (!this.options.includeMetadataChanges) {\n      // Remove the metadata only changes.\n      const docChanges = [];\n\n      for (const docChange of snap.docChanges) {\n        if (docChange.type !== 3\n        /* ChangeType.Metadata */\n        ) {\n          docChanges.push(docChange);\n        }\n      }\n\n      snap = new ViewSnapshot(snap.query, snap.docs, snap.oldDocs, docChanges, snap.mutatedKeys, snap.fromCache, snap.syncStateChanged,\n      /* excludesMetadataChanges= */\n      true, snap.hasCachedResults);\n    }\n\n    let raisedEvent = false;\n\n    if (!this.raisedInitialEvent) {\n      if (this.shouldRaiseInitialEvent(snap, this.onlineState)) {\n        this.raiseInitialEvent(snap);\n        raisedEvent = true;\n      }\n    } else if (this.shouldRaiseEvent(snap)) {\n      this.queryObserver.next(snap);\n      raisedEvent = true;\n    }\n\n    this.snap = snap;\n    return raisedEvent;\n  }\n\n  onError(error) {\n    this.queryObserver.error(error);\n  }\n  /** Returns whether a snapshot was raised. */\n\n\n  applyOnlineStateChange(onlineState) {\n    this.onlineState = onlineState;\n    let raisedEvent = false;\n\n    if (this.snap && !this.raisedInitialEvent && this.shouldRaiseInitialEvent(this.snap, onlineState)) {\n      this.raiseInitialEvent(this.snap);\n      raisedEvent = true;\n    }\n\n    return raisedEvent;\n  }\n\n  shouldRaiseInitialEvent(snap, onlineState) {\n    // Always raise the first event when we're synced\n    if (!snap.fromCache) {\n      return true;\n    } // NOTE: We consider OnlineState.Unknown as online (it should become Offline\n    // or Online if we wait long enough).\n\n\n    const maybeOnline = onlineState !== \"Offline\"\n    /* OnlineState.Offline */\n    ; // Don't raise the event if we're online, aren't synced yet (checked\n    // above) and are waiting for a sync.\n\n    if (this.options.waitForSyncWhenOnline && maybeOnline) {\n      return false;\n    } // Raise data from cache if we have any documents, have cached results before,\n    // or we are offline.\n\n\n    return !snap.docs.isEmpty() || snap.hasCachedResults || onlineState === \"Offline\"\n    /* OnlineState.Offline */\n    ;\n  }\n\n  shouldRaiseEvent(snap) {\n    // We don't need to handle includeDocumentMetadataChanges here because\n    // the Metadata only changes have already been stripped out if needed.\n    // At this point the only changes we will see are the ones we should\n    // propagate.\n    if (snap.docChanges.length > 0) {\n      return true;\n    }\n\n    const hasPendingWritesChanged = this.snap && this.snap.hasPendingWrites !== snap.hasPendingWrites;\n\n    if (snap.syncStateChanged || hasPendingWritesChanged) {\n      return this.options.includeMetadataChanges === true;\n    } // Generally we should have hit one of the cases above, but it's possible\n    // to get here if there were only metadata docChanges and they got\n    // stripped out.\n\n\n    return false;\n  }\n\n  raiseInitialEvent(snap) {\n    snap = ViewSnapshot.fromInitialDocuments(snap.query, snap.docs, snap.mutatedKeys, snap.fromCache, snap.hasCachedResults);\n    this.raisedInitialEvent = true;\n    this.queryObserver.next(snap);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A set of changes to what documents are currently in view and out of view for\r\n * a given query. These changes are sent to the LocalStore by the View (via\r\n * the SyncEngine) and are used to pin / unpin documents as appropriate.\r\n */\n\n\nclass LocalViewChanges {\n  constructor(targetId, fromCache, addedKeys, removedKeys) {\n    this.targetId = targetId;\n    this.fromCache = fromCache;\n    this.addedKeys = addedKeys;\n    this.removedKeys = removedKeys;\n  }\n\n  static fromSnapshot(targetId, viewSnapshot) {\n    let addedKeys = documentKeySet();\n    let removedKeys = documentKeySet();\n\n    for (const docChange of viewSnapshot.docChanges) {\n      switch (docChange.type) {\n        case 0\n        /* ChangeType.Added */\n        :\n          addedKeys = addedKeys.add(docChange.doc.key);\n          break;\n\n        case 1\n        /* ChangeType.Removed */\n        :\n          removedKeys = removedKeys.add(docChange.doc.key);\n          break;\n        // do nothing\n      }\n    }\n\n    return new LocalViewChanges(targetId, viewSnapshot.fromCache, addedKeys, removedKeys);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Helper to convert objects from bundles to model objects in the SDK.\r\n */\n\n\nclass BundleConverterImpl {\n  constructor(serializer) {\n    this.serializer = serializer;\n  }\n\n  toDocumentKey(name) {\n    return fromName(this.serializer, name);\n  }\n  /**\r\n   * Converts a BundleDocument to a MutableDocument.\r\n   */\n\n\n  toMutableDocument(bundledDoc) {\n    if (bundledDoc.metadata.exists) {\n      return fromDocument(this.serializer, bundledDoc.document, false);\n    } else {\n      return MutableDocument.newNoDocument(this.toDocumentKey(bundledDoc.metadata.name), this.toSnapshotVersion(bundledDoc.metadata.readTime));\n    }\n  }\n\n  toSnapshotVersion(time) {\n    return fromVersion(time);\n  }\n\n}\n/**\r\n * A class to process the elements from a bundle, load them into local\r\n * storage and provide progress update while loading.\r\n */\n\n\nclass BundleLoader {\n  constructor(bundleMetadata, localStore, serializer) {\n    this.bundleMetadata = bundleMetadata;\n    this.localStore = localStore;\n    this.serializer = serializer;\n    /** Batched queries to be saved into storage */\n\n    this.queries = [];\n    /** Batched documents to be saved into storage */\n\n    this.documents = [];\n    /** The collection groups affected by this bundle. */\n\n    this.collectionGroups = new Set();\n    this.progress = bundleInitialProgress(bundleMetadata);\n  }\n  /**\r\n   * Adds an element from the bundle to the loader.\r\n   *\r\n   * Returns a new progress if adding the element leads to a new progress,\r\n   * otherwise returns null.\r\n   */\n\n\n  addSizedElement(element) {\n    this.progress.bytesLoaded += element.byteLength;\n    let documentsLoaded = this.progress.documentsLoaded;\n\n    if (element.payload.namedQuery) {\n      this.queries.push(element.payload.namedQuery);\n    } else if (element.payload.documentMetadata) {\n      this.documents.push({\n        metadata: element.payload.documentMetadata\n      });\n\n      if (!element.payload.documentMetadata.exists) {\n        ++documentsLoaded;\n      }\n\n      const path = ResourcePath.fromString(element.payload.documentMetadata.name);\n      this.collectionGroups.add(path.get(path.length - 2));\n    } else if (element.payload.document) {\n      this.documents[this.documents.length - 1].document = element.payload.document;\n      ++documentsLoaded;\n    }\n\n    if (documentsLoaded !== this.progress.documentsLoaded) {\n      this.progress.documentsLoaded = documentsLoaded;\n      return Object.assign({}, this.progress);\n    }\n\n    return null;\n  }\n\n  getQueryDocumentMapping(documents) {\n    const queryDocumentMap = new Map();\n    const bundleConverter = new BundleConverterImpl(this.serializer);\n\n    for (const bundleDoc of documents) {\n      if (bundleDoc.metadata.queries) {\n        const documentKey = bundleConverter.toDocumentKey(bundleDoc.metadata.name);\n\n        for (const queryName of bundleDoc.metadata.queries) {\n          const documentKeys = (queryDocumentMap.get(queryName) || documentKeySet()).add(documentKey);\n          queryDocumentMap.set(queryName, documentKeys);\n        }\n      }\n    }\n\n    return queryDocumentMap;\n  }\n  /**\r\n   * Update the progress to 'Success' and return the updated progress.\r\n   */\n\n\n  complete() {\n    var _this19 = this;\n\n    return _asyncToGenerator(function* () {\n      const changedDocs = yield localStoreApplyBundledDocuments(_this19.localStore, new BundleConverterImpl(_this19.serializer), _this19.documents, _this19.bundleMetadata.id);\n\n      const queryDocumentMap = _this19.getQueryDocumentMapping(_this19.documents);\n\n      for (const q of _this19.queries) {\n        yield localStoreSaveNamedQuery(_this19.localStore, q, queryDocumentMap.get(q.name));\n      }\n\n      _this19.progress.taskState = 'Success';\n      return {\n        progress: _this19.progress,\n        changedCollectionGroups: _this19.collectionGroups,\n        changedDocs\n      };\n    })();\n  }\n\n}\n/**\r\n * Returns a `LoadBundleTaskProgress` representing the initial progress of\r\n * loading a bundle.\r\n */\n\n\nfunction bundleInitialProgress(metadata) {\n  return {\n    taskState: 'Running',\n    documentsLoaded: 0,\n    bytesLoaded: 0,\n    totalDocuments: metadata.totalDocuments,\n    totalBytes: metadata.totalBytes\n  };\n}\n/**\r\n * Returns a `LoadBundleTaskProgress` representing the progress that the loading\r\n * has succeeded.\r\n */\n\n\nfunction bundleSuccessProgress(metadata) {\n  return {\n    taskState: 'Success',\n    documentsLoaded: metadata.totalDocuments,\n    bytesLoaded: metadata.totalBytes,\n    totalDocuments: metadata.totalDocuments,\n    totalBytes: metadata.totalBytes\n  };\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass AddedLimboDocument {\n  constructor(key) {\n    this.key = key;\n  }\n\n}\n\nclass RemovedLimboDocument {\n  constructor(key) {\n    this.key = key;\n  }\n\n}\n/**\r\n * View is responsible for computing the final merged truth of what docs are in\r\n * a query. It gets notified of local and remote changes to docs, and applies\r\n * the query filters and limits to determine the most correct possible results.\r\n */\n\n\nclass View {\n  constructor(query,\n  /** Documents included in the remote target */\n  _syncedDocuments) {\n    this.query = query;\n    this._syncedDocuments = _syncedDocuments;\n    this.syncState = null;\n    this.hasCachedResults = false;\n    /**\r\n     * A flag whether the view is current with the backend. A view is considered\r\n     * current after it has seen the current flag from the backend and did not\r\n     * lose consistency within the watch stream (e.g. because of an existence\r\n     * filter mismatch).\r\n     */\n\n    this.current = false;\n    /** Documents in the view but not in the remote target */\n\n    this.limboDocuments = documentKeySet();\n    /** Document Keys that have local changes */\n\n    this.mutatedKeys = documentKeySet();\n    this.docComparator = newQueryComparator(query);\n    this.documentSet = new DocumentSet(this.docComparator);\n  }\n  /**\r\n   * The set of remote documents that the server has told us belongs to the target associated with\r\n   * this view.\r\n   */\n\n\n  get syncedDocuments() {\n    return this._syncedDocuments;\n  }\n  /**\r\n   * Iterates over a set of doc changes, applies the query limit, and computes\r\n   * what the new results should be, what the changes were, and whether we may\r\n   * need to go back to the local cache for more results. Does not make any\r\n   * changes to the view.\r\n   * @param docChanges - The doc changes to apply to this view.\r\n   * @param previousChanges - If this is being called with a refill, then start\r\n   *        with this set of docs and changes instead of the current view.\r\n   * @returns a new set of docs, changes, and refill flag.\r\n   */\n\n\n  computeDocChanges(docChanges, previousChanges) {\n    const changeSet = previousChanges ? previousChanges.changeSet : new DocumentChangeSet();\n    const oldDocumentSet = previousChanges ? previousChanges.documentSet : this.documentSet;\n    let newMutatedKeys = previousChanges ? previousChanges.mutatedKeys : this.mutatedKeys;\n    let newDocumentSet = oldDocumentSet;\n    let needsRefill = false; // Track the last doc in a (full) limit. This is necessary, because some\n    // update (a delete, or an update moving a doc past the old limit) might\n    // mean there is some other document in the local cache that either should\n    // come (1) between the old last limit doc and the new last document, in the\n    // case of updates, or (2) after the new last document, in the case of\n    // deletes. So we keep this doc at the old limit to compare the updates to.\n    //\n    // Note that this should never get used in a refill (when previousChanges is\n    // set), because there will only be adds -- no deletes or updates.\n\n    const lastDocInLimit = this.query.limitType === \"F\"\n    /* LimitType.First */\n    && oldDocumentSet.size === this.query.limit ? oldDocumentSet.last() : null;\n    const firstDocInLimit = this.query.limitType === \"L\"\n    /* LimitType.Last */\n    && oldDocumentSet.size === this.query.limit ? oldDocumentSet.first() : null;\n    docChanges.inorderTraversal((key, entry) => {\n      const oldDoc = oldDocumentSet.get(key);\n      const newDoc = queryMatches(this.query, entry) ? entry : null;\n      const oldDocHadPendingMutations = oldDoc ? this.mutatedKeys.has(oldDoc.key) : false;\n      const newDocHasPendingMutations = newDoc ? newDoc.hasLocalMutations || // We only consider committed mutations for documents that were\n      // mutated during the lifetime of the view.\n      this.mutatedKeys.has(newDoc.key) && newDoc.hasCommittedMutations : false;\n      let changeApplied = false; // Calculate change\n\n      if (oldDoc && newDoc) {\n        const docsEqual = oldDoc.data.isEqual(newDoc.data);\n\n        if (!docsEqual) {\n          if (!this.shouldWaitForSyncedDocument(oldDoc, newDoc)) {\n            changeSet.track({\n              type: 2\n              /* ChangeType.Modified */\n              ,\n              doc: newDoc\n            });\n            changeApplied = true;\n\n            if (lastDocInLimit && this.docComparator(newDoc, lastDocInLimit) > 0 || firstDocInLimit && this.docComparator(newDoc, firstDocInLimit) < 0) {\n              // This doc moved from inside the limit to outside the limit.\n              // That means there may be some other doc in the local cache\n              // that should be included instead.\n              needsRefill = true;\n            }\n          }\n        } else if (oldDocHadPendingMutations !== newDocHasPendingMutations) {\n          changeSet.track({\n            type: 3\n            /* ChangeType.Metadata */\n            ,\n            doc: newDoc\n          });\n          changeApplied = true;\n        }\n      } else if (!oldDoc && newDoc) {\n        changeSet.track({\n          type: 0\n          /* ChangeType.Added */\n          ,\n          doc: newDoc\n        });\n        changeApplied = true;\n      } else if (oldDoc && !newDoc) {\n        changeSet.track({\n          type: 1\n          /* ChangeType.Removed */\n          ,\n          doc: oldDoc\n        });\n        changeApplied = true;\n\n        if (lastDocInLimit || firstDocInLimit) {\n          // A doc was removed from a full limit query. We'll need to\n          // requery from the local cache to see if we know about some other\n          // doc that should be in the results.\n          needsRefill = true;\n        }\n      }\n\n      if (changeApplied) {\n        if (newDoc) {\n          newDocumentSet = newDocumentSet.add(newDoc);\n\n          if (newDocHasPendingMutations) {\n            newMutatedKeys = newMutatedKeys.add(key);\n          } else {\n            newMutatedKeys = newMutatedKeys.delete(key);\n          }\n        } else {\n          newDocumentSet = newDocumentSet.delete(key);\n          newMutatedKeys = newMutatedKeys.delete(key);\n        }\n      }\n    }); // Drop documents out to meet limit/limitToLast requirement.\n\n    if (this.query.limit !== null) {\n      while (newDocumentSet.size > this.query.limit) {\n        const oldDoc = this.query.limitType === \"F\"\n        /* LimitType.First */\n        ? newDocumentSet.last() : newDocumentSet.first();\n        newDocumentSet = newDocumentSet.delete(oldDoc.key);\n        newMutatedKeys = newMutatedKeys.delete(oldDoc.key);\n        changeSet.track({\n          type: 1\n          /* ChangeType.Removed */\n          ,\n          doc: oldDoc\n        });\n      }\n    }\n\n    return {\n      documentSet: newDocumentSet,\n      changeSet,\n      needsRefill,\n      mutatedKeys: newMutatedKeys\n    };\n  }\n\n  shouldWaitForSyncedDocument(oldDoc, newDoc) {\n    // We suppress the initial change event for documents that were modified as\n    // part of a write acknowledgment (e.g. when the value of a server transform\n    // is applied) as Watch will send us the same document again.\n    // By suppressing the event, we only raise two user visible events (one with\n    // `hasPendingWrites` and the final state of the document) instead of three\n    // (one with `hasPendingWrites`, the modified document with\n    // `hasPendingWrites` and the final state of the document).\n    return oldDoc.hasLocalMutations && newDoc.hasCommittedMutations && !newDoc.hasLocalMutations;\n  }\n  /**\r\n   * Updates the view with the given ViewDocumentChanges and optionally updates\r\n   * limbo docs and sync state from the provided target change.\r\n   * @param docChanges - The set of changes to make to the view's docs.\r\n   * @param updateLimboDocuments - Whether to update limbo documents based on\r\n   *        this change.\r\n   * @param targetChange - A target change to apply for computing limbo docs and\r\n   *        sync state.\r\n   * @returns A new ViewChange with the given docs, changes, and sync state.\r\n   */\n  // PORTING NOTE: The iOS/Android clients always compute limbo document changes.\n\n\n  applyChanges(docChanges, updateLimboDocuments, targetChange) {\n    const oldDocs = this.documentSet;\n    this.documentSet = docChanges.documentSet;\n    this.mutatedKeys = docChanges.mutatedKeys; // Sort changes based on type and query comparator\n\n    const changes = docChanges.changeSet.getChanges();\n    changes.sort((c1, c2) => {\n      return compareChangeType(c1.type, c2.type) || this.docComparator(c1.doc, c2.doc);\n    });\n    this.applyTargetChange(targetChange);\n    const limboChanges = updateLimboDocuments ? this.updateLimboDocuments() : [];\n    const synced = this.limboDocuments.size === 0 && this.current;\n    const newSyncState = synced ? 1\n    /* SyncState.Synced */\n    : 0\n    /* SyncState.Local */\n    ;\n    const syncStateChanged = newSyncState !== this.syncState;\n    this.syncState = newSyncState;\n\n    if (changes.length === 0 && !syncStateChanged) {\n      // no changes\n      return {\n        limboChanges\n      };\n    } else {\n      const snap = new ViewSnapshot(this.query, docChanges.documentSet, oldDocs, changes, docChanges.mutatedKeys, newSyncState === 0\n      /* SyncState.Local */\n      , syncStateChanged,\n      /* excludesMetadataChanges= */\n      false, targetChange ? targetChange.resumeToken.approximateByteSize() > 0 : false);\n      return {\n        snapshot: snap,\n        limboChanges\n      };\n    }\n  }\n  /**\r\n   * Applies an OnlineState change to the view, potentially generating a\r\n   * ViewChange if the view's syncState changes as a result.\r\n   */\n\n\n  applyOnlineStateChange(onlineState) {\n    if (this.current && onlineState === \"Offline\"\n    /* OnlineState.Offline */\n    ) {\n      // If we're offline, set `current` to false and then call applyChanges()\n      // to refresh our syncState and generate a ViewChange as appropriate. We\n      // are guaranteed to get a new TargetChange that sets `current` back to\n      // true once the client is back online.\n      this.current = false;\n      return this.applyChanges({\n        documentSet: this.documentSet,\n        changeSet: new DocumentChangeSet(),\n        mutatedKeys: this.mutatedKeys,\n        needsRefill: false\n      },\n      /* updateLimboDocuments= */\n      false);\n    } else {\n      // No effect, just return a no-op ViewChange.\n      return {\n        limboChanges: []\n      };\n    }\n  }\n  /**\r\n   * Returns whether the doc for the given key should be in limbo.\r\n   */\n\n\n  shouldBeInLimbo(key) {\n    // If the remote end says it's part of this query, it's not in limbo.\n    if (this._syncedDocuments.has(key)) {\n      return false;\n    } // The local store doesn't think it's a result, so it shouldn't be in limbo.\n\n\n    if (!this.documentSet.has(key)) {\n      return false;\n    } // If there are local changes to the doc, they might explain why the server\n    // doesn't know that it's part of the query. So don't put it in limbo.\n    // TODO(klimt): Ideally, we would only consider changes that might actually\n    // affect this specific query.\n\n\n    if (this.documentSet.get(key).hasLocalMutations) {\n      return false;\n    } // Everything else is in limbo.\n\n\n    return true;\n  }\n  /**\r\n   * Updates syncedDocuments, current, and limbo docs based on the given change.\r\n   * Returns the list of changes to which docs are in limbo.\r\n   */\n\n\n  applyTargetChange(targetChange) {\n    if (targetChange) {\n      targetChange.addedDocuments.forEach(key => this._syncedDocuments = this._syncedDocuments.add(key));\n      targetChange.modifiedDocuments.forEach(key => {});\n      targetChange.removedDocuments.forEach(key => this._syncedDocuments = this._syncedDocuments.delete(key));\n      this.current = targetChange.current;\n    }\n  }\n\n  updateLimboDocuments() {\n    // We can only determine limbo documents when we're in-sync with the server.\n    if (!this.current) {\n      return [];\n    } // TODO(klimt): Do this incrementally so that it's not quadratic when\n    // updating many documents.\n\n\n    const oldLimboDocuments = this.limboDocuments;\n    this.limboDocuments = documentKeySet();\n    this.documentSet.forEach(doc => {\n      if (this.shouldBeInLimbo(doc.key)) {\n        this.limboDocuments = this.limboDocuments.add(doc.key);\n      }\n    }); // Diff the new limbo docs with the old limbo docs.\n\n    const changes = [];\n    oldLimboDocuments.forEach(key => {\n      if (!this.limboDocuments.has(key)) {\n        changes.push(new RemovedLimboDocument(key));\n      }\n    });\n    this.limboDocuments.forEach(key => {\n      if (!oldLimboDocuments.has(key)) {\n        changes.push(new AddedLimboDocument(key));\n      }\n    });\n    return changes;\n  }\n  /**\r\n   * Update the in-memory state of the current view with the state read from\r\n   * persistence.\r\n   *\r\n   * We update the query view whenever a client's primary status changes:\r\n   * - When a client transitions from primary to secondary, it can miss\r\n   *   LocalStorage updates and its query views may temporarily not be\r\n   *   synchronized with the state on disk.\r\n   * - For secondary to primary transitions, the client needs to update the list\r\n   *   of `syncedDocuments` since secondary clients update their query views\r\n   *   based purely on synthesized RemoteEvents.\r\n   *\r\n   * @param queryResult.documents - The documents that match the query according\r\n   * to the LocalStore.\r\n   * @param queryResult.remoteKeys - The keys of the documents that match the\r\n   * query according to the backend.\r\n   *\r\n   * @returns The ViewChange that resulted from this synchronization.\r\n   */\n  // PORTING NOTE: Multi-tab only.\n\n\n  synchronizeWithPersistedState(queryResult) {\n    this._syncedDocuments = queryResult.remoteKeys;\n    this.limboDocuments = documentKeySet();\n    const docChanges = this.computeDocChanges(queryResult.documents);\n    return this.applyChanges(docChanges,\n    /*updateLimboDocuments=*/\n    true);\n  }\n  /**\r\n   * Returns a view snapshot as if this query was just listened to. Contains\r\n   * a document add for every existing document and the `fromCache` and\r\n   * `hasPendingWrites` status of the already established view.\r\n   */\n  // PORTING NOTE: Multi-tab only.\n\n\n  computeInitialSnapshot() {\n    return ViewSnapshot.fromInitialDocuments(this.query, this.documentSet, this.mutatedKeys, this.syncState === 0\n    /* SyncState.Local */\n    , this.hasCachedResults);\n  }\n\n}\n\nfunction compareChangeType(c1, c2) {\n  const order = change => {\n    switch (change) {\n      case 0\n      /* ChangeType.Added */\n      :\n        return 1;\n\n      case 2\n      /* ChangeType.Modified */\n      :\n        return 2;\n\n      case 3\n      /* ChangeType.Metadata */\n      :\n        // A metadata change is converted to a modified change at the public\n        // api layer.  Since we sort by document key and then change type,\n        // metadata and modified changes must be sorted equivalently.\n        return 2;\n\n      case 1\n      /* ChangeType.Removed */\n      :\n        return 0;\n\n      default:\n        return fail();\n    }\n  };\n\n  return order(c1) - order(c2);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$3 = 'SyncEngine';\n/**\r\n * QueryView contains all of the data that SyncEngine needs to keep track of for\r\n * a particular query.\r\n */\n\nclass QueryView {\n  constructor(\n  /**\r\n   * The query itself.\r\n   */\n  query,\n  /**\r\n   * The target number created by the client that is used in the watch\r\n   * stream to identify this query.\r\n   */\n  targetId,\n  /**\r\n   * The view is responsible for computing the final merged truth of what\r\n   * docs are in the query. It gets notified of local and remote changes,\r\n   * and applies the query filters and limits to determine the most correct\r\n   * possible results.\r\n   */\n  view) {\n    this.query = query;\n    this.targetId = targetId;\n    this.view = view;\n  }\n\n}\n/** Tracks a limbo resolution. */\n\n\nclass LimboResolution {\n  constructor(key) {\n    this.key = key;\n    /**\r\n     * Set to true once we've received a document. This is used in\r\n     * getRemoteKeysForTarget() and ultimately used by WatchChangeAggregator to\r\n     * decide whether it needs to manufacture a delete event for the target once\r\n     * the target is CURRENT.\r\n     */\n\n    this.receivedDocument = false;\n  }\n\n}\n/**\r\n * An implementation of `SyncEngine` coordinating with other parts of SDK.\r\n *\r\n * The parts of SyncEngine that act as a callback to RemoteStore need to be\r\n * registered individually. This is done in `syncEngineWrite()` and\r\n * `syncEngineListen()` (as well as `applyPrimaryState()`) as these methods\r\n * serve as entry points to RemoteStore's functionality.\r\n *\r\n * Note: some field defined in this class might have public access level, but\r\n * the class is not exported so they are only accessible from this module.\r\n * This is useful to implement optional features (like bundles) in free\r\n * functions, such that they are tree-shakeable.\r\n */\n\n\nclass SyncEngineImpl {\n  constructor(localStore, remoteStore, eventManager, // PORTING NOTE: Manages state synchronization in multi-tab environments.\n  sharedClientState, currentUser, maxConcurrentLimboResolutions) {\n    this.localStore = localStore;\n    this.remoteStore = remoteStore;\n    this.eventManager = eventManager;\n    this.sharedClientState = sharedClientState;\n    this.currentUser = currentUser;\n    this.maxConcurrentLimboResolutions = maxConcurrentLimboResolutions;\n    this.syncEngineListener = {};\n    this.queryViewsByQuery = new ObjectMap(q => canonifyQuery(q), queryEquals);\n    this.queriesByTarget = new Map();\n    /**\r\n     * The keys of documents that are in limbo for which we haven't yet started a\r\n     * limbo resolution query. The strings in this set are the result of calling\r\n     * `key.path.canonicalString()` where `key` is a `DocumentKey` object.\r\n     *\r\n     * The `Set` type was chosen because it provides efficient lookup and removal\r\n     * of arbitrary elements and it also maintains insertion order, providing the\r\n     * desired queue-like FIFO semantics.\r\n     */\n\n    this.enqueuedLimboResolutions = new Set();\n    /**\r\n     * Keeps track of the target ID for each document that is in limbo with an\r\n     * active target.\r\n     */\n\n    this.activeLimboTargetsByKey = new SortedMap(DocumentKey.comparator);\n    /**\r\n     * Keeps track of the information about an active limbo resolution for each\r\n     * active target ID that was started for the purpose of limbo resolution.\r\n     */\n\n    this.activeLimboResolutionsByTarget = new Map();\n    this.limboDocumentRefs = new ReferenceSet();\n    /** Stores user completion handlers, indexed by User and BatchId. */\n\n    this.mutationUserCallbacks = {};\n    /** Stores user callbacks waiting for all pending writes to be acknowledged. */\n\n    this.pendingWritesCallbacks = new Map();\n    this.limboTargetIdGenerator = TargetIdGenerator.forSyncEngine();\n    this.onlineState = \"Unknown\"\n    /* OnlineState.Unknown */\n    ; // The primary state is set to `true` or `false` immediately after Firestore\n    // startup. In the interim, a client should only be considered primary if\n    // `isPrimary` is true.\n\n    this._isPrimaryClient = undefined;\n  }\n\n  get isPrimaryClient() {\n    return this._isPrimaryClient === true;\n  }\n\n}\n\nfunction newSyncEngine(localStore, remoteStore, eventManager, // PORTING NOTE: Manages state synchronization in multi-tab environments.\nsharedClientState, currentUser, maxConcurrentLimboResolutions, isPrimary) {\n  const syncEngine = new SyncEngineImpl(localStore, remoteStore, eventManager, sharedClientState, currentUser, maxConcurrentLimboResolutions);\n\n  if (isPrimary) {\n    syncEngine._isPrimaryClient = true;\n  }\n\n  return syncEngine;\n}\n/**\r\n * Initiates the new listen, resolves promise when listen enqueued to the\r\n * server. All the subsequent view snapshots or errors are sent to the\r\n * subscribed handlers. Returns the initial snapshot.\r\n */\n\n\nfunction syncEngineListen(_x63, _x64) {\n  return _syncEngineListen.apply(this, arguments);\n}\n/**\r\n * Registers a view for a previously unknown query and computes its initial\r\n * snapshot.\r\n */\n\n\nfunction _syncEngineListen() {\n  _syncEngineListen = _asyncToGenerator(function* (syncEngine, query) {\n    const syncEngineImpl = ensureWatchCallbacks(syncEngine);\n    let targetId;\n    let viewSnapshot;\n    const queryView = syncEngineImpl.queryViewsByQuery.get(query);\n\n    if (queryView) {\n      // PORTING NOTE: With Multi-Tab Web, it is possible that a query view\n      // already exists when EventManager calls us for the first time. This\n      // happens when the primary tab is already listening to this query on\n      // behalf of another tab and the user of the primary also starts listening\n      // to the query. EventManager will not have an assigned target ID in this\n      // case and calls `listen` to obtain this ID.\n      targetId = queryView.targetId;\n      syncEngineImpl.sharedClientState.addLocalQueryTarget(targetId);\n      viewSnapshot = queryView.view.computeInitialSnapshot();\n    } else {\n      const targetData = yield localStoreAllocateTarget(syncEngineImpl.localStore, queryToTarget(query));\n      const status = syncEngineImpl.sharedClientState.addLocalQueryTarget(targetData.targetId);\n      targetId = targetData.targetId;\n      viewSnapshot = yield initializeViewAndComputeSnapshot(syncEngineImpl, query, targetId, status === 'current', targetData.resumeToken);\n\n      if (syncEngineImpl.isPrimaryClient) {\n        remoteStoreListen(syncEngineImpl.remoteStore, targetData);\n      }\n    }\n\n    return viewSnapshot;\n  });\n  return _syncEngineListen.apply(this, arguments);\n}\n\nfunction initializeViewAndComputeSnapshot(_x65, _x66, _x67, _x68, _x69) {\n  return _initializeViewAndComputeSnapshot.apply(this, arguments);\n}\n/** Stops listening to the query. */\n\n\nfunction _initializeViewAndComputeSnapshot() {\n  _initializeViewAndComputeSnapshot = _asyncToGenerator(function* (syncEngineImpl, query, targetId, current, resumeToken) {\n    // PORTING NOTE: On Web only, we inject the code that registers new Limbo\n    // targets based on view changes. This allows us to only depend on Limbo\n    // changes when user code includes queries.\n    syncEngineImpl.applyDocChanges = (queryView, changes, remoteEvent) => applyDocChanges(syncEngineImpl, queryView, changes, remoteEvent);\n\n    const queryResult = yield localStoreExecuteQuery(syncEngineImpl.localStore, query,\n    /* usePreviousResults= */\n    true);\n    const view = new View(query, queryResult.remoteKeys);\n    const viewDocChanges = view.computeDocChanges(queryResult.documents);\n    const synthesizedTargetChange = TargetChange.createSynthesizedTargetChangeForCurrentChange(targetId, current && syncEngineImpl.onlineState !== \"Offline\"\n    /* OnlineState.Offline */\n    , resumeToken);\n    const viewChange = view.applyChanges(viewDocChanges,\n    /* updateLimboDocuments= */\n    syncEngineImpl.isPrimaryClient, synthesizedTargetChange);\n    updateTrackedLimbos(syncEngineImpl, targetId, viewChange.limboChanges);\n    const data = new QueryView(query, targetId, view);\n    syncEngineImpl.queryViewsByQuery.set(query, data);\n\n    if (syncEngineImpl.queriesByTarget.has(targetId)) {\n      syncEngineImpl.queriesByTarget.get(targetId).push(query);\n    } else {\n      syncEngineImpl.queriesByTarget.set(targetId, [query]);\n    }\n\n    return viewChange.snapshot;\n  });\n  return _initializeViewAndComputeSnapshot.apply(this, arguments);\n}\n\nfunction syncEngineUnlisten(_x70, _x71) {\n  return _syncEngineUnlisten.apply(this, arguments);\n}\n/**\r\n * Initiates the write of local mutation batch which involves adding the\r\n * writes to the mutation queue, notifying the remote store about new\r\n * mutations and raising events for any changes this write caused.\r\n *\r\n * The promise returned by this call is resolved when the above steps\r\n * have completed, *not* when the write was acked by the backend. The\r\n * userCallback is resolved once the write was acked/rejected by the\r\n * backend (or failed locally for any other reason).\r\n */\n\n\nfunction _syncEngineUnlisten() {\n  _syncEngineUnlisten = _asyncToGenerator(function* (syncEngine, query) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const queryView = syncEngineImpl.queryViewsByQuery.get(query); // Only clean up the query view and target if this is the only query mapped\n    // to the target.\n\n    const queries = syncEngineImpl.queriesByTarget.get(queryView.targetId);\n\n    if (queries.length > 1) {\n      syncEngineImpl.queriesByTarget.set(queryView.targetId, queries.filter(q => !queryEquals(q, query)));\n      syncEngineImpl.queryViewsByQuery.delete(query);\n      return;\n    } // No other queries are mapped to the target, clean up the query and the target.\n\n\n    if (syncEngineImpl.isPrimaryClient) {\n      // We need to remove the local query target first to allow us to verify\n      // whether any other client is still interested in this target.\n      syncEngineImpl.sharedClientState.removeLocalQueryTarget(queryView.targetId);\n      const targetRemainsActive = syncEngineImpl.sharedClientState.isActiveQueryTarget(queryView.targetId);\n\n      if (!targetRemainsActive) {\n        yield localStoreReleaseTarget(syncEngineImpl.localStore, queryView.targetId,\n        /*keepPersistedTargetData=*/\n        false).then(() => {\n          syncEngineImpl.sharedClientState.clearQueryState(queryView.targetId);\n          remoteStoreUnlisten(syncEngineImpl.remoteStore, queryView.targetId);\n          removeAndCleanupTarget(syncEngineImpl, queryView.targetId);\n        }).catch(ignoreIfPrimaryLeaseLoss);\n      }\n    } else {\n      removeAndCleanupTarget(syncEngineImpl, queryView.targetId);\n      yield localStoreReleaseTarget(syncEngineImpl.localStore, queryView.targetId,\n      /*keepPersistedTargetData=*/\n      true);\n    }\n  });\n  return _syncEngineUnlisten.apply(this, arguments);\n}\n\nfunction syncEngineWrite(_x72, _x73, _x74) {\n  return _syncEngineWrite.apply(this, arguments);\n}\n/**\r\n * Applies one remote event to the sync engine, notifying any views of the\r\n * changes, and releasing any pending mutation batches that would become\r\n * visible because of the snapshot version the remote event contains.\r\n */\n\n\nfunction _syncEngineWrite() {\n  _syncEngineWrite = _asyncToGenerator(function* (syncEngine, batch, userCallback) {\n    const syncEngineImpl = syncEngineEnsureWriteCallbacks(syncEngine);\n\n    try {\n      const result = yield localStoreWriteLocally(syncEngineImpl.localStore, batch);\n      syncEngineImpl.sharedClientState.addPendingMutation(result.batchId);\n      addMutationCallback(syncEngineImpl, result.batchId, userCallback);\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, result.changes);\n      yield fillWritePipeline(syncEngineImpl.remoteStore);\n    } catch (e) {\n      // If we can't persist the mutation, we reject the user callback and\n      // don't send the mutation. The user can then retry the write.\n      const error = wrapInUserErrorIfRecoverable(e, `Failed to persist write`);\n      userCallback.reject(error);\n    }\n  });\n  return _syncEngineWrite.apply(this, arguments);\n}\n\nfunction syncEngineApplyRemoteEvent(_x75, _x76) {\n  return _syncEngineApplyRemoteEvent.apply(this, arguments);\n}\n/**\r\n * Applies an OnlineState change to the sync engine and notifies any views of\r\n * the change.\r\n */\n\n\nfunction _syncEngineApplyRemoteEvent() {\n  _syncEngineApplyRemoteEvent = _asyncToGenerator(function* (syncEngine, remoteEvent) {\n    const syncEngineImpl = debugCast(syncEngine);\n\n    try {\n      const changes = yield localStoreApplyRemoteEventToLocalCache(syncEngineImpl.localStore, remoteEvent); // Update `receivedDocument` as appropriate for any limbo targets.\n\n      remoteEvent.targetChanges.forEach((targetChange, targetId) => {\n        const limboResolution = syncEngineImpl.activeLimboResolutionsByTarget.get(targetId);\n\n        if (limboResolution) {\n          // Since this is a limbo resolution lookup, it's for a single document\n          // and it could be added, modified, or removed, but not a combination.\n          hardAssert(targetChange.addedDocuments.size + targetChange.modifiedDocuments.size + targetChange.removedDocuments.size <= 1);\n\n          if (targetChange.addedDocuments.size > 0) {\n            limboResolution.receivedDocument = true;\n          } else if (targetChange.modifiedDocuments.size > 0) {\n            hardAssert(limboResolution.receivedDocument);\n          } else if (targetChange.removedDocuments.size > 0) {\n            hardAssert(limboResolution.receivedDocument);\n            limboResolution.receivedDocument = false;\n          } else {// This was probably just a CURRENT targetChange or similar.\n          }\n        }\n      });\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, changes, remoteEvent);\n    } catch (error) {\n      yield ignoreIfPrimaryLeaseLoss(error);\n    }\n  });\n  return _syncEngineApplyRemoteEvent.apply(this, arguments);\n}\n\nfunction syncEngineApplyOnlineStateChange(syncEngine, onlineState, source) {\n  const syncEngineImpl = debugCast(syncEngine); // If we are the secondary client, we explicitly ignore the remote store's\n  // online state (the local client may go offline, even though the primary\n  // tab remains online) and only apply the primary tab's online state from\n  // SharedClientState.\n\n  if (syncEngineImpl.isPrimaryClient && source === 0\n  /* OnlineStateSource.RemoteStore */\n  || !syncEngineImpl.isPrimaryClient && source === 1\n  /* OnlineStateSource.SharedClientState */\n  ) {\n    const newViewSnapshots = [];\n    syncEngineImpl.queryViewsByQuery.forEach((query, queryView) => {\n      const viewChange = queryView.view.applyOnlineStateChange(onlineState);\n\n      if (viewChange.snapshot) {\n        newViewSnapshots.push(viewChange.snapshot);\n      }\n    });\n    eventManagerOnOnlineStateChange(syncEngineImpl.eventManager, onlineState);\n\n    if (newViewSnapshots.length) {\n      syncEngineImpl.syncEngineListener.onWatchChange(newViewSnapshots);\n    }\n\n    syncEngineImpl.onlineState = onlineState;\n\n    if (syncEngineImpl.isPrimaryClient) {\n      syncEngineImpl.sharedClientState.setOnlineState(onlineState);\n    }\n  }\n}\n/**\r\n * Rejects the listen for the given targetID. This can be triggered by the\r\n * backend for any active target.\r\n *\r\n * @param syncEngine - The sync engine implementation.\r\n * @param targetId - The targetID corresponds to one previously initiated by the\r\n * user as part of TargetData passed to listen() on RemoteStore.\r\n * @param err - A description of the condition that has forced the rejection.\r\n * Nearly always this will be an indication that the user is no longer\r\n * authorized to see the data matching the target.\r\n */\n\n\nfunction syncEngineRejectListen(_x77, _x78, _x79) {\n  return _syncEngineRejectListen.apply(this, arguments);\n}\n\nfunction _syncEngineRejectListen() {\n  _syncEngineRejectListen = _asyncToGenerator(function* (syncEngine, targetId, err) {\n    const syncEngineImpl = debugCast(syncEngine); // PORTING NOTE: Multi-tab only.\n\n    syncEngineImpl.sharedClientState.updateQueryState(targetId, 'rejected', err);\n    const limboResolution = syncEngineImpl.activeLimboResolutionsByTarget.get(targetId);\n    const limboKey = limboResolution && limboResolution.key;\n\n    if (limboKey) {\n      // TODO(klimt): We really only should do the following on permission\n      // denied errors, but we don't have the cause code here.\n      // It's a limbo doc. Create a synthetic event saying it was deleted.\n      // This is kind of a hack. Ideally, we would have a method in the local\n      // store to purge a document. However, it would be tricky to keep all of\n      // the local store's invariants with another method.\n      let documentUpdates = new SortedMap(DocumentKey.comparator); // TODO(b/217189216): This limbo document should ideally have a read time,\n      // so that it is picked up by any read-time based scans. The backend,\n      // however, does not send a read time for target removals.\n\n      documentUpdates = documentUpdates.insert(limboKey, MutableDocument.newNoDocument(limboKey, SnapshotVersion.min()));\n      const resolvedLimboDocuments = documentKeySet().add(limboKey);\n      const event = new RemoteEvent(SnapshotVersion.min(),\n      /* targetChanges= */\n      new Map(),\n      /* targetMismatches= */\n      new SortedMap(primitiveComparator), documentUpdates, resolvedLimboDocuments);\n      yield syncEngineApplyRemoteEvent(syncEngineImpl, event); // Since this query failed, we won't want to manually unlisten to it.\n      // We only remove it from bookkeeping after we successfully applied the\n      // RemoteEvent. If `applyRemoteEvent()` throws, we want to re-listen to\n      // this query when the RemoteStore restarts the Watch stream, which should\n      // re-trigger the target failure.\n\n      syncEngineImpl.activeLimboTargetsByKey = syncEngineImpl.activeLimboTargetsByKey.remove(limboKey);\n      syncEngineImpl.activeLimboResolutionsByTarget.delete(targetId);\n      pumpEnqueuedLimboResolutions(syncEngineImpl);\n    } else {\n      yield localStoreReleaseTarget(syncEngineImpl.localStore, targetId,\n      /* keepPersistedTargetData */\n      false).then(() => removeAndCleanupTarget(syncEngineImpl, targetId, err)).catch(ignoreIfPrimaryLeaseLoss);\n    }\n  });\n  return _syncEngineRejectListen.apply(this, arguments);\n}\n\nfunction syncEngineApplySuccessfulWrite(_x80, _x81) {\n  return _syncEngineApplySuccessfulWrite.apply(this, arguments);\n}\n\nfunction _syncEngineApplySuccessfulWrite() {\n  _syncEngineApplySuccessfulWrite = _asyncToGenerator(function* (syncEngine, mutationBatchResult) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const batchId = mutationBatchResult.batch.batchId;\n\n    try {\n      const changes = yield localStoreAcknowledgeBatch(syncEngineImpl.localStore, mutationBatchResult); // The local store may or may not be able to apply the write result and\n      // raise events immediately (depending on whether the watcher is caught\n      // up), so we raise user callbacks first so that they consistently happen\n      // before listen events.\n\n      processUserCallback(syncEngineImpl, batchId,\n      /*error=*/\n      null);\n      triggerPendingWritesCallbacks(syncEngineImpl, batchId);\n      syncEngineImpl.sharedClientState.updateMutationState(batchId, 'acknowledged');\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, changes);\n    } catch (error) {\n      yield ignoreIfPrimaryLeaseLoss(error);\n    }\n  });\n  return _syncEngineApplySuccessfulWrite.apply(this, arguments);\n}\n\nfunction syncEngineRejectFailedWrite(_x82, _x83, _x84) {\n  return _syncEngineRejectFailedWrite.apply(this, arguments);\n}\n/**\r\n * Registers a user callback that resolves when all pending mutations at the moment of calling\r\n * are acknowledged .\r\n */\n\n\nfunction _syncEngineRejectFailedWrite() {\n  _syncEngineRejectFailedWrite = _asyncToGenerator(function* (syncEngine, batchId, error) {\n    const syncEngineImpl = debugCast(syncEngine);\n\n    try {\n      const changes = yield localStoreRejectBatch(syncEngineImpl.localStore, batchId); // The local store may or may not be able to apply the write result and\n      // raise events immediately (depending on whether the watcher is caught up),\n      // so we raise user callbacks first so that they consistently happen before\n      // listen events.\n\n      processUserCallback(syncEngineImpl, batchId, error);\n      triggerPendingWritesCallbacks(syncEngineImpl, batchId);\n      syncEngineImpl.sharedClientState.updateMutationState(batchId, 'rejected', error);\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, changes);\n    } catch (error) {\n      yield ignoreIfPrimaryLeaseLoss(error);\n    }\n  });\n  return _syncEngineRejectFailedWrite.apply(this, arguments);\n}\n\nfunction syncEngineRegisterPendingWritesCallback(_x85, _x86) {\n  return _syncEngineRegisterPendingWritesCallback.apply(this, arguments);\n}\n/**\r\n * Triggers the callbacks that are waiting for this batch id to get acknowledged by server,\r\n * if there are any.\r\n */\n\n\nfunction _syncEngineRegisterPendingWritesCallback() {\n  _syncEngineRegisterPendingWritesCallback = _asyncToGenerator(function* (syncEngine, callback) {\n    const syncEngineImpl = debugCast(syncEngine);\n\n    if (!canUseNetwork(syncEngineImpl.remoteStore)) {\n      logDebug(LOG_TAG$3, 'The network is disabled. The task returned by ' + \"'awaitPendingWrites()' will not complete until the network is enabled.\");\n    }\n\n    try {\n      const highestBatchId = yield localStoreGetHighestUnacknowledgedBatchId(syncEngineImpl.localStore);\n\n      if (highestBatchId === BATCHID_UNKNOWN) {\n        // Trigger the callback right away if there is no pending writes at the moment.\n        callback.resolve();\n        return;\n      }\n\n      const callbacks = syncEngineImpl.pendingWritesCallbacks.get(highestBatchId) || [];\n      callbacks.push(callback);\n      syncEngineImpl.pendingWritesCallbacks.set(highestBatchId, callbacks);\n    } catch (e) {\n      const firestoreError = wrapInUserErrorIfRecoverable(e, 'Initialization of waitForPendingWrites() operation failed');\n      callback.reject(firestoreError);\n    }\n  });\n  return _syncEngineRegisterPendingWritesCallback.apply(this, arguments);\n}\n\nfunction triggerPendingWritesCallbacks(syncEngineImpl, batchId) {\n  (syncEngineImpl.pendingWritesCallbacks.get(batchId) || []).forEach(callback => {\n    callback.resolve();\n  });\n  syncEngineImpl.pendingWritesCallbacks.delete(batchId);\n}\n/** Reject all outstanding callbacks waiting for pending writes to complete. */\n\n\nfunction rejectOutstandingPendingWritesCallbacks(syncEngineImpl, errorMessage) {\n  syncEngineImpl.pendingWritesCallbacks.forEach(callbacks => {\n    callbacks.forEach(callback => {\n      callback.reject(new FirestoreError(Code.CANCELLED, errorMessage));\n    });\n  });\n  syncEngineImpl.pendingWritesCallbacks.clear();\n}\n\nfunction addMutationCallback(syncEngineImpl, batchId, callback) {\n  let newCallbacks = syncEngineImpl.mutationUserCallbacks[syncEngineImpl.currentUser.toKey()];\n\n  if (!newCallbacks) {\n    newCallbacks = new SortedMap(primitiveComparator);\n  }\n\n  newCallbacks = newCallbacks.insert(batchId, callback);\n  syncEngineImpl.mutationUserCallbacks[syncEngineImpl.currentUser.toKey()] = newCallbacks;\n}\n/**\r\n * Resolves or rejects the user callback for the given batch and then discards\r\n * it.\r\n */\n\n\nfunction processUserCallback(syncEngine, batchId, error) {\n  const syncEngineImpl = debugCast(syncEngine);\n  let newCallbacks = syncEngineImpl.mutationUserCallbacks[syncEngineImpl.currentUser.toKey()]; // NOTE: Mutations restored from persistence won't have callbacks, so it's\n  // okay for there to be no callback for this ID.\n\n  if (newCallbacks) {\n    const callback = newCallbacks.get(batchId);\n\n    if (callback) {\n      if (error) {\n        callback.reject(error);\n      } else {\n        callback.resolve();\n      }\n\n      newCallbacks = newCallbacks.remove(batchId);\n    }\n\n    syncEngineImpl.mutationUserCallbacks[syncEngineImpl.currentUser.toKey()] = newCallbacks;\n  }\n}\n\nfunction removeAndCleanupTarget(syncEngineImpl, targetId, error = null) {\n  syncEngineImpl.sharedClientState.removeLocalQueryTarget(targetId);\n\n  for (const query of syncEngineImpl.queriesByTarget.get(targetId)) {\n    syncEngineImpl.queryViewsByQuery.delete(query);\n\n    if (error) {\n      syncEngineImpl.syncEngineListener.onWatchError(query, error);\n    }\n  }\n\n  syncEngineImpl.queriesByTarget.delete(targetId);\n\n  if (syncEngineImpl.isPrimaryClient) {\n    const limboKeys = syncEngineImpl.limboDocumentRefs.removeReferencesForId(targetId);\n    limboKeys.forEach(limboKey => {\n      const isReferenced = syncEngineImpl.limboDocumentRefs.containsKey(limboKey);\n\n      if (!isReferenced) {\n        // We removed the last reference for this key\n        removeLimboTarget(syncEngineImpl, limboKey);\n      }\n    });\n  }\n}\n\nfunction removeLimboTarget(syncEngineImpl, key) {\n  syncEngineImpl.enqueuedLimboResolutions.delete(key.path.canonicalString()); // It's possible that the target already got removed because the query failed. In that case,\n  // the key won't exist in `limboTargetsByKey`. Only do the cleanup if we still have the target.\n\n  const limboTargetId = syncEngineImpl.activeLimboTargetsByKey.get(key);\n\n  if (limboTargetId === null) {\n    // This target already got removed, because the query failed.\n    return;\n  }\n\n  remoteStoreUnlisten(syncEngineImpl.remoteStore, limboTargetId);\n  syncEngineImpl.activeLimboTargetsByKey = syncEngineImpl.activeLimboTargetsByKey.remove(key);\n  syncEngineImpl.activeLimboResolutionsByTarget.delete(limboTargetId);\n  pumpEnqueuedLimboResolutions(syncEngineImpl);\n}\n\nfunction updateTrackedLimbos(syncEngineImpl, targetId, limboChanges) {\n  for (const limboChange of limboChanges) {\n    if (limboChange instanceof AddedLimboDocument) {\n      syncEngineImpl.limboDocumentRefs.addReference(limboChange.key, targetId);\n      trackLimboChange(syncEngineImpl, limboChange);\n    } else if (limboChange instanceof RemovedLimboDocument) {\n      logDebug(LOG_TAG$3, 'Document no longer in limbo: ' + limboChange.key);\n      syncEngineImpl.limboDocumentRefs.removeReference(limboChange.key, targetId);\n      const isReferenced = syncEngineImpl.limboDocumentRefs.containsKey(limboChange.key);\n\n      if (!isReferenced) {\n        // We removed the last reference for this key\n        removeLimboTarget(syncEngineImpl, limboChange.key);\n      }\n    } else {\n      fail();\n    }\n  }\n}\n\nfunction trackLimboChange(syncEngineImpl, limboChange) {\n  const key = limboChange.key;\n  const keyString = key.path.canonicalString();\n\n  if (!syncEngineImpl.activeLimboTargetsByKey.get(key) && !syncEngineImpl.enqueuedLimboResolutions.has(keyString)) {\n    logDebug(LOG_TAG$3, 'New document in limbo: ' + key);\n    syncEngineImpl.enqueuedLimboResolutions.add(keyString);\n    pumpEnqueuedLimboResolutions(syncEngineImpl);\n  }\n}\n/**\r\n * Starts listens for documents in limbo that are enqueued for resolution,\r\n * subject to a maximum number of concurrent resolutions.\r\n *\r\n * Without bounding the number of concurrent resolutions, the server can fail\r\n * with \"resource exhausted\" errors which can lead to pathological client\r\n * behavior as seen in https://github.com/firebase/firebase-js-sdk/issues/2683.\r\n */\n\n\nfunction pumpEnqueuedLimboResolutions(syncEngineImpl) {\n  while (syncEngineImpl.enqueuedLimboResolutions.size > 0 && syncEngineImpl.activeLimboTargetsByKey.size < syncEngineImpl.maxConcurrentLimboResolutions) {\n    const keyString = syncEngineImpl.enqueuedLimboResolutions.values().next().value;\n    syncEngineImpl.enqueuedLimboResolutions.delete(keyString);\n    const key = new DocumentKey(ResourcePath.fromString(keyString));\n    const limboTargetId = syncEngineImpl.limboTargetIdGenerator.next();\n    syncEngineImpl.activeLimboResolutionsByTarget.set(limboTargetId, new LimboResolution(key));\n    syncEngineImpl.activeLimboTargetsByKey = syncEngineImpl.activeLimboTargetsByKey.insert(key, limboTargetId);\n    remoteStoreListen(syncEngineImpl.remoteStore, new TargetData(queryToTarget(newQueryForPath(key.path)), limboTargetId, \"TargetPurposeLimboResolution\"\n    /* TargetPurpose.LimboResolution */\n    , ListenSequence.INVALID));\n  }\n}\n\nfunction syncEngineEmitNewSnapsAndNotifyLocalStore(_x87, _x88, _x89) {\n  return _syncEngineEmitNewSnapsAndNotifyLocalStore.apply(this, arguments);\n}\n\nfunction _syncEngineEmitNewSnapsAndNotifyLocalStore() {\n  _syncEngineEmitNewSnapsAndNotifyLocalStore = _asyncToGenerator(function* (syncEngine, changes, remoteEvent) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const newSnaps = [];\n    const docChangesInAllViews = [];\n    const queriesProcessed = [];\n\n    if (syncEngineImpl.queryViewsByQuery.isEmpty()) {\n      // Return early since `onWatchChange()` might not have been assigned yet.\n      return;\n    }\n\n    syncEngineImpl.queryViewsByQuery.forEach((_, queryView) => {\n      queriesProcessed.push(syncEngineImpl.applyDocChanges(queryView, changes, remoteEvent).then(viewSnapshot => {\n        // If there are changes, or we are handling a global snapshot, notify\n        // secondary clients to update query state.\n        if (viewSnapshot || remoteEvent) {\n          if (syncEngineImpl.isPrimaryClient) {\n            syncEngineImpl.sharedClientState.updateQueryState(queryView.targetId, (viewSnapshot === null || viewSnapshot === void 0 ? void 0 : viewSnapshot.fromCache) ? 'not-current' : 'current');\n          }\n        } // Update views if there are actual changes.\n\n\n        if (!!viewSnapshot) {\n          newSnaps.push(viewSnapshot);\n          const docChanges = LocalViewChanges.fromSnapshot(queryView.targetId, viewSnapshot);\n          docChangesInAllViews.push(docChanges);\n        }\n      }));\n    });\n    yield Promise.all(queriesProcessed);\n    syncEngineImpl.syncEngineListener.onWatchChange(newSnaps);\n    yield localStoreNotifyLocalViewChanges(syncEngineImpl.localStore, docChangesInAllViews);\n  });\n  return _syncEngineEmitNewSnapsAndNotifyLocalStore.apply(this, arguments);\n}\n\nfunction applyDocChanges(_x90, _x91, _x92, _x93) {\n  return _applyDocChanges.apply(this, arguments);\n}\n\nfunction _applyDocChanges() {\n  _applyDocChanges = _asyncToGenerator(function* (syncEngineImpl, queryView, changes, remoteEvent) {\n    let viewDocChanges = queryView.view.computeDocChanges(changes);\n\n    if (viewDocChanges.needsRefill) {\n      // The query has a limit and some docs were removed, so we need\n      // to re-run the query against the local store to make sure we\n      // didn't lose any good docs that had been past the limit.\n      viewDocChanges = yield localStoreExecuteQuery(syncEngineImpl.localStore, queryView.query,\n      /* usePreviousResults= */\n      false).then(({\n        documents\n      }) => {\n        return queryView.view.computeDocChanges(documents, viewDocChanges);\n      });\n    }\n\n    const targetChange = remoteEvent && remoteEvent.targetChanges.get(queryView.targetId);\n    const viewChange = queryView.view.applyChanges(viewDocChanges,\n    /* updateLimboDocuments= */\n    syncEngineImpl.isPrimaryClient, targetChange);\n    updateTrackedLimbos(syncEngineImpl, queryView.targetId, viewChange.limboChanges);\n    return viewChange.snapshot;\n  });\n  return _applyDocChanges.apply(this, arguments);\n}\n\nfunction syncEngineHandleCredentialChange(_x94, _x95) {\n  return _syncEngineHandleCredentialChange.apply(this, arguments);\n}\n\nfunction _syncEngineHandleCredentialChange() {\n  _syncEngineHandleCredentialChange = _asyncToGenerator(function* (syncEngine, user) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const userChanged = !syncEngineImpl.currentUser.isEqual(user);\n\n    if (userChanged) {\n      logDebug(LOG_TAG$3, 'User change. New user:', user.toKey());\n      const result = yield localStoreHandleUserChange(syncEngineImpl.localStore, user);\n      syncEngineImpl.currentUser = user; // Fails tasks waiting for pending writes requested by previous user.\n\n      rejectOutstandingPendingWritesCallbacks(syncEngineImpl, \"'waitForPendingWrites' promise is rejected due to a user change.\"); // TODO(b/114226417): Consider calling this only in the primary tab.\n\n      syncEngineImpl.sharedClientState.handleUserChange(user, result.removedBatchIds, result.addedBatchIds);\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, result.affectedDocuments);\n    }\n  });\n  return _syncEngineHandleCredentialChange.apply(this, arguments);\n}\n\nfunction syncEngineGetRemoteKeysForTarget(syncEngine, targetId) {\n  const syncEngineImpl = debugCast(syncEngine);\n  const limboResolution = syncEngineImpl.activeLimboResolutionsByTarget.get(targetId);\n\n  if (limboResolution && limboResolution.receivedDocument) {\n    return documentKeySet().add(limboResolution.key);\n  } else {\n    let keySet = documentKeySet();\n    const queries = syncEngineImpl.queriesByTarget.get(targetId);\n\n    if (!queries) {\n      return keySet;\n    }\n\n    for (const query of queries) {\n      const queryView = syncEngineImpl.queryViewsByQuery.get(query);\n      keySet = keySet.unionWith(queryView.view.syncedDocuments);\n    }\n\n    return keySet;\n  }\n}\n/**\r\n * Reconcile the list of synced documents in an existing view with those\r\n * from persistence.\r\n */\n\n\nfunction synchronizeViewAndComputeSnapshot(_x96, _x97) {\n  return _synchronizeViewAndComputeSnapshot.apply(this, arguments);\n}\n/**\r\n * Retrieves newly changed documents from remote document cache and raises\r\n * snapshots if needed.\r\n */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction _synchronizeViewAndComputeSnapshot() {\n  _synchronizeViewAndComputeSnapshot = _asyncToGenerator(function* (syncEngine, queryView) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const queryResult = yield localStoreExecuteQuery(syncEngineImpl.localStore, queryView.query,\n    /* usePreviousResults= */\n    true);\n    const viewSnapshot = queryView.view.synchronizeWithPersistedState(queryResult);\n\n    if (syncEngineImpl.isPrimaryClient) {\n      updateTrackedLimbos(syncEngineImpl, queryView.targetId, viewSnapshot.limboChanges);\n    }\n\n    return viewSnapshot;\n  });\n  return _synchronizeViewAndComputeSnapshot.apply(this, arguments);\n}\n\nfunction syncEngineSynchronizeWithChangedDocuments(_x98, _x99) {\n  return _syncEngineSynchronizeWithChangedDocuments.apply(this, arguments);\n}\n/** Applies a mutation state to an existing batch.  */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction _syncEngineSynchronizeWithChangedDocuments() {\n  _syncEngineSynchronizeWithChangedDocuments = _asyncToGenerator(function* (syncEngine, collectionGroup) {\n    const syncEngineImpl = debugCast(syncEngine);\n    return localStoreGetNewDocumentChanges(syncEngineImpl.localStore, collectionGroup).then(changes => syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, changes));\n  });\n  return _syncEngineSynchronizeWithChangedDocuments.apply(this, arguments);\n}\n\nfunction syncEngineApplyBatchState(_x100, _x101, _x102, _x103) {\n  return _syncEngineApplyBatchState.apply(this, arguments);\n}\n/** Applies a query target change from a different tab. */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction _syncEngineApplyBatchState() {\n  _syncEngineApplyBatchState = _asyncToGenerator(function* (syncEngine, batchId, batchState, error) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const documents = yield localStoreLookupMutationDocuments(syncEngineImpl.localStore, batchId);\n\n    if (documents === null) {\n      // A throttled tab may not have seen the mutation before it was completed\n      // and removed from the mutation queue, in which case we won't have cached\n      // the affected documents. In this case we can safely ignore the update\n      // since that means we didn't apply the mutation locally at all (if we\n      // had, we would have cached the affected documents), and so we will just\n      // see any resulting document changes via normal remote document updates\n      // as applicable.\n      logDebug(LOG_TAG$3, 'Cannot apply mutation batch with id: ' + batchId);\n      return;\n    }\n\n    if (batchState === 'pending') {\n      // If we are the primary client, we need to send this write to the\n      // backend. Secondary clients will ignore these writes since their remote\n      // connection is disabled.\n      yield fillWritePipeline(syncEngineImpl.remoteStore);\n    } else if (batchState === 'acknowledged' || batchState === 'rejected') {\n      // NOTE: Both these methods are no-ops for batches that originated from\n      // other clients.\n      processUserCallback(syncEngineImpl, batchId, error ? error : null);\n      triggerPendingWritesCallbacks(syncEngineImpl, batchId);\n      localStoreRemoveCachedMutationBatchMetadata(syncEngineImpl.localStore, batchId);\n    } else {\n      fail();\n    }\n\n    yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, documents);\n  });\n  return _syncEngineApplyBatchState.apply(this, arguments);\n}\n\nfunction syncEngineApplyPrimaryState(_x104, _x105) {\n  return _syncEngineApplyPrimaryState.apply(this, arguments);\n} // PORTING NOTE: Multi-Tab only.\n\n\nfunction _syncEngineApplyPrimaryState() {\n  _syncEngineApplyPrimaryState = _asyncToGenerator(function* (syncEngine, isPrimary) {\n    const syncEngineImpl = debugCast(syncEngine);\n    ensureWatchCallbacks(syncEngineImpl);\n    syncEngineEnsureWriteCallbacks(syncEngineImpl);\n\n    if (isPrimary === true && syncEngineImpl._isPrimaryClient !== true) {\n      // Secondary tabs only maintain Views for their local listeners and the\n      // Views internal state may not be 100% populated (in particular\n      // secondary tabs don't track syncedDocuments, the set of documents the\n      // server considers to be in the target). So when a secondary becomes\n      // primary, we need to need to make sure that all views for all targets\n      // match the state on disk.\n      const activeTargets = syncEngineImpl.sharedClientState.getAllActiveQueryTargets();\n      const activeQueries = yield synchronizeQueryViewsAndRaiseSnapshots(syncEngineImpl, activeTargets.toArray());\n      syncEngineImpl._isPrimaryClient = true;\n      yield remoteStoreApplyPrimaryState(syncEngineImpl.remoteStore, true);\n\n      for (const targetData of activeQueries) {\n        remoteStoreListen(syncEngineImpl.remoteStore, targetData);\n      }\n    } else if (isPrimary === false && syncEngineImpl._isPrimaryClient !== false) {\n      const activeTargets = [];\n      let p = Promise.resolve();\n      syncEngineImpl.queriesByTarget.forEach((_, targetId) => {\n        if (syncEngineImpl.sharedClientState.isLocalQueryTarget(targetId)) {\n          activeTargets.push(targetId);\n        } else {\n          p = p.then(() => {\n            removeAndCleanupTarget(syncEngineImpl, targetId);\n            return localStoreReleaseTarget(syncEngineImpl.localStore, targetId,\n            /*keepPersistedTargetData=*/\n            true);\n          });\n        }\n\n        remoteStoreUnlisten(syncEngineImpl.remoteStore, targetId);\n      });\n      yield p;\n      yield synchronizeQueryViewsAndRaiseSnapshots(syncEngineImpl, activeTargets);\n      resetLimboDocuments(syncEngineImpl);\n      syncEngineImpl._isPrimaryClient = false;\n      yield remoteStoreApplyPrimaryState(syncEngineImpl.remoteStore, false);\n    }\n  });\n  return _syncEngineApplyPrimaryState.apply(this, arguments);\n}\n\nfunction resetLimboDocuments(syncEngine) {\n  const syncEngineImpl = debugCast(syncEngine);\n  syncEngineImpl.activeLimboResolutionsByTarget.forEach((_, targetId) => {\n    remoteStoreUnlisten(syncEngineImpl.remoteStore, targetId);\n  });\n  syncEngineImpl.limboDocumentRefs.removeAllReferences();\n  syncEngineImpl.activeLimboResolutionsByTarget = new Map();\n  syncEngineImpl.activeLimboTargetsByKey = new SortedMap(DocumentKey.comparator);\n}\n/**\r\n * Reconcile the query views of the provided query targets with the state from\r\n * persistence. Raises snapshots for any changes that affect the local\r\n * client and returns the updated state of all target's query data.\r\n *\r\n * @param syncEngine - The sync engine implementation\r\n * @param targets - the list of targets with views that need to be recomputed\r\n * @param transitionToPrimary - `true` iff the tab transitions from a secondary\r\n * tab to a primary tab\r\n */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction synchronizeQueryViewsAndRaiseSnapshots(_x106, _x107, _x108) {\n  return _synchronizeQueryViewsAndRaiseSnapshots.apply(this, arguments);\n}\n/**\r\n * Creates a `Query` object from the specified `Target`. There is no way to\r\n * obtain the original `Query`, so we synthesize a `Query` from the `Target`\r\n * object.\r\n *\r\n * The synthesized result might be different from the original `Query`, but\r\n * since the synthesized `Query` should return the same results as the\r\n * original one (only the presentation of results might differ), the potential\r\n * difference will not cause issues.\r\n */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction _synchronizeQueryViewsAndRaiseSnapshots() {\n  _synchronizeQueryViewsAndRaiseSnapshots = _asyncToGenerator(function* (syncEngine, targets, transitionToPrimary) {\n    const syncEngineImpl = debugCast(syncEngine);\n    const activeQueries = [];\n    const newViewSnapshots = [];\n\n    for (const targetId of targets) {\n      let targetData;\n      const queries = syncEngineImpl.queriesByTarget.get(targetId);\n\n      if (queries && queries.length !== 0) {\n        // For queries that have a local View, we fetch their current state\n        // from LocalStore (as the resume token and the snapshot version\n        // might have changed) and reconcile their views with the persisted\n        // state (the list of syncedDocuments may have gotten out of sync).\n        targetData = yield localStoreAllocateTarget(syncEngineImpl.localStore, queryToTarget(queries[0]));\n\n        for (const query of queries) {\n          const queryView = syncEngineImpl.queryViewsByQuery.get(query);\n          const viewChange = yield synchronizeViewAndComputeSnapshot(syncEngineImpl, queryView);\n\n          if (viewChange.snapshot) {\n            newViewSnapshots.push(viewChange.snapshot);\n          }\n        }\n      } else {\n        // For queries that never executed on this client, we need to\n        // allocate the target in LocalStore and initialize a new View.\n        const target = yield localStoreGetCachedTarget(syncEngineImpl.localStore, targetId);\n        targetData = yield localStoreAllocateTarget(syncEngineImpl.localStore, target);\n        yield initializeViewAndComputeSnapshot(syncEngineImpl, synthesizeTargetToQuery(target), targetId,\n        /*current=*/\n        false, targetData.resumeToken);\n      }\n\n      activeQueries.push(targetData);\n    }\n\n    syncEngineImpl.syncEngineListener.onWatchChange(newViewSnapshots);\n    return activeQueries;\n  });\n  return _synchronizeQueryViewsAndRaiseSnapshots.apply(this, arguments);\n}\n\nfunction synthesizeTargetToQuery(target) {\n  return newQuery(target.path, target.collectionGroup, target.orderBy, target.filters, target.limit, \"F\"\n  /* LimitType.First */\n  , target.startAt, target.endAt);\n}\n/** Returns the IDs of the clients that are currently active. */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction syncEngineGetActiveClients(syncEngine) {\n  const syncEngineImpl = debugCast(syncEngine);\n  return localStoreGetActiveClients(syncEngineImpl.localStore);\n}\n/** Applies a query target change from a different tab. */\n// PORTING NOTE: Multi-Tab only.\n\n\nfunction syncEngineApplyTargetState(_x109, _x110, _x111, _x112) {\n  return _syncEngineApplyTargetState.apply(this, arguments);\n}\n/** Adds or removes Watch targets for queries from different tabs. */\n\n\nfunction _syncEngineApplyTargetState() {\n  _syncEngineApplyTargetState = _asyncToGenerator(function* (syncEngine, targetId, state, error) {\n    const syncEngineImpl = debugCast(syncEngine);\n\n    if (syncEngineImpl._isPrimaryClient) {\n      // If we receive a target state notification via WebStorage, we are\n      // either already secondary or another tab has taken the primary lease.\n      logDebug(LOG_TAG$3, 'Ignoring unexpected query state notification.');\n      return;\n    }\n\n    const query = syncEngineImpl.queriesByTarget.get(targetId);\n\n    if (query && query.length > 0) {\n      switch (state) {\n        case 'current':\n        case 'not-current':\n          {\n            const changes = yield localStoreGetNewDocumentChanges(syncEngineImpl.localStore, queryCollectionGroup(query[0]));\n            const synthesizedRemoteEvent = RemoteEvent.createSynthesizedRemoteEventForCurrentChange(targetId, state === 'current', ByteString.EMPTY_BYTE_STRING);\n            yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngineImpl, changes, synthesizedRemoteEvent);\n            break;\n          }\n\n        case 'rejected':\n          {\n            yield localStoreReleaseTarget(syncEngineImpl.localStore, targetId,\n            /* keepPersistedTargetData */\n            true);\n            removeAndCleanupTarget(syncEngineImpl, targetId, error);\n            break;\n          }\n\n        default:\n          fail();\n      }\n    }\n  });\n  return _syncEngineApplyTargetState.apply(this, arguments);\n}\n\nfunction syncEngineApplyActiveTargetsChange(_x113, _x114, _x115) {\n  return _syncEngineApplyActiveTargetsChange.apply(this, arguments);\n}\n\nfunction _syncEngineApplyActiveTargetsChange() {\n  _syncEngineApplyActiveTargetsChange = _asyncToGenerator(function* (syncEngine, added, removed) {\n    const syncEngineImpl = ensureWatchCallbacks(syncEngine);\n\n    if (!syncEngineImpl._isPrimaryClient) {\n      return;\n    }\n\n    for (const targetId of added) {\n      if (syncEngineImpl.queriesByTarget.has(targetId)) {\n        // A target might have been added in a previous attempt\n        logDebug(LOG_TAG$3, 'Adding an already active target ' + targetId);\n        continue;\n      }\n\n      const target = yield localStoreGetCachedTarget(syncEngineImpl.localStore, targetId);\n      const targetData = yield localStoreAllocateTarget(syncEngineImpl.localStore, target);\n      yield initializeViewAndComputeSnapshot(syncEngineImpl, synthesizeTargetToQuery(target), targetData.targetId,\n      /*current=*/\n      false, targetData.resumeToken);\n      remoteStoreListen(syncEngineImpl.remoteStore, targetData);\n    }\n\n    for (const targetId of removed) {\n      // Check that the target is still active since the target might have been\n      // removed if it has been rejected by the backend.\n      if (!syncEngineImpl.queriesByTarget.has(targetId)) {\n        continue;\n      } // Release queries that are still active.\n\n\n      yield localStoreReleaseTarget(syncEngineImpl.localStore, targetId,\n      /* keepPersistedTargetData */\n      false).then(() => {\n        remoteStoreUnlisten(syncEngineImpl.remoteStore, targetId);\n        removeAndCleanupTarget(syncEngineImpl, targetId);\n      }).catch(ignoreIfPrimaryLeaseLoss);\n    }\n  });\n  return _syncEngineApplyActiveTargetsChange.apply(this, arguments);\n}\n\nfunction ensureWatchCallbacks(syncEngine) {\n  const syncEngineImpl = debugCast(syncEngine);\n  syncEngineImpl.remoteStore.remoteSyncer.applyRemoteEvent = syncEngineApplyRemoteEvent.bind(null, syncEngineImpl);\n  syncEngineImpl.remoteStore.remoteSyncer.getRemoteKeysForTarget = syncEngineGetRemoteKeysForTarget.bind(null, syncEngineImpl);\n  syncEngineImpl.remoteStore.remoteSyncer.rejectListen = syncEngineRejectListen.bind(null, syncEngineImpl);\n  syncEngineImpl.syncEngineListener.onWatchChange = eventManagerOnWatchChange.bind(null, syncEngineImpl.eventManager);\n  syncEngineImpl.syncEngineListener.onWatchError = eventManagerOnWatchError.bind(null, syncEngineImpl.eventManager);\n  return syncEngineImpl;\n}\n\nfunction syncEngineEnsureWriteCallbacks(syncEngine) {\n  const syncEngineImpl = debugCast(syncEngine);\n  syncEngineImpl.remoteStore.remoteSyncer.applySuccessfulWrite = syncEngineApplySuccessfulWrite.bind(null, syncEngineImpl);\n  syncEngineImpl.remoteStore.remoteSyncer.rejectFailedWrite = syncEngineRejectFailedWrite.bind(null, syncEngineImpl);\n  return syncEngineImpl;\n}\n/**\r\n * Loads a Firestore bundle into the SDK. The returned promise resolves when\r\n * the bundle finished loading.\r\n *\r\n * @param syncEngine - SyncEngine to use.\r\n * @param bundleReader - Bundle to load into the SDK.\r\n * @param task - LoadBundleTask used to update the loading progress to public API.\r\n */\n\n\nfunction syncEngineLoadBundle(syncEngine, bundleReader, task) {\n  const syncEngineImpl = debugCast(syncEngine); // eslint-disable-next-line @typescript-eslint/no-floating-promises\n\n  loadBundleImpl(syncEngineImpl, bundleReader, task).then(collectionGroups => {\n    syncEngineImpl.sharedClientState.notifyBundleLoaded(collectionGroups);\n  });\n}\n/** Loads a bundle and returns the list of affected collection groups. */\n\n\nfunction loadBundleImpl(_x116, _x117, _x118) {\n  return _loadBundleImpl.apply(this, arguments);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Provides all components needed for Firestore with in-memory persistence.\r\n * Uses EagerGC garbage collection.\r\n */\n\n\nfunction _loadBundleImpl() {\n  _loadBundleImpl = _asyncToGenerator(function* (syncEngine, reader, task) {\n    try {\n      const metadata = yield reader.getMetadata();\n      const skip = yield localStoreHasNewerBundle(syncEngine.localStore, metadata);\n\n      if (skip) {\n        yield reader.close();\n\n        task._completeWith(bundleSuccessProgress(metadata));\n\n        return Promise.resolve(new Set());\n      }\n\n      task._updateProgress(bundleInitialProgress(metadata));\n\n      const loader = new BundleLoader(metadata, syncEngine.localStore, reader.serializer);\n      let element = yield reader.nextElement();\n\n      while (element) {\n        ;\n        const progress = yield loader.addSizedElement(element);\n\n        if (progress) {\n          task._updateProgress(progress);\n        }\n\n        element = yield reader.nextElement();\n      }\n\n      const result = yield loader.complete();\n      yield syncEngineEmitNewSnapsAndNotifyLocalStore(syncEngine, result.changedDocs,\n      /* remoteEvent */\n      undefined); // Save metadata, so loading the same bundle will skip.\n\n      yield localStoreSaveBundle(syncEngine.localStore, metadata);\n\n      task._completeWith(result.progress);\n\n      return Promise.resolve(result.changedCollectionGroups);\n    } catch (e) {\n      logWarn(LOG_TAG$3, `Loading bundle failed with ${e}`);\n\n      task._failWith(e);\n\n      return Promise.resolve(new Set());\n    }\n  });\n  return _loadBundleImpl.apply(this, arguments);\n}\n\nclass MemoryOfflineComponentProvider {\n  constructor() {\n    this.synchronizeTabs = false;\n  }\n\n  initialize(cfg) {\n    var _this20 = this;\n\n    return _asyncToGenerator(function* () {\n      _this20.serializer = newSerializer(cfg.databaseInfo.databaseId);\n      _this20.sharedClientState = _this20.createSharedClientState(cfg);\n      _this20.persistence = _this20.createPersistence(cfg);\n      yield _this20.persistence.start();\n      _this20.localStore = _this20.createLocalStore(cfg);\n      _this20.gcScheduler = _this20.createGarbageCollectionScheduler(cfg, _this20.localStore);\n      _this20.indexBackfillerScheduler = _this20.createIndexBackfillerScheduler(cfg, _this20.localStore);\n    })();\n  }\n\n  createGarbageCollectionScheduler(cfg, localStore) {\n    return null;\n  }\n\n  createIndexBackfillerScheduler(cfg, localStore) {\n    return null;\n  }\n\n  createLocalStore(cfg) {\n    return newLocalStore(this.persistence, new QueryEngine(), cfg.initialUser, this.serializer);\n  }\n\n  createPersistence(cfg) {\n    return new MemoryPersistence(MemoryEagerDelegate.factory, this.serializer);\n  }\n\n  createSharedClientState(cfg) {\n    return new MemorySharedClientState();\n  }\n\n  terminate() {\n    var _this21 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this21.gcScheduler) {\n        _this21.gcScheduler.stop();\n      }\n\n      yield _this21.sharedClientState.shutdown();\n      yield _this21.persistence.shutdown();\n    })();\n  }\n\n}\n\nclass LruGcMemoryOfflineComponentProvider extends MemoryOfflineComponentProvider {\n  constructor(cacheSizeBytes) {\n    super();\n    this.cacheSizeBytes = cacheSizeBytes;\n  }\n\n  createGarbageCollectionScheduler(cfg, localStore) {\n    hardAssert(this.persistence.referenceDelegate instanceof MemoryLruDelegate);\n    const garbageCollector = this.persistence.referenceDelegate.garbageCollector;\n    return new LruScheduler(garbageCollector, cfg.asyncQueue, localStore);\n  }\n\n  createPersistence(cfg) {\n    const lruParams = this.cacheSizeBytes !== undefined ? LruParams.withCacheSize(this.cacheSizeBytes) : LruParams.DEFAULT;\n    return new MemoryPersistence(p => MemoryLruDelegate.factory(p, lruParams), this.serializer);\n  }\n\n}\n/**\r\n * Provides all components needed for Firestore with IndexedDB persistence.\r\n */\n\n\nclass IndexedDbOfflineComponentProvider extends MemoryOfflineComponentProvider {\n  constructor(onlineComponentProvider, cacheSizeBytes, forceOwnership) {\n    super();\n    this.onlineComponentProvider = onlineComponentProvider;\n    this.cacheSizeBytes = cacheSizeBytes;\n    this.forceOwnership = forceOwnership;\n    this.synchronizeTabs = false;\n  }\n\n  initialize(cfg) {\n    var _superprop_getInitialize = () => super.initialize,\n        _this22 = this;\n\n    return _asyncToGenerator(function* () {\n      yield _superprop_getInitialize().call(_this22, cfg);\n      yield _this22.onlineComponentProvider.initialize(_this22, cfg); // Enqueue writes from a previous session\n\n      yield syncEngineEnsureWriteCallbacks(_this22.onlineComponentProvider.syncEngine);\n      yield fillWritePipeline(_this22.onlineComponentProvider.remoteStore); // NOTE: This will immediately call the listener, so we make sure to\n      // set it after localStore / remoteStore are started.\n\n      yield _this22.persistence.setPrimaryStateListener(() => {\n        if (_this22.gcScheduler && !_this22.gcScheduler.started) {\n          _this22.gcScheduler.start();\n        }\n\n        if (_this22.indexBackfillerScheduler && !_this22.indexBackfillerScheduler.started) {\n          _this22.indexBackfillerScheduler.start();\n        }\n\n        return Promise.resolve();\n      });\n    })();\n  }\n\n  createLocalStore(cfg) {\n    return newLocalStore(this.persistence, new QueryEngine(), cfg.initialUser, this.serializer);\n  }\n\n  createGarbageCollectionScheduler(cfg, localStore) {\n    const garbageCollector = this.persistence.referenceDelegate.garbageCollector;\n    return new LruScheduler(garbageCollector, cfg.asyncQueue, localStore);\n  }\n\n  createIndexBackfillerScheduler(cfg, localStore) {\n    const indexBackfiller = new IndexBackfiller(localStore, this.persistence);\n    return new IndexBackfillerScheduler(cfg.asyncQueue, indexBackfiller);\n  }\n\n  createPersistence(cfg) {\n    const persistenceKey = indexedDbStoragePrefix(cfg.databaseInfo.databaseId, cfg.databaseInfo.persistenceKey);\n    const lruParams = this.cacheSizeBytes !== undefined ? LruParams.withCacheSize(this.cacheSizeBytes) : LruParams.DEFAULT;\n    return new IndexedDbPersistence(this.synchronizeTabs, persistenceKey, cfg.clientId, lruParams, cfg.asyncQueue, getWindow(), getDocument(), this.serializer, this.sharedClientState, !!this.forceOwnership);\n  }\n\n  createSharedClientState(cfg) {\n    return new MemorySharedClientState();\n  }\n\n}\n/**\r\n * Provides all components needed for Firestore with multi-tab IndexedDB\r\n * persistence.\r\n *\r\n * In the legacy client, this provider is used to provide both multi-tab and\r\n * non-multi-tab persistence since we cannot tell at build time whether\r\n * `synchronizeTabs` will be enabled.\r\n */\n\n\nclass MultiTabOfflineComponentProvider extends IndexedDbOfflineComponentProvider {\n  constructor(onlineComponentProvider, cacheSizeBytes) {\n    super(onlineComponentProvider, cacheSizeBytes,\n    /* forceOwnership= */\n    false);\n    this.onlineComponentProvider = onlineComponentProvider;\n    this.cacheSizeBytes = cacheSizeBytes;\n    this.synchronizeTabs = true;\n  }\n\n  initialize(cfg) {\n    var _superprop_getInitialize2 = () => super.initialize,\n        _this23 = this;\n\n    return _asyncToGenerator(function* () {\n      yield _superprop_getInitialize2().call(_this23, cfg);\n      const syncEngine = _this23.onlineComponentProvider.syncEngine;\n\n      if (_this23.sharedClientState instanceof WebStorageSharedClientState) {\n        _this23.sharedClientState.syncEngine = {\n          applyBatchState: syncEngineApplyBatchState.bind(null, syncEngine),\n          applyTargetState: syncEngineApplyTargetState.bind(null, syncEngine),\n          applyActiveTargetsChange: syncEngineApplyActiveTargetsChange.bind(null, syncEngine),\n          getActiveClients: syncEngineGetActiveClients.bind(null, syncEngine),\n          synchronizeWithChangedDocuments: syncEngineSynchronizeWithChangedDocuments.bind(null, syncEngine)\n        };\n        yield _this23.sharedClientState.start();\n      } // NOTE: This will immediately call the listener, so we make sure to\n      // set it after localStore / remoteStore are started.\n\n\n      yield _this23.persistence.setPrimaryStateListener( /*#__PURE__*/function () {\n        var _ref12 = _asyncToGenerator(function* (isPrimary) {\n          yield syncEngineApplyPrimaryState(_this23.onlineComponentProvider.syncEngine, isPrimary);\n\n          if (_this23.gcScheduler) {\n            if (isPrimary && !_this23.gcScheduler.started) {\n              _this23.gcScheduler.start();\n            } else if (!isPrimary) {\n              _this23.gcScheduler.stop();\n            }\n          }\n\n          if (_this23.indexBackfillerScheduler) {\n            if (isPrimary && !_this23.indexBackfillerScheduler.started) {\n              _this23.indexBackfillerScheduler.start();\n            } else if (!isPrimary) {\n              _this23.indexBackfillerScheduler.stop();\n            }\n          }\n        });\n\n        return function (_x119) {\n          return _ref12.apply(this, arguments);\n        };\n      }());\n    })();\n  }\n\n  createSharedClientState(cfg) {\n    const window = getWindow();\n\n    if (!WebStorageSharedClientState.isAvailable(window)) {\n      throw new FirestoreError(Code.UNIMPLEMENTED, 'IndexedDB persistence is only available on platforms that support LocalStorage.');\n    }\n\n    const persistenceKey = indexedDbStoragePrefix(cfg.databaseInfo.databaseId, cfg.databaseInfo.persistenceKey);\n    return new WebStorageSharedClientState(window, cfg.asyncQueue, persistenceKey, cfg.clientId, cfg.initialUser);\n  }\n\n}\n/**\r\n * Initializes and wires the components that are needed to interface with the\r\n * network.\r\n */\n\n\nclass OnlineComponentProvider {\n  initialize(offlineComponentProvider, cfg) {\n    var _this24 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this24.localStore) {\n        // OnlineComponentProvider may get initialized multiple times if\n        // multi-tab persistence is used.\n        return;\n      }\n\n      _this24.localStore = offlineComponentProvider.localStore;\n      _this24.sharedClientState = offlineComponentProvider.sharedClientState;\n      _this24.datastore = _this24.createDatastore(cfg);\n      _this24.remoteStore = _this24.createRemoteStore(cfg);\n      _this24.eventManager = _this24.createEventManager(cfg);\n      _this24.syncEngine = _this24.createSyncEngine(cfg,\n      /* startAsPrimary=*/\n      !offlineComponentProvider.synchronizeTabs);\n\n      _this24.sharedClientState.onlineStateHandler = onlineState => syncEngineApplyOnlineStateChange(_this24.syncEngine, onlineState, 1\n      /* OnlineStateSource.SharedClientState */\n      );\n\n      _this24.remoteStore.remoteSyncer.handleCredentialChange = syncEngineHandleCredentialChange.bind(null, _this24.syncEngine);\n      yield remoteStoreApplyPrimaryState(_this24.remoteStore, _this24.syncEngine.isPrimaryClient);\n    })();\n  }\n\n  createEventManager(cfg) {\n    return newEventManager();\n  }\n\n  createDatastore(cfg) {\n    const serializer = newSerializer(cfg.databaseInfo.databaseId);\n    const connection = newConnection(cfg.databaseInfo);\n    return newDatastore(cfg.authCredentials, cfg.appCheckCredentials, connection, serializer);\n  }\n\n  createRemoteStore(cfg) {\n    return newRemoteStore(this.localStore, this.datastore, cfg.asyncQueue, onlineState => syncEngineApplyOnlineStateChange(this.syncEngine, onlineState, 0\n    /* OnlineStateSource.RemoteStore */\n    ), newConnectivityMonitor());\n  }\n\n  createSyncEngine(cfg, startAsPrimary) {\n    return newSyncEngine(this.localStore, this.remoteStore, this.eventManager, this.sharedClientState, cfg.initialUser, cfg.maxConcurrentLimboResolutions, startAsPrimary);\n  }\n\n  terminate() {\n    return remoteStoreShutdown(this.remoteStore);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * How many bytes to read each time when `ReadableStreamReader.read()` is\r\n * called. Only applicable for byte streams that we control (e.g. those backed\r\n * by an UInt8Array).\r\n */\n\n\nconst DEFAULT_BYTES_PER_READ = 10240;\n/**\r\n * Builds a `ByteStreamReader` from a UInt8Array.\r\n * @param source - The data source to use.\r\n * @param bytesPerRead - How many bytes each `read()` from the returned reader\r\n *        will read.\r\n */\n\nfunction toByteStreamReaderHelper(source, bytesPerRead = DEFAULT_BYTES_PER_READ) {\n  let readFrom = 0; // The TypeScript definition for ReadableStreamReader changed. We use\n  // `any` here to allow this code to compile with different versions.\n  // See https://github.com/microsoft/TypeScript/issues/42970\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n  const reader = {\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    read() {\n      return _asyncToGenerator(function* () {\n        if (readFrom < source.byteLength) {\n          const result = {\n            value: source.slice(readFrom, readFrom + bytesPerRead),\n            done: false\n          };\n          readFrom += bytesPerRead;\n          return result;\n        }\n\n        return {\n          done: true\n        };\n      })();\n    },\n\n    cancel() {\n      return _asyncToGenerator(function* () {})();\n    },\n\n    releaseLock() {},\n\n    closed: Promise.resolve()\n  };\n  return reader;\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction validateNonEmptyArgument(functionName, argumentName, argument) {\n  if (!argument) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Function ${functionName}() cannot be called with an empty ${argumentName}.`);\n  }\n}\n/**\r\n * Validates that two boolean options are not set at the same time.\r\n * @internal\r\n */\n\n\nfunction validateIsNotUsedTogether(optionName1, argument1, optionName2, argument2) {\n  if (argument1 === true && argument2 === true) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `${optionName1} and ${optionName2} cannot be used together.`);\n  }\n}\n/**\r\n * Validates that `path` refers to a document (indicated by the fact it contains\r\n * an even numbers of segments).\r\n */\n\n\nfunction validateDocumentPath(path) {\n  if (!DocumentKey.isDocumentKey(path)) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid document reference. Document references must have an even number of segments, but ${path} has ${path.length}.`);\n  }\n}\n/**\r\n * Validates that `path` refers to a collection (indicated by the fact it\r\n * contains an odd numbers of segments).\r\n */\n\n\nfunction validateCollectionPath(path) {\n  if (DocumentKey.isDocumentKey(path)) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid collection reference. Collection references must have an odd number of segments, but ${path} has ${path.length}.`);\n  }\n}\n/**\r\n * Returns true if it's a non-null object without a custom prototype\r\n * (i.e. excludes Array, Date, etc.).\r\n */\n\n\nfunction isPlainObject(input) {\n  return typeof input === 'object' && input !== null && (Object.getPrototypeOf(input) === Object.prototype || Object.getPrototypeOf(input) === null);\n}\n/** Returns a string describing the type / value of the provided input. */\n\n\nfunction valueDescription(input) {\n  if (input === undefined) {\n    return 'undefined';\n  } else if (input === null) {\n    return 'null';\n  } else if (typeof input === 'string') {\n    if (input.length > 20) {\n      input = `${input.substring(0, 20)}...`;\n    }\n\n    return JSON.stringify(input);\n  } else if (typeof input === 'number' || typeof input === 'boolean') {\n    return '' + input;\n  } else if (typeof input === 'object') {\n    if (input instanceof Array) {\n      return 'an array';\n    } else {\n      const customObjectName = tryGetCustomObjectType(input);\n\n      if (customObjectName) {\n        return `a custom ${customObjectName} object`;\n      } else {\n        return 'an object';\n      }\n    }\n  } else if (typeof input === 'function') {\n    return 'a function';\n  } else {\n    return fail();\n  }\n}\n/** try to get the constructor name for an object. */\n\n\nfunction tryGetCustomObjectType(input) {\n  if (input.constructor) {\n    return input.constructor.name;\n  }\n\n  return null;\n}\n/**\r\n * Casts `obj` to `T`, optionally unwrapping Compat types to expose the\r\n * underlying instance. Throws if  `obj` is not an instance of `T`.\r\n *\r\n * This cast is used in the Lite and Full SDK to verify instance types for\r\n * arguments passed to the public API.\r\n * @internal\r\n */\n\n\nfunction cast(obj, // eslint-disable-next-line @typescript-eslint/no-explicit-any\nconstructor) {\n  if ('_delegate' in obj) {\n    // Unwrap Compat types\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    obj = obj._delegate;\n  }\n\n  if (!(obj instanceof constructor)) {\n    if (constructor.name === obj.constructor.name) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Type does not match the expected instance. Did you pass a ' + `reference from a different Firestore SDK?`);\n    } else {\n      const description = valueDescription(obj);\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `Expected type '${constructor.name}', but it was: ${description}`);\n    }\n  }\n\n  return obj;\n}\n\nfunction validatePositiveNumber(functionName, n) {\n  if (n <= 0) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Function ${functionName}() requires a positive number, but it was: ${n}.`);\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * On Node, only supported data source is a `Uint8Array` for now.\r\n */\n\n\nfunction toByteStreamReader(source, bytesPerRead) {\n  if (!(source instanceof Uint8Array)) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `NodePlatform.toByteStreamReader expects source to be Uint8Array, got ${valueDescription(source)}`);\n  }\n\n  return toByteStreamReaderHelper(source, bytesPerRead);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/*\r\n * A wrapper implementation of Observer<T> that will dispatch events\r\n * asynchronously. To allow immediate silencing, a mute call is added which\r\n * causes events scheduled to no longer be raised.\r\n */\n\n\nclass AsyncObserver {\n  constructor(observer) {\n    this.observer = observer;\n    /**\r\n     * When set to true, will not raise future events. Necessary to deal with\r\n     * async detachment of listener.\r\n     */\n\n    this.muted = false;\n  }\n\n  next(value) {\n    if (this.observer.next) {\n      this.scheduleEvent(this.observer.next, value);\n    }\n  }\n\n  error(error) {\n    if (this.observer.error) {\n      this.scheduleEvent(this.observer.error, error);\n    } else {\n      logError('Uncaught Error in snapshot listener:', error.toString());\n    }\n  }\n\n  mute() {\n    this.muted = true;\n  }\n\n  scheduleEvent(eventHandler, event) {\n    if (!this.muted) {\n      setTimeout(() => {\n        if (!this.muted) {\n          eventHandler(event);\n        }\n      }, 0);\n    }\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A complete element in the bundle stream, together with the byte length it\r\n * occupies in the stream.\r\n */\n\n\nclass SizedBundleElement {\n  constructor(payload, // How many bytes this element takes to store in the bundle.\n  byteLength) {\n    this.payload = payload;\n    this.byteLength = byteLength;\n  }\n\n  isBundleMetadata() {\n    return 'metadata' in this.payload;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A class representing a bundle.\r\n *\r\n * Takes a bundle stream or buffer, and presents abstractions to read bundled\r\n * elements out of the underlying content.\r\n */\n\n\nclass BundleReaderImpl {\n  constructor(\n  /** The reader to read from underlying binary bundle data source. */\n  reader, serializer) {\n    this.reader = reader;\n    this.serializer = serializer;\n    /** Cached bundle metadata. */\n\n    this.metadata = new Deferred();\n    /**\r\n     * Internal buffer to hold bundle content, accumulating incomplete element\r\n     * content.\r\n     */\n\n    this.buffer = new Uint8Array();\n    this.textDecoder = newTextDecoder(); // Read the metadata (which is the first element).\n\n    this.nextElementImpl().then(element => {\n      if (element && element.isBundleMetadata()) {\n        this.metadata.resolve(element.payload.metadata);\n      } else {\n        this.metadata.reject(new Error(`The first element of the bundle is not a metadata, it is\n             ${JSON.stringify(element === null || element === void 0 ? void 0 : element.payload)}`));\n      }\n    }, error => this.metadata.reject(error));\n  }\n\n  close() {\n    return this.reader.cancel();\n  }\n\n  getMetadata() {\n    var _this25 = this;\n\n    return _asyncToGenerator(function* () {\n      return _this25.metadata.promise;\n    })();\n  }\n\n  nextElement() {\n    var _this26 = this;\n\n    return _asyncToGenerator(function* () {\n      // Makes sure metadata is read before proceeding.\n      yield _this26.getMetadata();\n      return _this26.nextElementImpl();\n    })();\n  }\n  /**\r\n   * Reads from the head of internal buffer, and pulling more data from\r\n   * underlying stream if a complete element cannot be found, until an\r\n   * element(including the prefixed length and the JSON string) is found.\r\n   *\r\n   * Once a complete element is read, it is dropped from internal buffer.\r\n   *\r\n   * Returns either the bundled element, or null if we have reached the end of\r\n   * the stream.\r\n   */\n\n\n  nextElementImpl() {\n    var _this27 = this;\n\n    return _asyncToGenerator(function* () {\n      const lengthBuffer = yield _this27.readLength();\n\n      if (lengthBuffer === null) {\n        return null;\n      }\n\n      const lengthString = _this27.textDecoder.decode(lengthBuffer);\n\n      const length = Number(lengthString);\n\n      if (isNaN(length)) {\n        _this27.raiseError(`length string (${lengthString}) is not valid number`);\n      }\n\n      const jsonString = yield _this27.readJsonString(length);\n      return new SizedBundleElement(JSON.parse(jsonString), lengthBuffer.length + length);\n    })();\n  }\n  /** First index of '{' from the underlying buffer. */\n\n\n  indexOfOpenBracket() {\n    return this.buffer.findIndex(v => v === '{'.charCodeAt(0));\n  }\n  /**\r\n   * Reads from the beginning of the internal buffer, until the first '{', and\r\n   * return the content.\r\n   *\r\n   * If reached end of the stream, returns a null.\r\n   */\n\n\n  readLength() {\n    var _this28 = this;\n\n    return _asyncToGenerator(function* () {\n      while (_this28.indexOfOpenBracket() < 0) {\n        const done = yield _this28.pullMoreDataToBuffer();\n\n        if (done) {\n          break;\n        }\n      } // Broke out of the loop because underlying stream is closed, and there\n      // happens to be no more data to process.\n\n\n      if (_this28.buffer.length === 0) {\n        return null;\n      }\n\n      const position = _this28.indexOfOpenBracket(); // Broke out of the loop because underlying stream is closed, but still\n      // cannot find an open bracket.\n\n\n      if (position < 0) {\n        _this28.raiseError('Reached the end of bundle when a length string is expected.');\n      }\n\n      const result = _this28.buffer.slice(0, position); // Update the internal buffer to drop the read length.\n\n\n      _this28.buffer = _this28.buffer.slice(position);\n      return result;\n    })();\n  }\n  /**\r\n   * Reads from a specified position from the internal buffer, for a specified\r\n   * number of bytes, pulling more data from the underlying stream if needed.\r\n   *\r\n   * Returns a string decoded from the read bytes.\r\n   */\n\n\n  readJsonString(length) {\n    var _this29 = this;\n\n    return _asyncToGenerator(function* () {\n      while (_this29.buffer.length < length) {\n        const done = yield _this29.pullMoreDataToBuffer();\n\n        if (done) {\n          _this29.raiseError('Reached the end of bundle when more is expected.');\n        }\n      }\n\n      const result = _this29.textDecoder.decode(_this29.buffer.slice(0, length)); // Update the internal buffer to drop the read json string.\n\n\n      _this29.buffer = _this29.buffer.slice(length);\n      return result;\n    })();\n  }\n\n  raiseError(message) {\n    // eslint-disable-next-line @typescript-eslint/no-floating-promises\n    this.reader.cancel();\n    throw new Error(`Invalid bundle format: ${message}`);\n  }\n  /**\r\n   * Pulls more data from underlying stream to internal buffer.\r\n   * Returns a boolean indicating whether the stream is finished.\r\n   */\n\n\n  pullMoreDataToBuffer() {\n    var _this30 = this;\n\n    return _asyncToGenerator(function* () {\n      const result = yield _this30.reader.read();\n\n      if (!result.done) {\n        const newBuffer = new Uint8Array(_this30.buffer.length + result.value.length);\n        newBuffer.set(_this30.buffer);\n        newBuffer.set(result.value, _this30.buffer.length);\n        _this30.buffer = newBuffer;\n      }\n\n      return result.done;\n    })();\n  }\n\n}\n\nfunction newBundleReader(reader, serializer) {\n  return new BundleReaderImpl(reader, serializer);\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Internal transaction object responsible for accumulating the mutations to\r\n * perform and the base versions for any documents read.\r\n */\n\n\nclass Transaction$2 {\n  constructor(datastore) {\n    this.datastore = datastore; // The version of each document that was read during this transaction.\n\n    this.readVersions = new Map();\n    this.mutations = [];\n    this.committed = false;\n    /**\r\n     * A deferred usage error that occurred previously in this transaction that\r\n     * will cause the transaction to fail once it actually commits.\r\n     */\n\n    this.lastWriteError = null;\n    /**\r\n     * Set of documents that have been written in the transaction.\r\n     *\r\n     * When there's more than one write to the same key in a transaction, any\r\n     * writes after the first are handled differently.\r\n     */\n\n    this.writtenDocs = new Set();\n  }\n\n  lookup(keys) {\n    var _this31 = this;\n\n    return _asyncToGenerator(function* () {\n      _this31.ensureCommitNotCalled();\n\n      if (_this31.mutations.length > 0) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Firestore transactions require all reads to be executed before all writes.');\n      }\n\n      const docs = yield invokeBatchGetDocumentsRpc(_this31.datastore, keys);\n      docs.forEach(doc => _this31.recordVersion(doc));\n      return docs;\n    })();\n  }\n\n  set(key, data) {\n    this.write(data.toMutation(key, this.precondition(key)));\n    this.writtenDocs.add(key.toString());\n  }\n\n  update(key, data) {\n    try {\n      this.write(data.toMutation(key, this.preconditionForUpdate(key)));\n    } catch (e) {\n      this.lastWriteError = e;\n    }\n\n    this.writtenDocs.add(key.toString());\n  }\n\n  delete(key) {\n    this.write(new DeleteMutation(key, this.precondition(key)));\n    this.writtenDocs.add(key.toString());\n  }\n\n  commit() {\n    var _this32 = this;\n\n    return _asyncToGenerator(function* () {\n      _this32.ensureCommitNotCalled();\n\n      if (_this32.lastWriteError) {\n        throw _this32.lastWriteError;\n      }\n\n      const unwritten = _this32.readVersions; // For each mutation, note that the doc was written.\n\n      _this32.mutations.forEach(mutation => {\n        unwritten.delete(mutation.key.toString());\n      }); // For each document that was read but not written to, we want to perform\n      // a `verify` operation.\n\n\n      unwritten.forEach((_, path) => {\n        const key = DocumentKey.fromPath(path);\n\n        _this32.mutations.push(new VerifyMutation(key, _this32.precondition(key)));\n      });\n      yield invokeCommitRpc(_this32.datastore, _this32.mutations);\n      _this32.committed = true;\n    })();\n  }\n\n  recordVersion(doc) {\n    let docVersion;\n\n    if (doc.isFoundDocument()) {\n      docVersion = doc.version;\n    } else if (doc.isNoDocument()) {\n      // Represent a deleted doc using SnapshotVersion.min().\n      docVersion = SnapshotVersion.min();\n    } else {\n      throw fail();\n    }\n\n    const existingVersion = this.readVersions.get(doc.key.toString());\n\n    if (existingVersion) {\n      if (!docVersion.isEqual(existingVersion)) {\n        // This transaction will fail no matter what.\n        throw new FirestoreError(Code.ABORTED, 'Document version changed between two reads.');\n      }\n    } else {\n      this.readVersions.set(doc.key.toString(), docVersion);\n    }\n  }\n  /**\r\n   * Returns the version of this document when it was read in this transaction,\r\n   * as a precondition, or no precondition if it was not read.\r\n   */\n\n\n  precondition(key) {\n    const version = this.readVersions.get(key.toString());\n\n    if (!this.writtenDocs.has(key.toString()) && version) {\n      if (version.isEqual(SnapshotVersion.min())) {\n        return Precondition.exists(false);\n      } else {\n        return Precondition.updateTime(version);\n      }\n    } else {\n      return Precondition.none();\n    }\n  }\n  /**\r\n   * Returns the precondition for a document if the operation is an update.\r\n   */\n\n\n  preconditionForUpdate(key) {\n    const version = this.readVersions.get(key.toString()); // The first time a document is written, we want to take into account the\n    // read time and existence\n\n    if (!this.writtenDocs.has(key.toString()) && version) {\n      if (version.isEqual(SnapshotVersion.min())) {\n        // The document doesn't exist, so fail the transaction.\n        // This has to be validated locally because you can't send a\n        // precondition that a document does not exist without changing the\n        // semantics of the backend write to be an insert. This is the reverse\n        // of what we want, since we want to assert that the document doesn't\n        // exist but then send the update and have it fail. Since we can't\n        // express that to the backend, we have to validate locally.\n        // Note: this can change once we can send separate verify writes in the\n        // transaction.\n        throw new FirestoreError(Code.INVALID_ARGUMENT, \"Can't update a document that doesn't exist.\");\n      } // Document exists, base precondition on document update time.\n\n\n      return Precondition.updateTime(version);\n    } else {\n      // Document was not read, so we just use the preconditions for a blind\n      // update.\n      return Precondition.exists(true);\n    }\n  }\n\n  write(mutation) {\n    this.ensureCommitNotCalled();\n    this.mutations.push(mutation);\n  }\n\n  ensureCommitNotCalled() {}\n\n}\n/**\r\n * @license\r\n * Copyright 2019 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * TransactionRunner encapsulates the logic needed to run and retry transactions\r\n * with backoff.\r\n */\n\n\nclass TransactionRunner {\n  constructor(asyncQueue, datastore, options, updateFunction, deferred) {\n    this.asyncQueue = asyncQueue;\n    this.datastore = datastore;\n    this.options = options;\n    this.updateFunction = updateFunction;\n    this.deferred = deferred;\n    this.attemptsRemaining = options.maxAttempts;\n    this.backoff = new ExponentialBackoff(this.asyncQueue, \"transaction_retry\"\n    /* TimerId.TransactionRetry */\n    );\n  }\n  /** Runs the transaction and sets the result on deferred. */\n\n\n  run() {\n    this.attemptsRemaining -= 1;\n    this.runWithBackOff();\n  }\n\n  runWithBackOff() {\n    var _this33 = this;\n\n    this.backoff.backoffAndRun( /*#__PURE__*/_asyncToGenerator(function* () {\n      const transaction = new Transaction$2(_this33.datastore);\n\n      const userPromise = _this33.tryRunUpdateFunction(transaction);\n\n      if (userPromise) {\n        userPromise.then(result => {\n          _this33.asyncQueue.enqueueAndForget(() => {\n            return transaction.commit().then(() => {\n              _this33.deferred.resolve(result);\n            }).catch(commitError => {\n              _this33.handleTransactionError(commitError);\n            });\n          });\n        }).catch(userPromiseError => {\n          _this33.handleTransactionError(userPromiseError);\n        });\n      }\n    }));\n  }\n\n  tryRunUpdateFunction(transaction) {\n    try {\n      const userPromise = this.updateFunction(transaction);\n\n      if (isNullOrUndefined(userPromise) || !userPromise.catch || !userPromise.then) {\n        this.deferred.reject(Error('Transaction callback must return a Promise'));\n        return null;\n      }\n\n      return userPromise;\n    } catch (error) {\n      // Do not retry errors thrown by user provided updateFunction.\n      this.deferred.reject(error);\n      return null;\n    }\n  }\n\n  handleTransactionError(error) {\n    if (this.attemptsRemaining > 0 && this.isRetryableTransactionError(error)) {\n      this.attemptsRemaining -= 1;\n      this.asyncQueue.enqueueAndForget(() => {\n        this.runWithBackOff();\n        return Promise.resolve();\n      });\n    } else {\n      this.deferred.reject(error);\n    }\n  }\n\n  isRetryableTransactionError(error) {\n    if (error.name === 'FirebaseError') {\n      // In transactions, the backend will fail outdated reads with FAILED_PRECONDITION and\n      // non-matching document versions with ABORTED. These errors should be retried.\n      const code = error.code;\n      return code === 'aborted' || code === 'failed-precondition' || code === 'already-exists' || !isPermanentError(code);\n    }\n\n    return false;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$2 = 'FirestoreClient';\nconst MAX_CONCURRENT_LIMBO_RESOLUTIONS = 100;\n/** DOMException error code constants. */\n\nconst DOM_EXCEPTION_INVALID_STATE = 11;\nconst DOM_EXCEPTION_ABORTED = 20;\nconst DOM_EXCEPTION_QUOTA_EXCEEDED = 22;\n/**\r\n * FirestoreClient is a top-level class that constructs and owns all of the //\r\n * pieces of the client SDK architecture. It is responsible for creating the //\r\n * async queue that is shared by all of the other components in the system. //\r\n */\n\nclass FirestoreClient {\n  constructor(authCredentials, appCheckCredentials,\n  /**\r\n   * Asynchronous queue responsible for all of our internal processing. When\r\n   * we get incoming work from the user (via public API) or the network\r\n   * (incoming GRPC messages), we should always schedule onto this queue.\r\n   * This ensures all of our work is properly serialized (e.g. we don't\r\n   * start processing a new operation while the previous one is waiting for\r\n   * an async I/O to complete).\r\n   */\n  asyncQueue, databaseInfo) {\n    var _this34 = this;\n\n    this.authCredentials = authCredentials;\n    this.appCheckCredentials = appCheckCredentials;\n    this.asyncQueue = asyncQueue;\n    this.databaseInfo = databaseInfo;\n    this.user = User.UNAUTHENTICATED;\n    this.clientId = AutoId.newId();\n\n    this.authCredentialListener = () => Promise.resolve();\n\n    this.appCheckCredentialListener = () => Promise.resolve();\n\n    this.authCredentials.start(asyncQueue, /*#__PURE__*/function () {\n      var _ref14 = _asyncToGenerator(function* (user) {\n        logDebug(LOG_TAG$2, 'Received user=', user.uid);\n        yield _this34.authCredentialListener(user);\n        _this34.user = user;\n      });\n\n      return function (_x120) {\n        return _ref14.apply(this, arguments);\n      };\n    }());\n    this.appCheckCredentials.start(asyncQueue, newAppCheckToken => {\n      logDebug(LOG_TAG$2, 'Received new app check token=', newAppCheckToken);\n      return this.appCheckCredentialListener(newAppCheckToken, this.user);\n    });\n  }\n\n  getConfiguration() {\n    var _this35 = this;\n\n    return _asyncToGenerator(function* () {\n      return {\n        asyncQueue: _this35.asyncQueue,\n        databaseInfo: _this35.databaseInfo,\n        clientId: _this35.clientId,\n        authCredentials: _this35.authCredentials,\n        appCheckCredentials: _this35.appCheckCredentials,\n        initialUser: _this35.user,\n        maxConcurrentLimboResolutions: MAX_CONCURRENT_LIMBO_RESOLUTIONS\n      };\n    })();\n  }\n\n  setCredentialChangeListener(listener) {\n    this.authCredentialListener = listener;\n  }\n\n  setAppCheckTokenChangeListener(listener) {\n    this.appCheckCredentialListener = listener;\n  }\n  /**\r\n   * Checks that the client has not been terminated. Ensures that other methods on //\r\n   * this class cannot be called after the client is terminated. //\r\n   */\n\n\n  verifyNotTerminated() {\n    if (this.asyncQueue.isShuttingDown) {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, 'The client has already been terminated.');\n    }\n  }\n\n  terminate() {\n    var _this36 = this;\n\n    this.asyncQueue.enterRestrictedMode();\n    const deferred = new Deferred();\n    this.asyncQueue.enqueueAndForgetEvenWhileRestricted( /*#__PURE__*/_asyncToGenerator(function* () {\n      try {\n        if (_this36._onlineComponents) {\n          yield _this36._onlineComponents.terminate();\n        }\n\n        if (_this36._offlineComponents) {\n          yield _this36._offlineComponents.terminate();\n        } // The credentials provider must be terminated after shutting down the\n        // RemoteStore as it will prevent the RemoteStore from retrieving auth\n        // tokens.\n\n\n        _this36.authCredentials.shutdown();\n\n        _this36.appCheckCredentials.shutdown();\n\n        deferred.resolve();\n      } catch (e) {\n        const firestoreError = wrapInUserErrorIfRecoverable(e, `Failed to shutdown persistence`);\n        deferred.reject(firestoreError);\n      }\n    }));\n    return deferred.promise;\n  }\n\n}\n\nfunction setOfflineComponentProvider(_x121, _x122) {\n  return _setOfflineComponentProvider.apply(this, arguments);\n}\n\nfunction _setOfflineComponentProvider() {\n  _setOfflineComponentProvider = _asyncToGenerator(function* (client, offlineComponentProvider) {\n    client.asyncQueue.verifyOperationInProgress();\n    logDebug(LOG_TAG$2, 'Initializing OfflineComponentProvider');\n    const configuration = yield client.getConfiguration();\n    yield offlineComponentProvider.initialize(configuration);\n    let currentUser = configuration.initialUser;\n    client.setCredentialChangeListener( /*#__PURE__*/function () {\n      var _ref36 = _asyncToGenerator(function* (user) {\n        if (!currentUser.isEqual(user)) {\n          yield localStoreHandleUserChange(offlineComponentProvider.localStore, user);\n          currentUser = user;\n        }\n      });\n\n      return function (_x134) {\n        return _ref36.apply(this, arguments);\n      };\n    }()); // When a user calls clearPersistence() in one client, all other clients\n    // need to be terminated to allow the delete to succeed.\n\n    offlineComponentProvider.persistence.setDatabaseDeletedListener(() => client.terminate());\n    client._offlineComponents = offlineComponentProvider;\n  });\n  return _setOfflineComponentProvider.apply(this, arguments);\n}\n\nfunction setOnlineComponentProvider(_x123, _x124) {\n  return _setOnlineComponentProvider.apply(this, arguments);\n}\n/**\r\n * Decides whether the provided error allows us to gracefully disable\r\n * persistence (as opposed to crashing the client).\r\n */\n\n\nfunction _setOnlineComponentProvider() {\n  _setOnlineComponentProvider = _asyncToGenerator(function* (client, onlineComponentProvider) {\n    client.asyncQueue.verifyOperationInProgress();\n    const offlineComponentProvider = yield ensureOfflineComponents(client);\n    logDebug(LOG_TAG$2, 'Initializing OnlineComponentProvider');\n    const configuration = yield client.getConfiguration();\n    yield onlineComponentProvider.initialize(offlineComponentProvider, configuration); // The CredentialChangeListener of the online component provider takes\n    // precedence over the offline component provider.\n\n    client.setCredentialChangeListener(user => remoteStoreHandleCredentialChange(onlineComponentProvider.remoteStore, user));\n    client.setAppCheckTokenChangeListener((_, user) => remoteStoreHandleCredentialChange(onlineComponentProvider.remoteStore, user));\n    client._onlineComponents = onlineComponentProvider;\n  });\n  return _setOnlineComponentProvider.apply(this, arguments);\n}\n\nfunction canFallbackFromIndexedDbError(error) {\n  if (error.name === 'FirebaseError') {\n    return error.code === Code.FAILED_PRECONDITION || error.code === Code.UNIMPLEMENTED;\n  } else if (typeof DOMException !== 'undefined' && error instanceof DOMException) {\n    // There are a few known circumstances where we can open IndexedDb but\n    // trying to read/write will fail (e.g. quota exceeded). For\n    // well-understood cases, we attempt to detect these and then gracefully\n    // fall back to memory persistence.\n    // NOTE: Rather than continue to add to this list, we could decide to\n    // always fall back, with the risk that we might accidentally hide errors\n    // representing actual SDK bugs.\n    return (// When the browser is out of quota we could get either quota exceeded\n      // or an aborted error depending on whether the error happened during\n      // schema migration.\n      error.code === DOM_EXCEPTION_QUOTA_EXCEEDED || error.code === DOM_EXCEPTION_ABORTED || // Firefox Private Browsing mode disables IndexedDb and returns\n      // INVALID_STATE for any usage.\n      error.code === DOM_EXCEPTION_INVALID_STATE\n    );\n  }\n\n  return true;\n}\n\nfunction ensureOfflineComponents(_x125) {\n  return _ensureOfflineComponents.apply(this, arguments);\n}\n\nfunction _ensureOfflineComponents() {\n  _ensureOfflineComponents = _asyncToGenerator(function* (client) {\n    if (!client._offlineComponents) {\n      if (client._uninitializedComponentsProvider) {\n        logDebug(LOG_TAG$2, 'Using user provided OfflineComponentProvider');\n\n        try {\n          yield setOfflineComponentProvider(client, client._uninitializedComponentsProvider._offline);\n        } catch (e) {\n          const error = e;\n\n          if (!canFallbackFromIndexedDbError(error)) {\n            throw error;\n          }\n\n          logWarn('Error using user provided cache. Falling back to ' + 'memory cache: ' + error);\n          yield setOfflineComponentProvider(client, new MemoryOfflineComponentProvider());\n        }\n      } else {\n        logDebug(LOG_TAG$2, 'Using default OfflineComponentProvider');\n        yield setOfflineComponentProvider(client, new MemoryOfflineComponentProvider());\n      }\n    }\n\n    return client._offlineComponents;\n  });\n  return _ensureOfflineComponents.apply(this, arguments);\n}\n\nfunction ensureOnlineComponents(_x126) {\n  return _ensureOnlineComponents.apply(this, arguments);\n}\n\nfunction _ensureOnlineComponents() {\n  _ensureOnlineComponents = _asyncToGenerator(function* (client) {\n    if (!client._onlineComponents) {\n      if (client._uninitializedComponentsProvider) {\n        logDebug(LOG_TAG$2, 'Using user provided OnlineComponentProvider');\n        yield setOnlineComponentProvider(client, client._uninitializedComponentsProvider._online);\n      } else {\n        logDebug(LOG_TAG$2, 'Using default OnlineComponentProvider');\n        yield setOnlineComponentProvider(client, new OnlineComponentProvider());\n      }\n    }\n\n    return client._onlineComponents;\n  });\n  return _ensureOnlineComponents.apply(this, arguments);\n}\n\nfunction getPersistence(client) {\n  return ensureOfflineComponents(client).then(c => c.persistence);\n}\n\nfunction getLocalStore(client) {\n  return ensureOfflineComponents(client).then(c => c.localStore);\n}\n\nfunction getRemoteStore(client) {\n  return ensureOnlineComponents(client).then(c => c.remoteStore);\n}\n\nfunction getSyncEngine(client) {\n  return ensureOnlineComponents(client).then(c => c.syncEngine);\n}\n\nfunction getDatastore(client) {\n  return ensureOnlineComponents(client).then(c => c.datastore);\n}\n\nfunction getEventManager(_x127) {\n  return _getEventManager.apply(this, arguments);\n}\n/** Enables the network connection and re-enqueues all pending operations. */\n\n\nfunction _getEventManager() {\n  _getEventManager = _asyncToGenerator(function* (client) {\n    const onlineComponentProvider = yield ensureOnlineComponents(client);\n    const eventManager = onlineComponentProvider.eventManager;\n    eventManager.onListen = syncEngineListen.bind(null, onlineComponentProvider.syncEngine);\n    eventManager.onUnlisten = syncEngineUnlisten.bind(null, onlineComponentProvider.syncEngine);\n    return eventManager;\n  });\n  return _getEventManager.apply(this, arguments);\n}\n\nfunction firestoreClientEnableNetwork(client) {\n  return client.asyncQueue.enqueue( /*#__PURE__*/_asyncToGenerator(function* () {\n    const persistence = yield getPersistence(client);\n    const remoteStore = yield getRemoteStore(client);\n    persistence.setNetworkEnabled(true);\n    return remoteStoreEnableNetwork(remoteStore);\n  }));\n}\n/** Disables the network connection. Pending operations will not complete. */\n\n\nfunction firestoreClientDisableNetwork(client) {\n  return client.asyncQueue.enqueue( /*#__PURE__*/_asyncToGenerator(function* () {\n    const persistence = yield getPersistence(client);\n    const remoteStore = yield getRemoteStore(client);\n    persistence.setNetworkEnabled(false);\n    return remoteStoreDisableNetwork(remoteStore);\n  }));\n}\n/**\r\n * Returns a Promise that resolves when all writes that were pending at the time\r\n * this method was called received server acknowledgement. An acknowledgement\r\n * can be either acceptance or rejection.\r\n */\n\n\nfunction firestoreClientWaitForPendingWrites(client) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const syncEngine = yield getSyncEngine(client);\n    return syncEngineRegisterPendingWritesCallback(syncEngine, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientListen(client, query, options, observer) {\n  const wrappedObserver = new AsyncObserver(observer);\n  const listener = new QueryListener(query, wrappedObserver, options);\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const eventManager = yield getEventManager(client);\n    return eventManagerListen(eventManager, listener);\n  }));\n  return () => {\n    wrappedObserver.mute();\n    client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n      const eventManager = yield getEventManager(client);\n      return eventManagerUnlisten(eventManager, listener);\n    }));\n  };\n}\n\nfunction firestoreClientGetDocumentFromLocalCache(client, docKey) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const localStore = yield getLocalStore(client);\n    return readDocumentFromCache(localStore, docKey, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientGetDocumentViaSnapshotListener(client, key, options = {}) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const eventManager = yield getEventManager(client);\n    return readDocumentViaSnapshotListener(eventManager, client.asyncQueue, key, options, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientGetDocumentsFromLocalCache(client, query) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const localStore = yield getLocalStore(client);\n    return executeQueryFromCache(localStore, query, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientGetDocumentsViaSnapshotListener(client, query, options = {}) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const eventManager = yield getEventManager(client);\n    return executeQueryViaSnapshotListener(eventManager, client.asyncQueue, query, options, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientRunAggregateQuery(client, query, aggregates) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    // TODO (sum/avg) should we update this to use the event manager?\n    // Implement and call executeAggregateQueryViaSnapshotListener, similar\n    // to the implementation in firestoreClientGetDocumentsViaSnapshotListener\n    // above\n    try {\n      // TODO(b/277628384): check `canUseNetwork()` and handle multi-tab.\n      const datastore = yield getDatastore(client);\n      deferred.resolve(invokeRunAggregationQueryRpc(datastore, query, aggregates));\n    } catch (e) {\n      deferred.reject(e);\n    }\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientWrite(client, mutations) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const syncEngine = yield getSyncEngine(client);\n    return syncEngineWrite(syncEngine, mutations, deferred);\n  }));\n  return deferred.promise;\n}\n\nfunction firestoreClientAddSnapshotsInSyncListener(client, observer) {\n  const wrappedObserver = new AsyncObserver(observer);\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const eventManager = yield getEventManager(client);\n    return addSnapshotsInSyncListener(eventManager, wrappedObserver);\n  }));\n  return () => {\n    wrappedObserver.mute();\n    client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n      const eventManager = yield getEventManager(client);\n      return removeSnapshotsInSyncListener(eventManager, wrappedObserver);\n    }));\n  };\n}\n/**\r\n * Takes an updateFunction in which a set of reads and writes can be performed\r\n * atomically. In the updateFunction, the client can read and write values\r\n * using the supplied transaction object. After the updateFunction, all\r\n * changes will be committed. If a retryable error occurs (ex: some other\r\n * client has changed any of the data referenced), then the updateFunction\r\n * will be called again after a backoff. If the updateFunction still fails\r\n * after all retries, then the transaction will be rejected.\r\n *\r\n * The transaction object passed to the updateFunction contains methods for\r\n * accessing documents and collections. Unlike other datastore access, data\r\n * accessed with the transaction will not reflect local changes that have not\r\n * been committed. For this reason, it is required that all reads are\r\n * performed before any writes. Transactions must be performed while online.\r\n */\n\n\nfunction firestoreClientTransaction(client, updateFunction, options) {\n  const deferred = new Deferred();\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    const datastore = yield getDatastore(client);\n    new TransactionRunner(client.asyncQueue, datastore, options, updateFunction, deferred).run();\n  }));\n  return deferred.promise;\n}\n\nfunction readDocumentFromCache(_x128, _x129, _x130) {\n  return _readDocumentFromCache.apply(this, arguments);\n}\n/**\r\n * Retrieves a latency-compensated document from the backend via a\r\n * SnapshotListener.\r\n */\n\n\nfunction _readDocumentFromCache() {\n  _readDocumentFromCache = _asyncToGenerator(function* (localStore, docKey, result) {\n    try {\n      const document = yield localStoreReadDocument(localStore, docKey);\n\n      if (document.isFoundDocument()) {\n        result.resolve(document);\n      } else if (document.isNoDocument()) {\n        result.resolve(null);\n      } else {\n        result.reject(new FirestoreError(Code.UNAVAILABLE, 'Failed to get document from cache. (However, this document may ' + \"exist on the server. Run again without setting 'source' in \" + 'the GetOptions to attempt to retrieve the document from the ' + 'server.)'));\n      }\n    } catch (e) {\n      const firestoreError = wrapInUserErrorIfRecoverable(e, `Failed to get document '${docKey} from cache`);\n      result.reject(firestoreError);\n    }\n  });\n  return _readDocumentFromCache.apply(this, arguments);\n}\n\nfunction readDocumentViaSnapshotListener(eventManager, asyncQueue, key, options, result) {\n  const wrappedObserver = new AsyncObserver({\n    next: snap => {\n      // Remove query first before passing event to user to avoid\n      // user actions affecting the now stale query.\n      asyncQueue.enqueueAndForget(() => eventManagerUnlisten(eventManager, listener));\n      const exists = snap.docs.has(key);\n\n      if (!exists && snap.fromCache) {\n        // TODO(dimond): If we're online and the document doesn't\n        // exist then we resolve with a doc.exists set to false. If\n        // we're offline however, we reject the Promise in this\n        // case. Two options: 1) Cache the negative response from\n        // the server so we can deliver that even when you're\n        // offline 2) Actually reject the Promise in the online case\n        // if the document doesn't exist.\n        result.reject(new FirestoreError(Code.UNAVAILABLE, 'Failed to get document because the client is offline.'));\n      } else if (exists && snap.fromCache && options && options.source === 'server') {\n        result.reject(new FirestoreError(Code.UNAVAILABLE, 'Failed to get document from server. (However, this ' + 'document does exist in the local cache. Run again ' + 'without setting source to \"server\" to ' + 'retrieve the cached document.)'));\n      } else {\n        result.resolve(snap);\n      }\n    },\n    error: e => result.reject(e)\n  });\n  const listener = new QueryListener(newQueryForPath(key.path), wrappedObserver, {\n    includeMetadataChanges: true,\n    waitForSyncWhenOnline: true\n  });\n  return eventManagerListen(eventManager, listener);\n}\n\nfunction executeQueryFromCache(_x131, _x132, _x133) {\n  return _executeQueryFromCache.apply(this, arguments);\n}\n/**\r\n * Retrieves a latency-compensated query snapshot from the backend via a\r\n * SnapshotListener.\r\n */\n\n\nfunction _executeQueryFromCache() {\n  _executeQueryFromCache = _asyncToGenerator(function* (localStore, query, result) {\n    try {\n      const queryResult = yield localStoreExecuteQuery(localStore, query,\n      /* usePreviousResults= */\n      true);\n      const view = new View(query, queryResult.remoteKeys);\n      const viewDocChanges = view.computeDocChanges(queryResult.documents);\n      const viewChange = view.applyChanges(viewDocChanges,\n      /* updateLimboDocuments= */\n      false);\n      result.resolve(viewChange.snapshot);\n    } catch (e) {\n      const firestoreError = wrapInUserErrorIfRecoverable(e, `Failed to execute query '${query} against cache`);\n      result.reject(firestoreError);\n    }\n  });\n  return _executeQueryFromCache.apply(this, arguments);\n}\n\nfunction executeQueryViaSnapshotListener(eventManager, asyncQueue, query, options, result) {\n  const wrappedObserver = new AsyncObserver({\n    next: snapshot => {\n      // Remove query first before passing event to user to avoid\n      // user actions affecting the now stale query.\n      asyncQueue.enqueueAndForget(() => eventManagerUnlisten(eventManager, listener));\n\n      if (snapshot.fromCache && options.source === 'server') {\n        result.reject(new FirestoreError(Code.UNAVAILABLE, 'Failed to get documents from server. (However, these ' + 'documents may exist in the local cache. Run again ' + 'without setting source to \"server\" to ' + 'retrieve the cached documents.)'));\n      } else {\n        result.resolve(snapshot);\n      }\n    },\n    error: e => result.reject(e)\n  });\n  const listener = new QueryListener(query, wrappedObserver, {\n    includeMetadataChanges: true,\n    waitForSyncWhenOnline: true\n  });\n  return eventManagerListen(eventManager, listener);\n}\n\nfunction firestoreClientLoadBundle(client, databaseId, data, resultTask) {\n  const reader = createBundleReader(data, newSerializer(databaseId));\n  client.asyncQueue.enqueueAndForget( /*#__PURE__*/_asyncToGenerator(function* () {\n    syncEngineLoadBundle(yield getSyncEngine(client), reader, resultTask);\n  }));\n}\n\nfunction firestoreClientGetNamedQuery(client, queryName) {\n  return client.asyncQueue.enqueue( /*#__PURE__*/_asyncToGenerator(function* () {\n    return localStoreGetNamedQuery(yield getLocalStore(client), queryName);\n  }));\n}\n\nfunction createBundleReader(data, serializer) {\n  let content;\n\n  if (typeof data === 'string') {\n    content = newTextEncoder().encode(data);\n  } else {\n    content = data;\n  }\n\n  return newBundleReader(toByteStreamReader(content), serializer);\n}\n\nfunction firestoreClientSetIndexConfiguration(client, indexes) {\n  return client.asyncQueue.enqueue( /*#__PURE__*/_asyncToGenerator(function* () {\n    return localStoreConfigureFieldIndexes(yield getLocalStore(client), indexes);\n  }));\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Compares two `ExperimentalLongPollingOptions` objects for equality.\r\n */\n\n\nfunction longPollingOptionsEqual(options1, options2) {\n  return options1.timeoutSeconds === options2.timeoutSeconds;\n}\n/**\r\n * Creates and returns a new `ExperimentalLongPollingOptions` with the same\r\n * option values as the given instance.\r\n */\n\n\nfunction cloneLongPollingOptions(options) {\n  const clone = {};\n\n  if (options.timeoutSeconds !== undefined) {\n    clone.timeoutSeconds = options.timeoutSeconds;\n  }\n\n  return clone;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG$1 = 'ComponentProvider';\n/**\r\n * An instance map that ensures only one Datastore exists per Firestore\r\n * instance.\r\n */\n\nconst datastoreInstances = new Map();\n/**\r\n * Removes all components associated with the provided instance. Must be called\r\n * when the `Firestore` instance is terminated.\r\n */\n\nfunction removeComponents(firestore) {\n  const datastore = datastoreInstances.get(firestore);\n\n  if (datastore) {\n    logDebug(LOG_TAG$1, 'Removing Datastore');\n    datastoreInstances.delete(firestore);\n    datastore.terminate();\n  }\n}\n\nfunction makeDatabaseInfo(databaseId, appId, persistenceKey, settings) {\n  return new DatabaseInfo(databaseId, appId, persistenceKey, settings.host, settings.ssl, settings.experimentalForceLongPolling, settings.experimentalAutoDetectLongPolling, cloneLongPollingOptions(settings.experimentalLongPollingOptions), settings.useFetchStreams);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// settings() defaults:\n\n\nconst DEFAULT_HOST = 'firestore.googleapis.com';\nconst DEFAULT_SSL = true; // The minimum long-polling timeout is hardcoded on the server. The value here\n// should be kept in sync with the value used by the server, as the server will\n// silently ignore a value below the minimum and fall back to the default.\n// Googlers see b/266868871 for relevant discussion.\n\nconst MIN_LONG_POLLING_TIMEOUT_SECONDS = 5; // No maximum long-polling timeout is configured in the server, and defaults to\n// 30 seconds, which is what Watch appears to use.\n// Googlers see b/266868871 for relevant discussion.\n\nconst MAX_LONG_POLLING_TIMEOUT_SECONDS = 30; // Whether long-polling auto-detected is enabled by default.\n\nconst DEFAULT_AUTO_DETECT_LONG_POLLING = true;\n/**\r\n * A concrete type describing all the values that can be applied via a\r\n * user-supplied `FirestoreSettings` object. This is a separate type so that\r\n * defaults can be supplied and the value can be checked for equality.\r\n */\n\nclass FirestoreSettingsImpl {\n  constructor(settings) {\n    var _a, _b;\n\n    if (settings.host === undefined) {\n      if (settings.ssl !== undefined) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, \"Can't provide ssl option if host option is not set\");\n      }\n\n      this.host = DEFAULT_HOST;\n      this.ssl = DEFAULT_SSL;\n    } else {\n      this.host = settings.host;\n      this.ssl = (_a = settings.ssl) !== null && _a !== void 0 ? _a : DEFAULT_SSL;\n    }\n\n    this.credentials = settings.credentials;\n    this.ignoreUndefinedProperties = !!settings.ignoreUndefinedProperties;\n    this.cache = settings.localCache;\n\n    if (settings.cacheSizeBytes === undefined) {\n      this.cacheSizeBytes = LRU_DEFAULT_CACHE_SIZE_BYTES;\n    } else {\n      if (settings.cacheSizeBytes !== LRU_COLLECTION_DISABLED && settings.cacheSizeBytes < LRU_MINIMUM_CACHE_SIZE_BYTES) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `cacheSizeBytes must be at least ${LRU_MINIMUM_CACHE_SIZE_BYTES}`);\n      } else {\n        this.cacheSizeBytes = settings.cacheSizeBytes;\n      }\n    }\n\n    validateIsNotUsedTogether('experimentalForceLongPolling', settings.experimentalForceLongPolling, 'experimentalAutoDetectLongPolling', settings.experimentalAutoDetectLongPolling);\n    this.experimentalForceLongPolling = !!settings.experimentalForceLongPolling;\n\n    if (this.experimentalForceLongPolling) {\n      this.experimentalAutoDetectLongPolling = false;\n    } else if (settings.experimentalAutoDetectLongPolling === undefined) {\n      this.experimentalAutoDetectLongPolling = DEFAULT_AUTO_DETECT_LONG_POLLING;\n    } else {\n      // For backwards compatibility, coerce the value to boolean even though\n      // the TypeScript compiler has narrowed the type to boolean already.\n      // noinspection PointlessBooleanExpressionJS\n      this.experimentalAutoDetectLongPolling = !!settings.experimentalAutoDetectLongPolling;\n    }\n\n    this.experimentalLongPollingOptions = cloneLongPollingOptions((_b = settings.experimentalLongPollingOptions) !== null && _b !== void 0 ? _b : {});\n    validateLongPollingOptions(this.experimentalLongPollingOptions);\n    this.useFetchStreams = !!settings.useFetchStreams;\n  }\n\n  isEqual(other) {\n    return this.host === other.host && this.ssl === other.ssl && this.credentials === other.credentials && this.cacheSizeBytes === other.cacheSizeBytes && this.experimentalForceLongPolling === other.experimentalForceLongPolling && this.experimentalAutoDetectLongPolling === other.experimentalAutoDetectLongPolling && longPollingOptionsEqual(this.experimentalLongPollingOptions, other.experimentalLongPollingOptions) && this.ignoreUndefinedProperties === other.ignoreUndefinedProperties && this.useFetchStreams === other.useFetchStreams;\n  }\n\n}\n\nfunction validateLongPollingOptions(options) {\n  if (options.timeoutSeconds !== undefined) {\n    if (isNaN(options.timeoutSeconds)) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `invalid long polling timeout: ` + `${options.timeoutSeconds} (must not be NaN)`);\n    }\n\n    if (options.timeoutSeconds < MIN_LONG_POLLING_TIMEOUT_SECONDS) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `invalid long polling timeout: ${options.timeoutSeconds} ` + `(minimum allowed value is ${MIN_LONG_POLLING_TIMEOUT_SECONDS})`);\n    }\n\n    if (options.timeoutSeconds > MAX_LONG_POLLING_TIMEOUT_SECONDS) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `invalid long polling timeout: ${options.timeoutSeconds} ` + `(maximum allowed value is ${MAX_LONG_POLLING_TIMEOUT_SECONDS})`);\n    }\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * The Cloud Firestore service interface.\r\n *\r\n * Do not call this constructor directly. Instead, use {@link (getFirestore:1)}.\r\n */\n\n\nclass Firestore$1 {\n  /** @hideconstructor */\n  constructor(_authCredentials, _appCheckCredentials, _databaseId, _app) {\n    this._authCredentials = _authCredentials;\n    this._appCheckCredentials = _appCheckCredentials;\n    this._databaseId = _databaseId;\n    this._app = _app;\n    /**\r\n     * Whether it's a Firestore or Firestore Lite instance.\r\n     */\n\n    this.type = 'firestore-lite';\n    this._persistenceKey = '(lite)';\n    this._settings = new FirestoreSettingsImpl({});\n    this._settingsFrozen = false;\n  }\n  /**\r\n   * The {@link @firebase/app#FirebaseApp} associated with this `Firestore` service\r\n   * instance.\r\n   */\n\n\n  get app() {\n    if (!this._app) {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, \"Firestore was not initialized using the Firebase SDK. 'app' is \" + 'not available');\n    }\n\n    return this._app;\n  }\n\n  get _initialized() {\n    return this._settingsFrozen;\n  }\n\n  get _terminated() {\n    return this._terminateTask !== undefined;\n  }\n\n  _setSettings(settings) {\n    if (this._settingsFrozen) {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, 'Firestore has already been started and its settings can no longer ' + 'be changed. You can only modify settings before calling any other ' + 'methods on a Firestore object.');\n    }\n\n    this._settings = new FirestoreSettingsImpl(settings);\n\n    if (settings.credentials !== undefined) {\n      this._authCredentials = makeAuthCredentialsProvider(settings.credentials);\n    }\n  }\n\n  _getSettings() {\n    return this._settings;\n  }\n\n  _freezeSettings() {\n    this._settingsFrozen = true;\n    return this._settings;\n  }\n\n  _delete() {\n    if (!this._terminateTask) {\n      this._terminateTask = this._terminate();\n    }\n\n    return this._terminateTask;\n  }\n  /** Returns a JSON-serializable representation of this `Firestore` instance. */\n\n\n  toJSON() {\n    return {\n      app: this._app,\n      databaseId: this._databaseId,\n      settings: this._settings\n    };\n  }\n  /**\r\n   * Terminates all components used by this client. Subclasses can override\r\n   * this method to clean up their own dependencies, but must also call this\r\n   * method.\r\n   *\r\n   * Only ever called once.\r\n   */\n\n\n  _terminate() {\n    removeComponents(this);\n    return Promise.resolve();\n  }\n\n}\n/**\r\n * Modify this instance to communicate with the Cloud Firestore emulator.\r\n *\r\n * Note: This must be called before this instance has been used to do any\r\n * operations.\r\n *\r\n * @param firestore - The `Firestore` instance to configure to connect to the\r\n * emulator.\r\n * @param host - the emulator host (ex: localhost).\r\n * @param port - the emulator port (ex: 9000).\r\n * @param options.mockUserToken - the mock auth token to use for unit testing\r\n * Security Rules.\r\n */\n\n\nfunction connectFirestoreEmulator(firestore, host, port, options = {}) {\n  var _a;\n\n  firestore = cast(firestore, Firestore$1);\n\n  const settings = firestore._getSettings();\n\n  const newHostSetting = `${host}:${port}`;\n\n  if (settings.host !== DEFAULT_HOST && settings.host !== newHostSetting) {\n    logWarn('Host has been set in both settings() and connectFirestoreEmulator(), emulator host ' + 'will be used.');\n  }\n\n  firestore._setSettings(Object.assign(Object.assign({}, settings), {\n    host: newHostSetting,\n    ssl: false\n  }));\n\n  if (options.mockUserToken) {\n    let token;\n    let user;\n\n    if (typeof options.mockUserToken === 'string') {\n      token = options.mockUserToken;\n      user = User.MOCK_USER;\n    } else {\n      // Let createMockUserToken validate first (catches common mistakes like\n      // invalid field \"uid\" and missing field \"sub\" / \"user_id\".)\n      token = createMockUserToken(options.mockUserToken, (_a = firestore._app) === null || _a === void 0 ? void 0 : _a.options.projectId);\n      const uid = options.mockUserToken.sub || options.mockUserToken.user_id;\n\n      if (!uid) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, \"mockUserToken must contain 'sub' or 'user_id' field!\");\n      }\n\n      user = new User(uid);\n    }\n\n    firestore._authCredentials = new EmulatorAuthCredentialsProvider(new OAuthToken(token, user));\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A `DocumentReference` refers to a document location in a Firestore database\r\n * and can be used to write, read, or listen to the location. The document at\r\n * the referenced location may or may not exist.\r\n */\n\n\nclass DocumentReference {\n  /** @hideconstructor */\n  constructor(firestore,\n  /**\r\n   * If provided, the `FirestoreDataConverter` associated with this instance.\r\n   */\n  converter, _key) {\n    this.converter = converter;\n    this._key = _key;\n    /** The type of this Firestore reference. */\n\n    this.type = 'document';\n    this.firestore = firestore;\n  }\n\n  get _path() {\n    return this._key.path;\n  }\n  /**\r\n   * The document's identifier within its collection.\r\n   */\n\n\n  get id() {\n    return this._key.path.lastSegment();\n  }\n  /**\r\n   * A string representing the path of the referenced document (relative\r\n   * to the root of the database).\r\n   */\n\n\n  get path() {\n    return this._key.path.canonicalString();\n  }\n  /**\r\n   * The collection this `DocumentReference` belongs to.\r\n   */\n\n\n  get parent() {\n    return new CollectionReference(this.firestore, this.converter, this._key.path.popLast());\n  }\n\n  withConverter(converter) {\n    return new DocumentReference(this.firestore, converter, this._key);\n  }\n\n}\n/**\r\n * A `Query` refers to a query which you can read or listen to. You can also\r\n * construct refined `Query` objects by adding filters and ordering.\r\n */\n\n\nclass Query {\n  // This is the lite version of the Query class in the main SDK.\n\n  /** @hideconstructor protected */\n  constructor(firestore,\n  /**\r\n   * If provided, the `FirestoreDataConverter` associated with this instance.\r\n   */\n  converter, _query) {\n    this.converter = converter;\n    this._query = _query;\n    /** The type of this Firestore reference. */\n\n    this.type = 'query';\n    this.firestore = firestore;\n  }\n\n  withConverter(converter) {\n    return new Query(this.firestore, converter, this._query);\n  }\n\n}\n/**\r\n * A `CollectionReference` object can be used for adding documents, getting\r\n * document references, and querying for documents (using {@link (query:1)}).\r\n */\n\n\nclass CollectionReference extends Query {\n  /** @hideconstructor */\n  constructor(firestore, converter, _path) {\n    super(firestore, converter, newQueryForPath(_path));\n    this._path = _path;\n    /** The type of this Firestore reference. */\n\n    this.type = 'collection';\n  }\n  /** The collection's identifier. */\n\n\n  get id() {\n    return this._query.path.lastSegment();\n  }\n  /**\r\n   * A string representing the path of the referenced collection (relative\r\n   * to the root of the database).\r\n   */\n\n\n  get path() {\n    return this._query.path.canonicalString();\n  }\n  /**\r\n   * A reference to the containing `DocumentReference` if this is a\r\n   * subcollection. If this isn't a subcollection, the reference is null.\r\n   */\n\n\n  get parent() {\n    const parentPath = this._path.popLast();\n\n    if (parentPath.isEmpty()) {\n      return null;\n    } else {\n      return new DocumentReference(this.firestore,\n      /* converter= */\n      null, new DocumentKey(parentPath));\n    }\n  }\n\n  withConverter(converter) {\n    return new CollectionReference(this.firestore, converter, this._path);\n  }\n\n}\n\nfunction collection(parent, path, ...pathSegments) {\n  parent = getModularInstance(parent);\n  validateNonEmptyArgument('collection', 'path', path);\n\n  if (parent instanceof Firestore$1) {\n    const absolutePath = ResourcePath.fromString(path, ...pathSegments);\n    validateCollectionPath(absolutePath);\n    return new CollectionReference(parent,\n    /* converter= */\n    null, absolutePath);\n  } else {\n    if (!(parent instanceof DocumentReference) && !(parent instanceof CollectionReference)) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Expected first argument to collection() to be a CollectionReference, ' + 'a DocumentReference or FirebaseFirestore');\n    }\n\n    const absolutePath = parent._path.child(ResourcePath.fromString(path, ...pathSegments));\n\n    validateCollectionPath(absolutePath);\n    return new CollectionReference(parent.firestore,\n    /* converter= */\n    null, absolutePath);\n  }\n} // TODO(firestorelite): Consider using ErrorFactory -\n// https://github.com/firebase/firebase-js-sdk/blob/0131e1f/packages/util/src/errors.ts#L106\n\n/**\r\n * Creates and returns a new `Query` instance that includes all documents in the\r\n * database that are contained in a collection or subcollection with the\r\n * given `collectionId`.\r\n *\r\n * @param firestore - A reference to the root `Firestore` instance.\r\n * @param collectionId - Identifies the collections to query over. Every\r\n * collection or subcollection with this ID as the last segment of its path\r\n * will be included. Cannot contain a slash.\r\n * @returns The created `Query`.\r\n */\n\n\nfunction collectionGroup(firestore, collectionId) {\n  firestore = cast(firestore, Firestore$1);\n  validateNonEmptyArgument('collectionGroup', 'collection id', collectionId);\n\n  if (collectionId.indexOf('/') >= 0) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid collection ID '${collectionId}' passed to function ` + `collectionGroup(). Collection IDs must not contain '/'.`);\n  }\n\n  return new Query(firestore,\n  /* converter= */\n  null, newQueryForCollectionGroup(collectionId));\n}\n\nfunction doc(parent, path, ...pathSegments) {\n  parent = getModularInstance(parent); // We allow omission of 'pathString' but explicitly prohibit passing in both\n  // 'undefined' and 'null'.\n\n  if (arguments.length === 1) {\n    path = AutoId.newId();\n  }\n\n  validateNonEmptyArgument('doc', 'path', path);\n\n  if (parent instanceof Firestore$1) {\n    const absolutePath = ResourcePath.fromString(path, ...pathSegments);\n    validateDocumentPath(absolutePath);\n    return new DocumentReference(parent,\n    /* converter= */\n    null, new DocumentKey(absolutePath));\n  } else {\n    if (!(parent instanceof DocumentReference) && !(parent instanceof CollectionReference)) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Expected first argument to collection() to be a CollectionReference, ' + 'a DocumentReference or FirebaseFirestore');\n    }\n\n    const absolutePath = parent._path.child(ResourcePath.fromString(path, ...pathSegments));\n\n    validateDocumentPath(absolutePath);\n    return new DocumentReference(parent.firestore, parent instanceof CollectionReference ? parent.converter : null, new DocumentKey(absolutePath));\n  }\n}\n/**\r\n * Returns true if the provided references are equal.\r\n *\r\n * @param left - A reference to compare.\r\n * @param right - A reference to compare.\r\n * @returns true if the references point to the same location in the same\r\n * Firestore database.\r\n */\n\n\nfunction refEqual(left, right) {\n  left = getModularInstance(left);\n  right = getModularInstance(right);\n\n  if ((left instanceof DocumentReference || left instanceof CollectionReference) && (right instanceof DocumentReference || right instanceof CollectionReference)) {\n    return left.firestore === right.firestore && left.path === right.path && left.converter === right.converter;\n  }\n\n  return false;\n}\n/**\r\n * Returns true if the provided queries point to the same collection and apply\r\n * the same constraints.\r\n *\r\n * @param left - A `Query` to compare.\r\n * @param right - A `Query` to compare.\r\n * @returns true if the references point to the same location in the same\r\n * Firestore database.\r\n */\n\n\nfunction queryEqual(left, right) {\n  left = getModularInstance(left);\n  right = getModularInstance(right);\n\n  if (left instanceof Query && right instanceof Query) {\n    return left.firestore === right.firestore && queryEquals(left._query, right._query) && left.converter === right.converter;\n  }\n\n  return false;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst LOG_TAG = 'AsyncQueue';\n\nclass AsyncQueueImpl {\n  constructor() {\n    // The last promise in the queue.\n    this.tail = Promise.resolve(); // A list of retryable operations. Retryable operations are run in order and\n    // retried with backoff.\n\n    this.retryableOps = []; // Is this AsyncQueue being shut down? Once it is set to true, it will not\n    // be changed again.\n\n    this._isShuttingDown = false; // Operations scheduled to be queued in the future. Operations are\n    // automatically removed after they are run or canceled.\n\n    this.delayedOperations = []; // visible for testing\n\n    this.failure = null; // Flag set while there's an outstanding AsyncQueue operation, used for\n    // assertion sanity-checks.\n\n    this.operationInProgress = false; // Enabled during shutdown on Safari to prevent future access to IndexedDB.\n\n    this.skipNonRestrictedTasks = false; // List of TimerIds to fast-forward delays for.\n\n    this.timerIdsToSkip = []; // Backoff timer used to schedule retries for retryable operations\n\n    this.backoff = new ExponentialBackoff(this, \"async_queue_retry\"\n    /* TimerId.AsyncQueueRetry */\n    ); // Visibility handler that triggers an immediate retry of all retryable\n    // operations. Meant to speed up recovery when we regain file system access\n    // after page comes into foreground.\n\n    this.visibilityHandler = () => {\n      this.backoff.skipBackoff();\n    };\n  }\n\n  get isShuttingDown() {\n    return this._isShuttingDown;\n  }\n  /**\r\n   * Adds a new operation to the queue without waiting for it to complete (i.e.\r\n   * we ignore the Promise result).\r\n   */\n\n\n  enqueueAndForget(op) {\n    // eslint-disable-next-line @typescript-eslint/no-floating-promises\n    this.enqueue(op);\n  }\n\n  enqueueAndForgetEvenWhileRestricted(op) {\n    this.verifyNotFailed(); // eslint-disable-next-line @typescript-eslint/no-floating-promises\n\n    this.enqueueInternal(op);\n  }\n\n  enterRestrictedMode(purgeExistingTasks) {\n    if (!this._isShuttingDown) {\n      this._isShuttingDown = true;\n      this.skipNonRestrictedTasks = purgeExistingTasks || false;\n    }\n  }\n\n  enqueue(op) {\n    this.verifyNotFailed();\n\n    if (this._isShuttingDown) {\n      // Return a Promise which never resolves.\n      return new Promise(() => {});\n    } // Create a deferred Promise that we can return to the callee. This\n    // allows us to return a \"hanging Promise\" only to the callee and still\n    // advance the queue even when the operation is not run.\n\n\n    const task = new Deferred();\n    return this.enqueueInternal(() => {\n      if (this._isShuttingDown && this.skipNonRestrictedTasks) {\n        // We do not resolve 'task'\n        return Promise.resolve();\n      }\n\n      op().then(task.resolve, task.reject);\n      return task.promise;\n    }).then(() => task.promise);\n  }\n\n  enqueueRetryable(op) {\n    this.enqueueAndForget(() => {\n      this.retryableOps.push(op);\n      return this.retryNextOp();\n    });\n  }\n  /**\r\n   * Runs the next operation from the retryable queue. If the operation fails,\r\n   * reschedules with backoff.\r\n   */\n\n\n  retryNextOp() {\n    var _this37 = this;\n\n    return _asyncToGenerator(function* () {\n      if (_this37.retryableOps.length === 0) {\n        return;\n      }\n\n      try {\n        yield _this37.retryableOps[0]();\n\n        _this37.retryableOps.shift();\n\n        _this37.backoff.reset();\n      } catch (e) {\n        if (isIndexedDbTransactionError(e)) {\n          logDebug(LOG_TAG, 'Operation failed with retryable error: ' + e);\n        } else {\n          throw e; // Failure will be handled by AsyncQueue\n        }\n      }\n\n      if (_this37.retryableOps.length > 0) {\n        // If there are additional operations, we re-schedule `retryNextOp()`.\n        // This is necessary to run retryable operations that failed during\n        // their initial attempt since we don't know whether they are already\n        // enqueued. If, for example, `op1`, `op2`, `op3` are enqueued and `op1`\n        // needs to  be re-run, we will run `op1`, `op1`, `op2` using the\n        // already enqueued calls to `retryNextOp()`. `op3()` will then run in the\n        // call scheduled here.\n        // Since `backoffAndRun()` cancels an existing backoff and schedules a\n        // new backoff on every call, there is only ever a single additional\n        // operation in the queue.\n        _this37.backoff.backoffAndRun(() => _this37.retryNextOp());\n      }\n    })();\n  }\n\n  enqueueInternal(op) {\n    const newTail = this.tail.then(() => {\n      this.operationInProgress = true;\n      return op().catch(error => {\n        this.failure = error;\n        this.operationInProgress = false;\n        const message = getMessageOrStack(error);\n        logError('INTERNAL UNHANDLED ERROR: ', message); // Re-throw the error so that this.tail becomes a rejected Promise and\n        // all further attempts to chain (via .then) will just short-circuit\n        // and return the rejected Promise.\n\n        throw error;\n      }).then(result => {\n        this.operationInProgress = false;\n        return result;\n      });\n    });\n    this.tail = newTail;\n    return newTail;\n  }\n\n  enqueueAfterDelay(timerId, delayMs, op) {\n    this.verifyNotFailed(); // Fast-forward delays for timerIds that have been overriden.\n\n    if (this.timerIdsToSkip.indexOf(timerId) > -1) {\n      delayMs = 0;\n    }\n\n    const delayedOp = DelayedOperation.createAndSchedule(this, timerId, delayMs, op, removedOp => this.removeDelayedOperation(removedOp));\n    this.delayedOperations.push(delayedOp);\n    return delayedOp;\n  }\n\n  verifyNotFailed() {\n    if (this.failure) {\n      fail();\n    }\n  }\n\n  verifyOperationInProgress() {}\n  /**\r\n   * Waits until all currently queued tasks are finished executing. Delayed\r\n   * operations are not run.\r\n   */\n\n\n  drain() {\n    var _this38 = this;\n\n    return _asyncToGenerator(function* () {\n      // Operations in the queue prior to draining may have enqueued additional\n      // operations. Keep draining the queue until the tail is no longer advanced,\n      // which indicates that no more new operations were enqueued and that all\n      // operations were executed.\n      let currentTail;\n\n      do {\n        currentTail = _this38.tail;\n        yield currentTail;\n      } while (currentTail !== _this38.tail);\n    })();\n  }\n  /**\r\n   * For Tests: Determine if a delayed operation with a particular TimerId\r\n   * exists.\r\n   */\n\n\n  containsDelayedOperation(timerId) {\n    for (const op of this.delayedOperations) {\n      if (op.timerId === timerId) {\n        return true;\n      }\n    }\n\n    return false;\n  }\n  /**\r\n   * For Tests: Runs some or all delayed operations early.\r\n   *\r\n   * @param lastTimerId - Delayed operations up to and including this TimerId\r\n   * will be drained. Pass TimerId.All to run all delayed operations.\r\n   * @returns a Promise that resolves once all operations have been run.\r\n   */\n\n\n  runAllDelayedOperationsUntil(lastTimerId) {\n    // Note that draining may generate more delayed ops, so we do that first.\n    return this.drain().then(() => {\n      // Run ops in the same order they'd run if they ran naturally.\n      this.delayedOperations.sort((a, b) => a.targetTimeMs - b.targetTimeMs);\n\n      for (const op of this.delayedOperations) {\n        op.skipDelay();\n\n        if (lastTimerId !== \"all\"\n        /* TimerId.All */\n        && op.timerId === lastTimerId) {\n          break;\n        }\n      }\n\n      return this.drain();\n    });\n  }\n  /**\r\n   * For Tests: Skip all subsequent delays for a timer id.\r\n   */\n\n\n  skipDelaysForTimerId(timerId) {\n    this.timerIdsToSkip.push(timerId);\n  }\n  /** Called once a DelayedOperation is run or canceled. */\n\n\n  removeDelayedOperation(op) {\n    // NOTE: indexOf / slice are O(n), but delayedOperations is expected to be small.\n    const index = this.delayedOperations.indexOf(op);\n    this.delayedOperations.splice(index, 1);\n  }\n\n}\n\nfunction newAsyncQueue() {\n  return new AsyncQueueImpl();\n}\n/**\r\n * Chrome includes Error.message in Error.stack. Other browsers do not.\r\n * This returns expected output of message + stack when available.\r\n * @param error - Error or FirestoreError\r\n */\n\n\nfunction getMessageOrStack(error) {\n  let message = error.message || '';\n\n  if (error.stack) {\n    if (error.stack.includes(error.message)) {\n      message = error.stack;\n    } else {\n      message = error.message + '\\n' + error.stack;\n    }\n  }\n\n  return message;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents the task of loading a Firestore bundle. It provides progress of bundle\r\n * loading, as well as task completion and error events.\r\n *\r\n * The API is compatible with `Promise<LoadBundleTaskProgress>`.\r\n */\n\n\nclass LoadBundleTask {\n  constructor() {\n    this._progressObserver = {};\n    this._taskCompletionResolver = new Deferred();\n    this._lastProgress = {\n      taskState: 'Running',\n      totalBytes: 0,\n      totalDocuments: 0,\n      bytesLoaded: 0,\n      documentsLoaded: 0\n    };\n  }\n  /**\r\n   * Registers functions to listen to bundle loading progress events.\r\n   * @param next - Called when there is a progress update from bundle loading. Typically `next` calls occur\r\n   *   each time a Firestore document is loaded from the bundle.\r\n   * @param error - Called when an error occurs during bundle loading. The task aborts after reporting the\r\n   *   error, and there should be no more updates after this.\r\n   * @param complete - Called when the loading task is complete.\r\n   */\n\n\n  onProgress(next, error, complete) {\n    this._progressObserver = {\n      next,\n      error,\n      complete\n    };\n  }\n  /**\r\n   * Implements the `Promise<LoadBundleTaskProgress>.catch` interface.\r\n   *\r\n   * @param onRejected - Called when an error occurs during bundle loading.\r\n   */\n\n\n  catch(onRejected) {\n    return this._taskCompletionResolver.promise.catch(onRejected);\n  }\n  /**\r\n   * Implements the `Promise<LoadBundleTaskProgress>.then` interface.\r\n   *\r\n   * @param onFulfilled - Called on the completion of the loading task with a final `LoadBundleTaskProgress` update.\r\n   *   The update will always have its `taskState` set to `\"Success\"`.\r\n   * @param onRejected - Called when an error occurs during bundle loading.\r\n   */\n\n\n  then(onFulfilled, onRejected) {\n    return this._taskCompletionResolver.promise.then(onFulfilled, onRejected);\n  }\n  /**\r\n   * Notifies all observers that bundle loading has completed, with a provided\r\n   * `LoadBundleTaskProgress` object.\r\n   *\r\n   * @private\r\n   */\n\n\n  _completeWith(progress) {\n    this._updateProgress(progress);\n\n    if (this._progressObserver.complete) {\n      this._progressObserver.complete();\n    }\n\n    this._taskCompletionResolver.resolve(progress);\n  }\n  /**\r\n   * Notifies all observers that bundle loading has failed, with a provided\r\n   * `Error` as the reason.\r\n   *\r\n   * @private\r\n   */\n\n\n  _failWith(error) {\n    this._lastProgress.taskState = 'Error';\n\n    if (this._progressObserver.next) {\n      this._progressObserver.next(this._lastProgress);\n    }\n\n    if (this._progressObserver.error) {\n      this._progressObserver.error(error);\n    }\n\n    this._taskCompletionResolver.reject(error);\n  }\n  /**\r\n   * Notifies a progress update of loading a bundle.\r\n   * @param progress - The new progress.\r\n   *\r\n   * @private\r\n   */\n\n\n  _updateProgress(progress) {\n    this._lastProgress = progress;\n\n    if (this._progressObserver.next) {\n      this._progressObserver.next(progress);\n    }\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Constant used to indicate the LRU garbage collection should be disabled.\r\n * Set this value as the `cacheSizeBytes` on the settings passed to the\r\n * {@link Firestore} instance.\r\n */\n\n\nconst CACHE_SIZE_UNLIMITED = LRU_COLLECTION_DISABLED;\n/**\r\n * The Cloud Firestore service interface.\r\n *\r\n * Do not call this constructor directly. Instead, use {@link (getFirestore:1)}.\r\n */\n\nclass Firestore extends Firestore$1 {\n  /** @hideconstructor */\n  constructor(authCredentialsProvider, appCheckCredentialsProvider, databaseId, app) {\n    super(authCredentialsProvider, appCheckCredentialsProvider, databaseId, app);\n    /**\r\n     * Whether it's a {@link Firestore} or Firestore Lite instance.\r\n     */\n\n    this.type = 'firestore';\n    this._queue = newAsyncQueue();\n    this._persistenceKey = (app === null || app === void 0 ? void 0 : app.name) || '[DEFAULT]';\n  }\n\n  _terminate() {\n    if (!this._firestoreClient) {\n      // The client must be initialized to ensure that all subsequent API\n      // usage throws an exception.\n      configureFirestore(this);\n    }\n\n    return this._firestoreClient.terminate();\n  }\n\n}\n/**\r\n * Initializes a new instance of {@link Firestore} with the provided settings.\r\n * Can only be called before any other function, including\r\n * {@link (getFirestore:1)}. If the custom settings are empty, this function is\r\n * equivalent to calling {@link (getFirestore:1)}.\r\n *\r\n * @param app - The {@link @firebase/app#FirebaseApp} with which the {@link Firestore} instance will\r\n * be associated.\r\n * @param settings - A settings object to configure the {@link Firestore} instance.\r\n * @param databaseId - The name of the database.\r\n * @returns A newly initialized {@link Firestore} instance.\r\n */\n\n\nfunction initializeFirestore(app, settings, databaseId) {\n  if (!databaseId) {\n    databaseId = DEFAULT_DATABASE_NAME;\n  }\n\n  const provider = _getProvider(app, 'firestore');\n\n  if (provider.isInitialized(databaseId)) {\n    const existingInstance = provider.getImmediate({\n      identifier: databaseId\n    });\n    const initialSettings = provider.getOptions(databaseId);\n\n    if (deepEqual(initialSettings, settings)) {\n      return existingInstance;\n    } else {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, 'initializeFirestore() has already been called with ' + 'different options. To avoid this error, call initializeFirestore() with the ' + 'same options as when it was originally called, or call getFirestore() to return the' + ' already initialized instance.');\n    }\n  }\n\n  if (settings.cacheSizeBytes !== undefined && settings.localCache !== undefined) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `cache and cacheSizeBytes cannot be specified at the same time as cacheSizeBytes will` + `be deprecated. Instead, specify the cache size in the cache object`);\n  }\n\n  if (settings.cacheSizeBytes !== undefined && settings.cacheSizeBytes !== CACHE_SIZE_UNLIMITED && settings.cacheSizeBytes < LRU_MINIMUM_CACHE_SIZE_BYTES) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `cacheSizeBytes must be at least ${LRU_MINIMUM_CACHE_SIZE_BYTES}`);\n  }\n\n  return provider.initialize({\n    options: settings,\n    instanceIdentifier: databaseId\n  });\n}\n\nfunction getFirestore(appOrDatabaseId, optionalDatabaseId) {\n  const app = typeof appOrDatabaseId === 'object' ? appOrDatabaseId : getApp();\n  const databaseId = typeof appOrDatabaseId === 'string' ? appOrDatabaseId : optionalDatabaseId || DEFAULT_DATABASE_NAME;\n\n  const db = _getProvider(app, 'firestore').getImmediate({\n    identifier: databaseId\n  });\n\n  if (!db._initialized) {\n    const emulator = getDefaultEmulatorHostnameAndPort('firestore');\n\n    if (emulator) {\n      connectFirestoreEmulator(db, ...emulator);\n    }\n  }\n\n  return db;\n}\n/**\r\n * @internal\r\n */\n\n\nfunction ensureFirestoreConfigured(firestore) {\n  if (!firestore._firestoreClient) {\n    configureFirestore(firestore);\n  }\n\n  firestore._firestoreClient.verifyNotTerminated();\n\n  return firestore._firestoreClient;\n}\n\nfunction configureFirestore(firestore) {\n  var _a, _b, _c;\n\n  const settings = firestore._freezeSettings();\n\n  const databaseInfo = makeDatabaseInfo(firestore._databaseId, ((_a = firestore._app) === null || _a === void 0 ? void 0 : _a.options.appId) || '', firestore._persistenceKey, settings);\n  firestore._firestoreClient = new FirestoreClient(firestore._authCredentials, firestore._appCheckCredentials, firestore._queue, databaseInfo);\n\n  if (((_b = settings.cache) === null || _b === void 0 ? void 0 : _b._offlineComponentProvider) && ((_c = settings.cache) === null || _c === void 0 ? void 0 : _c._onlineComponentProvider)) {\n    firestore._firestoreClient._uninitializedComponentsProvider = {\n      _offlineKind: settings.cache.kind,\n      _offline: settings.cache._offlineComponentProvider,\n      _online: settings.cache._onlineComponentProvider\n    };\n  }\n}\n/**\r\n * Attempts to enable persistent storage, if possible.\r\n *\r\n * Must be called before any other functions (other than\r\n * {@link initializeFirestore}, {@link (getFirestore:1)} or\r\n * {@link clearIndexedDbPersistence}.\r\n *\r\n * If this fails, `enableIndexedDbPersistence()` will reject the promise it\r\n * returns. Note that even after this failure, the {@link Firestore} instance will\r\n * remain usable, however offline persistence will be disabled.\r\n *\r\n * There are several reasons why this can fail, which can be identified by\r\n * the `code` on the error.\r\n *\r\n *   * failed-precondition: The app is already open in another browser tab.\r\n *   * unimplemented: The browser is incompatible with the offline\r\n *     persistence implementation.\r\n *\r\n * Persistence cannot be used in a Node.js environment.\r\n *\r\n * @param firestore - The {@link Firestore} instance to enable persistence for.\r\n * @param persistenceSettings - Optional settings object to configure\r\n * persistence.\r\n * @returns A `Promise` that represents successfully enabling persistent storage.\r\n * @deprecated This function will be removed in a future major release. Instead, set\r\n * `FirestoreSettings.cache` to an instance of `IndexedDbLocalCache` to\r\n * turn on IndexedDb cache. Calling this function when `FirestoreSettings.cache`\r\n * is already specified will throw an exception.\r\n */\n\n\nfunction enableIndexedDbPersistence(firestore, persistenceSettings) {\n  firestore = cast(firestore, Firestore);\n  verifyNotInitialized(firestore);\n  const client = ensureFirestoreConfigured(firestore);\n\n  if (client._uninitializedComponentsProvider) {\n    throw new FirestoreError(Code.FAILED_PRECONDITION, 'SDK cache is already specified.');\n  }\n\n  logWarn('enableIndexedDbPersistence() will be deprecated in the future, ' + 'you can use `FirestoreSettings.cache` instead.');\n\n  const settings = firestore._freezeSettings();\n\n  const onlineComponentProvider = new OnlineComponentProvider();\n  const offlineComponentProvider = new IndexedDbOfflineComponentProvider(onlineComponentProvider, settings.cacheSizeBytes, persistenceSettings === null || persistenceSettings === void 0 ? void 0 : persistenceSettings.forceOwnership);\n  return setPersistenceProviders(client, onlineComponentProvider, offlineComponentProvider);\n}\n/**\r\n * Attempts to enable multi-tab persistent storage, if possible. If enabled\r\n * across all tabs, all operations share access to local persistence, including\r\n * shared execution of queries and latency-compensated local document updates\r\n * across all connected instances.\r\n *\r\n * If this fails, `enableMultiTabIndexedDbPersistence()` will reject the promise\r\n * it returns. Note that even after this failure, the {@link Firestore} instance will\r\n * remain usable, however offline persistence will be disabled.\r\n *\r\n * There are several reasons why this can fail, which can be identified by\r\n * the `code` on the error.\r\n *\r\n *   * failed-precondition: The app is already open in another browser tab and\r\n *     multi-tab is not enabled.\r\n *   * unimplemented: The browser is incompatible with the offline\r\n *     persistence implementation.\r\n *\r\n * @param firestore - The {@link Firestore} instance to enable persistence for.\r\n * @returns A `Promise` that represents successfully enabling persistent\r\n * storage.\r\n * @deprecated This function will be removed in a future major release. Instead, set\r\n * `FirestoreSettings.cache` to an instance of `IndexedDbLocalCache` to\r\n * turn on indexeddb cache. Calling this function when `FirestoreSettings.cache`\r\n * is already specified will throw an exception.\r\n */\n\n\nfunction enableMultiTabIndexedDbPersistence(firestore) {\n  firestore = cast(firestore, Firestore);\n  verifyNotInitialized(firestore);\n  const client = ensureFirestoreConfigured(firestore);\n\n  if (client._uninitializedComponentsProvider) {\n    throw new FirestoreError(Code.FAILED_PRECONDITION, 'SDK cache is already specified.');\n  }\n\n  logWarn('enableMultiTabIndexedDbPersistence() will be deprecated in the future, ' + 'you can use `FirestoreSettings.cache` instead.');\n\n  const settings = firestore._freezeSettings();\n\n  const onlineComponentProvider = new OnlineComponentProvider();\n  const offlineComponentProvider = new MultiTabOfflineComponentProvider(onlineComponentProvider, settings.cacheSizeBytes);\n  return setPersistenceProviders(client, onlineComponentProvider, offlineComponentProvider);\n}\n/**\r\n * Registers both the `OfflineComponentProvider` and `OnlineComponentProvider`.\r\n * If the operation fails with a recoverable error (see\r\n * `canRecoverFromIndexedDbError()` below), the returned Promise is rejected\r\n * but the client remains usable.\r\n */\n\n\nfunction setPersistenceProviders(client, onlineComponentProvider, offlineComponentProvider) {\n  const persistenceResult = new Deferred();\n  return client.asyncQueue.enqueue( /*#__PURE__*/_asyncToGenerator(function* () {\n    try {\n      yield setOfflineComponentProvider(client, offlineComponentProvider);\n      yield setOnlineComponentProvider(client, onlineComponentProvider);\n      persistenceResult.resolve();\n    } catch (e) {\n      const error = e;\n\n      if (!canFallbackFromIndexedDbError(error)) {\n        throw error;\n      }\n\n      logWarn('Error enabling indexeddb cache. Falling back to ' + 'memory cache: ' + error);\n      persistenceResult.reject(error);\n    }\n  })).then(() => persistenceResult.promise);\n}\n/**\r\n * Clears the persistent storage. This includes pending writes and cached\r\n * documents.\r\n *\r\n * Must be called while the {@link Firestore} instance is not started (after the app is\r\n * terminated or when the app is first initialized). On startup, this function\r\n * must be called before other functions (other than {@link\r\n * initializeFirestore} or {@link (getFirestore:1)})). If the {@link Firestore}\r\n * instance is still running, the promise will be rejected with the error code\r\n * of `failed-precondition`.\r\n *\r\n * Note: `clearIndexedDbPersistence()` is primarily intended to help write\r\n * reliable tests that use Cloud Firestore. It uses an efficient mechanism for\r\n * dropping existing data but does not attempt to securely overwrite or\r\n * otherwise make cached data unrecoverable. For applications that are sensitive\r\n * to the disclosure of cached data in between user sessions, we strongly\r\n * recommend not enabling persistence at all.\r\n *\r\n * @param firestore - The {@link Firestore} instance to clear persistence for.\r\n * @returns A `Promise` that is resolved when the persistent storage is\r\n * cleared. Otherwise, the promise is rejected with an error.\r\n */\n\n\nfunction clearIndexedDbPersistence(firestore) {\n  if (firestore._initialized && !firestore._terminated) {\n    throw new FirestoreError(Code.FAILED_PRECONDITION, 'Persistence can only be cleared before a Firestore instance is ' + 'initialized or after it is terminated.');\n  }\n\n  const deferred = new Deferred();\n\n  firestore._queue.enqueueAndForgetEvenWhileRestricted( /*#__PURE__*/_asyncToGenerator(function* () {\n    try {\n      yield indexedDbClearPersistence(indexedDbStoragePrefix(firestore._databaseId, firestore._persistenceKey));\n      deferred.resolve();\n    } catch (e) {\n      deferred.reject(e);\n    }\n  }));\n\n  return deferred.promise;\n}\n/**\r\n * Waits until all currently pending writes for the active user have been\r\n * acknowledged by the backend.\r\n *\r\n * The returned promise resolves immediately if there are no outstanding writes.\r\n * Otherwise, the promise waits for all previously issued writes (including\r\n * those written in a previous app session), but it does not wait for writes\r\n * that were added after the function is called. If you want to wait for\r\n * additional writes, call `waitForPendingWrites()` again.\r\n *\r\n * Any outstanding `waitForPendingWrites()` promises are rejected during user\r\n * changes.\r\n *\r\n * @returns A `Promise` which resolves when all currently pending writes have been\r\n * acknowledged by the backend.\r\n */\n\n\nfunction waitForPendingWrites(firestore) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientWaitForPendingWrites(client);\n}\n/**\r\n * Re-enables use of the network for this {@link Firestore} instance after a prior\r\n * call to {@link disableNetwork}.\r\n *\r\n * @returns A `Promise` that is resolved once the network has been enabled.\r\n */\n\n\nfunction enableNetwork(firestore) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientEnableNetwork(client);\n}\n/**\r\n * Disables network usage for this instance. It can be re-enabled via {@link\r\n * enableNetwork}. While the network is disabled, any snapshot listeners,\r\n * `getDoc()` or `getDocs()` calls will return results from cache, and any write\r\n * operations will be queued until the network is restored.\r\n *\r\n * @returns A `Promise` that is resolved once the network has been disabled.\r\n */\n\n\nfunction disableNetwork(firestore) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientDisableNetwork(client);\n}\n/**\r\n * Terminates the provided {@link Firestore} instance.\r\n *\r\n * After calling `terminate()` only the `clearIndexedDbPersistence()` function\r\n * may be used. Any other function will throw a `FirestoreError`.\r\n *\r\n * To restart after termination, create a new instance of FirebaseFirestore with\r\n * {@link (getFirestore:1)}.\r\n *\r\n * Termination does not cancel any pending writes, and any promises that are\r\n * awaiting a response from the server will not be resolved. If you have\r\n * persistence enabled, the next time you start this instance, it will resume\r\n * sending these writes to the server.\r\n *\r\n * Note: Under normal circumstances, calling `terminate()` is not required. This\r\n * function is useful only when you want to force this instance to release all\r\n * of its resources or in combination with `clearIndexedDbPersistence()` to\r\n * ensure that all local state is destroyed between test runs.\r\n *\r\n * @returns A `Promise` that is resolved when the instance has been successfully\r\n * terminated.\r\n */\n\n\nfunction terminate(firestore) {\n  _removeServiceInstance(firestore.app, 'firestore', firestore._databaseId.database);\n\n  return firestore._delete();\n}\n/**\r\n * Loads a Firestore bundle into the local cache.\r\n *\r\n * @param firestore - The {@link Firestore} instance to load bundles for.\r\n * @param bundleData - An object representing the bundle to be loaded. Valid\r\n * objects are `ArrayBuffer`, `ReadableStream<Uint8Array>` or `string`.\r\n *\r\n * @returns A `LoadBundleTask` object, which notifies callers with progress\r\n * updates, and completion or error events. It can be used as a\r\n * `Promise<LoadBundleTaskProgress>`.\r\n */\n\n\nfunction loadBundle(firestore, bundleData) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const resultTask = new LoadBundleTask();\n  firestoreClientLoadBundle(client, firestore._databaseId, bundleData, resultTask);\n  return resultTask;\n}\n/**\r\n * Reads a Firestore {@link Query} from local cache, identified by the given\r\n * name.\r\n *\r\n * The named queries are packaged  into bundles on the server side (along\r\n * with resulting documents), and loaded to local cache using `loadBundle`. Once\r\n * in local cache, use this method to extract a {@link Query} by name.\r\n *\r\n * @param firestore - The {@link Firestore} instance to read the query from.\r\n * @param name - The name of the query.\r\n * @returns A `Promise` that is resolved with the Query or `null`.\r\n */\n\n\nfunction namedQuery(firestore, name) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientGetNamedQuery(client, name).then(namedQuery => {\n    if (!namedQuery) {\n      return null;\n    }\n\n    return new Query(firestore, null, namedQuery.query);\n  });\n}\n\nfunction verifyNotInitialized(firestore) {\n  if (firestore._initialized || firestore._terminated) {\n    throw new FirestoreError(Code.FAILED_PRECONDITION, 'Firestore has already been started and persistence can no longer be ' + 'enabled. You can only enable persistence before calling any other ' + 'methods on a Firestore object.');\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction registerFirestore(variant, useFetchStreams = true) {\n  setSDKVersion(SDK_VERSION$1);\n\n  _registerComponent(new Component('firestore', (container, {\n    instanceIdentifier: databaseId,\n    options: settings\n  }) => {\n    const app = container.getProvider('app').getImmediate();\n    const firestoreInstance = new Firestore(new FirebaseAuthCredentialsProvider(container.getProvider('auth-internal')), new FirebaseAppCheckTokenProvider(container.getProvider('app-check-internal')), databaseIdFromApp(app, databaseId), app);\n    settings = Object.assign({\n      useFetchStreams\n    }, settings);\n\n    firestoreInstance._setSettings(settings);\n\n    return firestoreInstance;\n  }, 'PUBLIC').setMultipleInstances(true));\n\n  registerVersion(name, version$1, variant); // BUILD_TARGET will be replaced by values like esm5, esm2017, cjs5, etc during the compilation\n\n  registerVersion(name, version$1, 'esm2017');\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Concrete implementation of the Aggregate type.\r\n */\n\n\nclass AggregateImpl {\n  constructor(alias, aggregateType, fieldPath) {\n    this.alias = alias;\n    this.aggregateType = aggregateType;\n    this.fieldPath = fieldPath;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Represents an aggregation that can be performed by Firestore.\r\n */\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\n\n\nclass AggregateField {\n  /**\r\n   * Create a new AggregateField<T>\r\n   * @param _aggregateType Specifies the type of aggregation operation to perform.\r\n   * @param _internalFieldPath Optionally specifies the field that is aggregated.\r\n   * @internal\r\n   */\n  constructor( // TODO (sum/avg) make aggregateType public when the feature is supported\n  _aggregateType = 'count', _internalFieldPath) {\n    this._aggregateType = _aggregateType;\n    this._internalFieldPath = _internalFieldPath;\n    /** A type string to uniquely identify instances of this class. */\n\n    this.type = 'AggregateField';\n  }\n\n}\n/**\r\n * The results of executing an aggregation query.\r\n */\n\n\nclass AggregateQuerySnapshot {\n  /** @hideconstructor */\n  constructor(query, _userDataWriter, _data) {\n    this._userDataWriter = _userDataWriter;\n    this._data = _data;\n    /** A type string to uniquely identify instances of this class. */\n\n    this.type = 'AggregateQuerySnapshot';\n    this.query = query;\n  }\n  /**\r\n   * Returns the results of the aggregations performed over the underlying\r\n   * query.\r\n   *\r\n   * The keys of the returned object will be the same as those of the\r\n   * `AggregateSpec` object specified to the aggregation method, and the values\r\n   * will be the corresponding aggregation result.\r\n   *\r\n   * @returns The results of the aggregations performed over the underlying\r\n   * query.\r\n   */\n\n\n  data() {\n    return this._userDataWriter.convertObjectMap(this._data);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An immutable object representing an array of bytes.\r\n */\n\n\nclass Bytes {\n  /** @hideconstructor */\n  constructor(byteString) {\n    this._byteString = byteString;\n  }\n  /**\r\n   * Creates a new `Bytes` object from the given Base64 string, converting it to\r\n   * bytes.\r\n   *\r\n   * @param base64 - The Base64 string used to create the `Bytes` object.\r\n   */\n\n\n  static fromBase64String(base64) {\n    try {\n      return new Bytes(ByteString.fromBase64String(base64));\n    } catch (e) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Failed to construct data from Base64 string: ' + e);\n    }\n  }\n  /**\r\n   * Creates a new `Bytes` object from the given Uint8Array.\r\n   *\r\n   * @param array - The Uint8Array used to create the `Bytes` object.\r\n   */\n\n\n  static fromUint8Array(array) {\n    return new Bytes(ByteString.fromUint8Array(array));\n  }\n  /**\r\n   * Returns the underlying bytes as a Base64-encoded string.\r\n   *\r\n   * @returns The Base64-encoded string created from the `Bytes` object.\r\n   */\n\n\n  toBase64() {\n    return this._byteString.toBase64();\n  }\n  /**\r\n   * Returns the underlying bytes in a new `Uint8Array`.\r\n   *\r\n   * @returns The Uint8Array created from the `Bytes` object.\r\n   */\n\n\n  toUint8Array() {\n    return this._byteString.toUint8Array();\n  }\n  /**\r\n   * Returns a string representation of the `Bytes` object.\r\n   *\r\n   * @returns A string representation of the `Bytes` object.\r\n   */\n\n\n  toString() {\n    return 'Bytes(base64: ' + this.toBase64() + ')';\n  }\n  /**\r\n   * Returns true if this `Bytes` object is equal to the provided one.\r\n   *\r\n   * @param other - The `Bytes` object to compare against.\r\n   * @returns true if this `Bytes` object is equal to the provided one.\r\n   */\n\n\n  isEqual(other) {\n    return this._byteString.isEqual(other._byteString);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A `FieldPath` refers to a field in a document. The path may consist of a\r\n * single field name (referring to a top-level field in the document), or a\r\n * list of field names (referring to a nested field in the document).\r\n *\r\n * Create a `FieldPath` by providing field names. If more than one field\r\n * name is provided, the path will point to a nested field in a document.\r\n */\n\n\nclass FieldPath {\n  /**\r\n   * Creates a `FieldPath` from the provided field names. If more than one field\r\n   * name is provided, the path will point to a nested field in a document.\r\n   *\r\n   * @param fieldNames - A list of field names.\r\n   */\n  constructor(...fieldNames) {\n    for (let i = 0; i < fieldNames.length; ++i) {\n      if (fieldNames[i].length === 0) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid field name at argument $(i + 1). ` + 'Field names must not be empty.');\n      }\n    }\n\n    this._internalPath = new FieldPath$1(fieldNames);\n  }\n  /**\r\n   * Returns true if this `FieldPath` is equal to the provided one.\r\n   *\r\n   * @param other - The `FieldPath` to compare against.\r\n   * @returns true if this `FieldPath` is equal to the provided one.\r\n   */\n\n\n  isEqual(other) {\n    return this._internalPath.isEqual(other._internalPath);\n  }\n\n}\n/**\r\n * Returns a special sentinel `FieldPath` to refer to the ID of a document.\r\n * It can be used in queries to sort or filter by the document ID.\r\n */\n\n\nfunction documentId() {\n  return new FieldPath(DOCUMENT_KEY_NAME);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Sentinel values that can be used when writing document fields with `set()`\r\n * or `update()`.\r\n */\n\n\nclass FieldValue {\n  /**\r\n   * @param _methodName - The public API endpoint that returns this class.\r\n   * @hideconstructor\r\n   */\n  constructor(_methodName) {\n    this._methodName = _methodName;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * An immutable object representing a geographic location in Firestore. The\r\n * location is represented as latitude/longitude pair.\r\n *\r\n * Latitude values are in the range of [-90, 90].\r\n * Longitude values are in the range of [-180, 180].\r\n */\n\n\nclass GeoPoint {\n  /**\r\n   * Creates a new immutable `GeoPoint` object with the provided latitude and\r\n   * longitude values.\r\n   * @param latitude - The latitude as number between -90 and 90.\r\n   * @param longitude - The longitude as number between -180 and 180.\r\n   */\n  constructor(latitude, longitude) {\n    if (!isFinite(latitude) || latitude < -90 || latitude > 90) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Latitude must be a number between -90 and 90, but was: ' + latitude);\n    }\n\n    if (!isFinite(longitude) || longitude < -180 || longitude > 180) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Longitude must be a number between -180 and 180, but was: ' + longitude);\n    }\n\n    this._lat = latitude;\n    this._long = longitude;\n  }\n  /**\r\n   * The latitude of this `GeoPoint` instance.\r\n   */\n\n\n  get latitude() {\n    return this._lat;\n  }\n  /**\r\n   * The longitude of this `GeoPoint` instance.\r\n   */\n\n\n  get longitude() {\n    return this._long;\n  }\n  /**\r\n   * Returns true if this `GeoPoint` is equal to the provided one.\r\n   *\r\n   * @param other - The `GeoPoint` to compare against.\r\n   * @returns true if this `GeoPoint` is equal to the provided one.\r\n   */\n\n\n  isEqual(other) {\n    return this._lat === other._lat && this._long === other._long;\n  }\n  /** Returns a JSON-serializable representation of this GeoPoint. */\n\n\n  toJSON() {\n    return {\n      latitude: this._lat,\n      longitude: this._long\n    };\n  }\n  /**\r\n   * Actually private to JS consumers of our API, so this function is prefixed\r\n   * with an underscore.\r\n   */\n\n\n  _compareTo(other) {\n    return primitiveComparator(this._lat, other._lat) || primitiveComparator(this._long, other._long);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst RESERVED_FIELD_REGEX = /^__.*__$/;\n/** The result of parsing document data (e.g. for a setData call). */\n\nclass ParsedSetData {\n  constructor(data, fieldMask, fieldTransforms) {\n    this.data = data;\n    this.fieldMask = fieldMask;\n    this.fieldTransforms = fieldTransforms;\n  }\n\n  toMutation(key, precondition) {\n    if (this.fieldMask !== null) {\n      return new PatchMutation(key, this.data, this.fieldMask, precondition, this.fieldTransforms);\n    } else {\n      return new SetMutation(key, this.data, precondition, this.fieldTransforms);\n    }\n  }\n\n}\n/** The result of parsing \"update\" data (i.e. for an updateData call). */\n\n\nclass ParsedUpdateData {\n  constructor(data, // The fieldMask does not include document transforms.\n  fieldMask, fieldTransforms) {\n    this.data = data;\n    this.fieldMask = fieldMask;\n    this.fieldTransforms = fieldTransforms;\n  }\n\n  toMutation(key, precondition) {\n    return new PatchMutation(key, this.data, this.fieldMask, precondition, this.fieldTransforms);\n  }\n\n}\n\nfunction isWrite(dataSource) {\n  switch (dataSource) {\n    case 0\n    /* UserDataSource.Set */\n    : // fall through\n\n    case 2\n    /* UserDataSource.MergeSet */\n    : // fall through\n\n    case 1\n    /* UserDataSource.Update */\n    :\n      return true;\n\n    case 3\n    /* UserDataSource.Argument */\n    :\n    case 4\n    /* UserDataSource.ArrayArgument */\n    :\n      return false;\n\n    default:\n      throw fail();\n  }\n}\n/** A \"context\" object passed around while parsing user data. */\n\n\nclass ParseContextImpl {\n  /**\r\n   * Initializes a ParseContext with the given source and path.\r\n   *\r\n   * @param settings - The settings for the parser.\r\n   * @param databaseId - The database ID of the Firestore instance.\r\n   * @param serializer - The serializer to use to generate the Value proto.\r\n   * @param ignoreUndefinedProperties - Whether to ignore undefined properties\r\n   * rather than throw.\r\n   * @param fieldTransforms - A mutable list of field transforms encountered\r\n   * while parsing the data.\r\n   * @param fieldMask - A mutable list of field paths encountered while parsing\r\n   * the data.\r\n   *\r\n   * TODO(b/34871131): We don't support array paths right now, so path can be\r\n   * null to indicate the context represents any location within an array (in\r\n   * which case certain features will not work and errors will be somewhat\r\n   * compromised).\r\n   */\n  constructor(settings, databaseId, serializer, ignoreUndefinedProperties, fieldTransforms, fieldMask) {\n    this.settings = settings;\n    this.databaseId = databaseId;\n    this.serializer = serializer;\n    this.ignoreUndefinedProperties = ignoreUndefinedProperties; // Minor hack: If fieldTransforms is undefined, we assume this is an\n    // external call and we need to validate the entire path.\n\n    if (fieldTransforms === undefined) {\n      this.validatePath();\n    }\n\n    this.fieldTransforms = fieldTransforms || [];\n    this.fieldMask = fieldMask || [];\n  }\n\n  get path() {\n    return this.settings.path;\n  }\n\n  get dataSource() {\n    return this.settings.dataSource;\n  }\n  /** Returns a new context with the specified settings overwritten. */\n\n\n  contextWith(configuration) {\n    return new ParseContextImpl(Object.assign(Object.assign({}, this.settings), configuration), this.databaseId, this.serializer, this.ignoreUndefinedProperties, this.fieldTransforms, this.fieldMask);\n  }\n\n  childContextForField(field) {\n    var _a;\n\n    const childPath = (_a = this.path) === null || _a === void 0 ? void 0 : _a.child(field);\n    const context = this.contextWith({\n      path: childPath,\n      arrayElement: false\n    });\n    context.validatePathSegment(field);\n    return context;\n  }\n\n  childContextForFieldPath(field) {\n    var _a;\n\n    const childPath = (_a = this.path) === null || _a === void 0 ? void 0 : _a.child(field);\n    const context = this.contextWith({\n      path: childPath,\n      arrayElement: false\n    });\n    context.validatePath();\n    return context;\n  }\n\n  childContextForArray(index) {\n    // TODO(b/34871131): We don't support array paths right now; so make path\n    // undefined.\n    return this.contextWith({\n      path: undefined,\n      arrayElement: true\n    });\n  }\n\n  createError(reason) {\n    return createError(reason, this.settings.methodName, this.settings.hasConverter || false, this.path, this.settings.targetDoc);\n  }\n  /** Returns 'true' if 'fieldPath' was traversed when creating this context. */\n\n\n  contains(fieldPath) {\n    return this.fieldMask.find(field => fieldPath.isPrefixOf(field)) !== undefined || this.fieldTransforms.find(transform => fieldPath.isPrefixOf(transform.field)) !== undefined;\n  }\n\n  validatePath() {\n    // TODO(b/34871131): Remove null check once we have proper paths for fields\n    // within arrays.\n    if (!this.path) {\n      return;\n    }\n\n    for (let i = 0; i < this.path.length; i++) {\n      this.validatePathSegment(this.path.get(i));\n    }\n  }\n\n  validatePathSegment(segment) {\n    if (segment.length === 0) {\n      throw this.createError('Document fields must not be empty');\n    }\n\n    if (isWrite(this.dataSource) && RESERVED_FIELD_REGEX.test(segment)) {\n      throw this.createError('Document fields cannot begin and end with \"__\"');\n    }\n  }\n\n}\n/**\r\n * Helper for parsing raw user input (provided via the API) into internal model\r\n * classes.\r\n */\n\n\nclass UserDataReader {\n  constructor(databaseId, ignoreUndefinedProperties, serializer) {\n    this.databaseId = databaseId;\n    this.ignoreUndefinedProperties = ignoreUndefinedProperties;\n    this.serializer = serializer || newSerializer(databaseId);\n  }\n  /** Creates a new top-level parse context. */\n\n\n  createContext(dataSource, methodName, targetDoc, hasConverter = false) {\n    return new ParseContextImpl({\n      dataSource,\n      methodName,\n      targetDoc,\n      path: FieldPath$1.emptyPath(),\n      arrayElement: false,\n      hasConverter\n    }, this.databaseId, this.serializer, this.ignoreUndefinedProperties);\n  }\n\n}\n\nfunction newUserDataReader(firestore) {\n  const settings = firestore._freezeSettings();\n\n  const serializer = newSerializer(firestore._databaseId);\n  return new UserDataReader(firestore._databaseId, !!settings.ignoreUndefinedProperties, serializer);\n}\n/** Parse document data from a set() call. */\n\n\nfunction parseSetData(userDataReader, methodName, targetDoc, input, hasConverter, options = {}) {\n  const context = userDataReader.createContext(options.merge || options.mergeFields ? 2\n  /* UserDataSource.MergeSet */\n  : 0\n  /* UserDataSource.Set */\n  , methodName, targetDoc, hasConverter);\n  validatePlainObject('Data must be an object, but it was:', context, input);\n  const updateData = parseObject(input, context);\n  let fieldMask;\n  let fieldTransforms;\n\n  if (options.merge) {\n    fieldMask = new FieldMask(context.fieldMask);\n    fieldTransforms = context.fieldTransforms;\n  } else if (options.mergeFields) {\n    const validatedFieldPaths = [];\n\n    for (const stringOrFieldPath of options.mergeFields) {\n      const fieldPath = fieldPathFromArgument$1(methodName, stringOrFieldPath, targetDoc);\n\n      if (!context.contains(fieldPath)) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Field '${fieldPath}' is specified in your field mask but missing from your input data.`);\n      }\n\n      if (!fieldMaskContains(validatedFieldPaths, fieldPath)) {\n        validatedFieldPaths.push(fieldPath);\n      }\n    }\n\n    fieldMask = new FieldMask(validatedFieldPaths);\n    fieldTransforms = context.fieldTransforms.filter(transform => fieldMask.covers(transform.field));\n  } else {\n    fieldMask = null;\n    fieldTransforms = context.fieldTransforms;\n  }\n\n  return new ParsedSetData(new ObjectValue(updateData), fieldMask, fieldTransforms);\n}\n\nclass DeleteFieldValueImpl extends FieldValue {\n  _toFieldTransform(context) {\n    if (context.dataSource === 2\n    /* UserDataSource.MergeSet */\n    ) {\n      // No transform to add for a delete, but we need to add it to our\n      // fieldMask so it gets deleted.\n      context.fieldMask.push(context.path);\n    } else if (context.dataSource === 1\n    /* UserDataSource.Update */\n    ) {\n      throw context.createError(`${this._methodName}() can only appear at the top level ` + 'of your update data');\n    } else {\n      // We shouldn't encounter delete sentinels for queries or non-merge set() calls.\n      throw context.createError(`${this._methodName}() cannot be used with set() unless you pass ` + '{merge:true}');\n    }\n\n    return null;\n  }\n\n  isEqual(other) {\n    return other instanceof DeleteFieldValueImpl;\n  }\n\n}\n/**\r\n * Creates a child context for parsing SerializableFieldValues.\r\n *\r\n * This is different than calling `ParseContext.contextWith` because it keeps\r\n * the fieldTransforms and fieldMask separate.\r\n *\r\n * The created context has its `dataSource` set to `UserDataSource.Argument`.\r\n * Although these values are used with writes, any elements in these FieldValues\r\n * are not considered writes since they cannot contain any FieldValue sentinels,\r\n * etc.\r\n *\r\n * @param fieldValue - The sentinel FieldValue for which to create a child\r\n *     context.\r\n * @param context - The parent context.\r\n * @param arrayElement - Whether or not the FieldValue has an array.\r\n */\n\n\nfunction createSentinelChildContext(fieldValue, context, arrayElement) {\n  return new ParseContextImpl({\n    dataSource: 3\n    /* UserDataSource.Argument */\n    ,\n    targetDoc: context.settings.targetDoc,\n    methodName: fieldValue._methodName,\n    arrayElement\n  }, context.databaseId, context.serializer, context.ignoreUndefinedProperties);\n}\n\nclass ServerTimestampFieldValueImpl extends FieldValue {\n  _toFieldTransform(context) {\n    return new FieldTransform(context.path, new ServerTimestampTransform());\n  }\n\n  isEqual(other) {\n    return other instanceof ServerTimestampFieldValueImpl;\n  }\n\n}\n\nclass ArrayUnionFieldValueImpl extends FieldValue {\n  constructor(methodName, _elements) {\n    super(methodName);\n    this._elements = _elements;\n  }\n\n  _toFieldTransform(context) {\n    const parseContext = createSentinelChildContext(this, context,\n    /*array=*/\n    true);\n\n    const parsedElements = this._elements.map(element => parseData(element, parseContext));\n\n    const arrayUnion = new ArrayUnionTransformOperation(parsedElements);\n    return new FieldTransform(context.path, arrayUnion);\n  }\n\n  isEqual(other) {\n    // TODO(mrschmidt): Implement isEquals\n    return this === other;\n  }\n\n}\n\nclass ArrayRemoveFieldValueImpl extends FieldValue {\n  constructor(methodName, _elements) {\n    super(methodName);\n    this._elements = _elements;\n  }\n\n  _toFieldTransform(context) {\n    const parseContext = createSentinelChildContext(this, context,\n    /*array=*/\n    true);\n\n    const parsedElements = this._elements.map(element => parseData(element, parseContext));\n\n    const arrayUnion = new ArrayRemoveTransformOperation(parsedElements);\n    return new FieldTransform(context.path, arrayUnion);\n  }\n\n  isEqual(other) {\n    // TODO(mrschmidt): Implement isEquals\n    return this === other;\n  }\n\n}\n\nclass NumericIncrementFieldValueImpl extends FieldValue {\n  constructor(methodName, _operand) {\n    super(methodName);\n    this._operand = _operand;\n  }\n\n  _toFieldTransform(context) {\n    const numericIncrement = new NumericIncrementTransformOperation(context.serializer, toNumber(context.serializer, this._operand));\n    return new FieldTransform(context.path, numericIncrement);\n  }\n\n  isEqual(other) {\n    // TODO(mrschmidt): Implement isEquals\n    return this === other;\n  }\n\n}\n/** Parse update data from an update() call. */\n\n\nfunction parseUpdateData(userDataReader, methodName, targetDoc, input) {\n  const context = userDataReader.createContext(1\n  /* UserDataSource.Update */\n  , methodName, targetDoc);\n  validatePlainObject('Data must be an object, but it was:', context, input);\n  const fieldMaskPaths = [];\n  const updateData = ObjectValue.empty();\n  forEach(input, (key, value) => {\n    const path = fieldPathFromDotSeparatedString(methodName, key, targetDoc); // For Compat types, we have to \"extract\" the underlying types before\n    // performing validation.\n\n    value = getModularInstance(value);\n    const childContext = context.childContextForFieldPath(path);\n\n    if (value instanceof DeleteFieldValueImpl) {\n      // Add it to the field mask, but don't add anything to updateData.\n      fieldMaskPaths.push(path);\n    } else {\n      const parsedValue = parseData(value, childContext);\n\n      if (parsedValue != null) {\n        fieldMaskPaths.push(path);\n        updateData.set(path, parsedValue);\n      }\n    }\n  });\n  const mask = new FieldMask(fieldMaskPaths);\n  return new ParsedUpdateData(updateData, mask, context.fieldTransforms);\n}\n/** Parse update data from a list of field/value arguments. */\n\n\nfunction parseUpdateVarargs(userDataReader, methodName, targetDoc, field, value, moreFieldsAndValues) {\n  const context = userDataReader.createContext(1\n  /* UserDataSource.Update */\n  , methodName, targetDoc);\n  const keys = [fieldPathFromArgument$1(methodName, field, targetDoc)];\n  const values = [value];\n\n  if (moreFieldsAndValues.length % 2 !== 0) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Function ${methodName}() needs to be called with an even number ` + 'of arguments that alternate between field names and values.');\n  }\n\n  for (let i = 0; i < moreFieldsAndValues.length; i += 2) {\n    keys.push(fieldPathFromArgument$1(methodName, moreFieldsAndValues[i]));\n    values.push(moreFieldsAndValues[i + 1]);\n  }\n\n  const fieldMaskPaths = [];\n  const updateData = ObjectValue.empty(); // We iterate in reverse order to pick the last value for a field if the\n  // user specified the field multiple times.\n\n  for (let i = keys.length - 1; i >= 0; --i) {\n    if (!fieldMaskContains(fieldMaskPaths, keys[i])) {\n      const path = keys[i];\n      let value = values[i]; // For Compat types, we have to \"extract\" the underlying types before\n      // performing validation.\n\n      value = getModularInstance(value);\n      const childContext = context.childContextForFieldPath(path);\n\n      if (value instanceof DeleteFieldValueImpl) {\n        // Add it to the field mask, but don't add anything to updateData.\n        fieldMaskPaths.push(path);\n      } else {\n        const parsedValue = parseData(value, childContext);\n\n        if (parsedValue != null) {\n          fieldMaskPaths.push(path);\n          updateData.set(path, parsedValue);\n        }\n      }\n    }\n  }\n\n  const mask = new FieldMask(fieldMaskPaths);\n  return new ParsedUpdateData(updateData, mask, context.fieldTransforms);\n}\n/**\r\n * Parse a \"query value\" (e.g. value in a where filter or a value in a cursor\r\n * bound).\r\n *\r\n * @param allowArrays - Whether the query value is an array that may directly\r\n * contain additional arrays (e.g. the operand of an `in` query).\r\n */\n\n\nfunction parseQueryValue(userDataReader, methodName, input, allowArrays = false) {\n  const context = userDataReader.createContext(allowArrays ? 4\n  /* UserDataSource.ArrayArgument */\n  : 3\n  /* UserDataSource.Argument */\n  , methodName);\n  const parsed = parseData(input, context);\n  return parsed;\n}\n/**\r\n * Parses user data to Protobuf Values.\r\n *\r\n * @param input - Data to be parsed.\r\n * @param context - A context object representing the current path being parsed,\r\n * the source of the data being parsed, etc.\r\n * @returns The parsed value, or null if the value was a FieldValue sentinel\r\n * that should not be included in the resulting parsed data.\r\n */\n\n\nfunction parseData(input, context) {\n  // Unwrap the API type from the Compat SDK. This will return the API type\n  // from firestore-exp.\n  input = getModularInstance(input);\n\n  if (looksLikeJsonObject(input)) {\n    validatePlainObject('Unsupported field value:', context, input);\n    return parseObject(input, context);\n  } else if (input instanceof FieldValue) {\n    // FieldValues usually parse into transforms (except deleteField())\n    // in which case we do not want to include this field in our parsed data\n    // (as doing so will overwrite the field directly prior to the transform\n    // trying to transform it). So we don't add this location to\n    // context.fieldMask and we return null as our parsing result.\n    parseSentinelFieldValue(input, context);\n    return null;\n  } else if (input === undefined && context.ignoreUndefinedProperties) {\n    // If the input is undefined it can never participate in the fieldMask, so\n    // don't handle this below. If `ignoreUndefinedProperties` is false,\n    // `parseScalarValue` will reject an undefined value.\n    return null;\n  } else {\n    // If context.path is null we are inside an array and we don't support\n    // field mask paths more granular than the top-level array.\n    if (context.path) {\n      context.fieldMask.push(context.path);\n    }\n\n    if (input instanceof Array) {\n      // TODO(b/34871131): Include the path containing the array in the error\n      // message.\n      // In the case of IN queries, the parsed data is an array (representing\n      // the set of values to be included for the IN query) that may directly\n      // contain additional arrays (each representing an individual field\n      // value), so we disable this validation.\n      if (context.settings.arrayElement && context.dataSource !== 4\n      /* UserDataSource.ArrayArgument */\n      ) {\n        throw context.createError('Nested arrays are not supported');\n      }\n\n      return parseArray(input, context);\n    } else {\n      return parseScalarValue(input, context);\n    }\n  }\n}\n\nfunction parseObject(obj, context) {\n  const fields = {};\n\n  if (isEmpty(obj)) {\n    // If we encounter an empty object, we explicitly add it to the update\n    // mask to ensure that the server creates a map entry.\n    if (context.path && context.path.length > 0) {\n      context.fieldMask.push(context.path);\n    }\n  } else {\n    forEach(obj, (key, val) => {\n      const parsedValue = parseData(val, context.childContextForField(key));\n\n      if (parsedValue != null) {\n        fields[key] = parsedValue;\n      }\n    });\n  }\n\n  return {\n    mapValue: {\n      fields\n    }\n  };\n}\n\nfunction parseArray(array, context) {\n  const values = [];\n  let entryIndex = 0;\n\n  for (const entry of array) {\n    let parsedEntry = parseData(entry, context.childContextForArray(entryIndex));\n\n    if (parsedEntry == null) {\n      // Just include nulls in the array for fields being replaced with a\n      // sentinel.\n      parsedEntry = {\n        nullValue: 'NULL_VALUE'\n      };\n    }\n\n    values.push(parsedEntry);\n    entryIndex++;\n  }\n\n  return {\n    arrayValue: {\n      values\n    }\n  };\n}\n/**\r\n * \"Parses\" the provided FieldValueImpl, adding any necessary transforms to\r\n * context.fieldTransforms.\r\n */\n\n\nfunction parseSentinelFieldValue(value, context) {\n  // Sentinels are only supported with writes, and not within arrays.\n  if (!isWrite(context.dataSource)) {\n    throw context.createError(`${value._methodName}() can only be used with update() and set()`);\n  }\n\n  if (!context.path) {\n    throw context.createError(`${value._methodName}() is not currently supported inside arrays`);\n  }\n\n  const fieldTransform = value._toFieldTransform(context);\n\n  if (fieldTransform) {\n    context.fieldTransforms.push(fieldTransform);\n  }\n}\n/**\r\n * Helper to parse a scalar value (i.e. not an Object, Array, or FieldValue)\r\n *\r\n * @returns The parsed value\r\n */\n\n\nfunction parseScalarValue(value, context) {\n  value = getModularInstance(value);\n\n  if (value === null) {\n    return {\n      nullValue: 'NULL_VALUE'\n    };\n  } else if (typeof value === 'number') {\n    return toNumber(context.serializer, value);\n  } else if (typeof value === 'boolean') {\n    return {\n      booleanValue: value\n    };\n  } else if (typeof value === 'string') {\n    return {\n      stringValue: value\n    };\n  } else if (value instanceof Date) {\n    const timestamp = Timestamp.fromDate(value);\n    return {\n      timestampValue: toTimestamp(context.serializer, timestamp)\n    };\n  } else if (value instanceof Timestamp) {\n    // Firestore backend truncates precision down to microseconds. To ensure\n    // offline mode works the same with regards to truncation, perform the\n    // truncation immediately without waiting for the backend to do that.\n    const timestamp = new Timestamp(value.seconds, Math.floor(value.nanoseconds / 1000) * 1000);\n    return {\n      timestampValue: toTimestamp(context.serializer, timestamp)\n    };\n  } else if (value instanceof GeoPoint) {\n    return {\n      geoPointValue: {\n        latitude: value.latitude,\n        longitude: value.longitude\n      }\n    };\n  } else if (value instanceof Bytes) {\n    return {\n      bytesValue: toBytes(context.serializer, value._byteString)\n    };\n  } else if (value instanceof DocumentReference) {\n    const thisDb = context.databaseId;\n    const otherDb = value.firestore._databaseId;\n\n    if (!otherDb.isEqual(thisDb)) {\n      throw context.createError('Document reference is for database ' + `${otherDb.projectId}/${otherDb.database} but should be ` + `for database ${thisDb.projectId}/${thisDb.database}`);\n    }\n\n    return {\n      referenceValue: toResourceName(value.firestore._databaseId || context.databaseId, value._key.path)\n    };\n  } else {\n    throw context.createError(`Unsupported field value: ${valueDescription(value)}`);\n  }\n}\n/**\r\n * Checks whether an object looks like a JSON object that should be converted\r\n * into a struct. Normal class/prototype instances are considered to look like\r\n * JSON objects since they should be converted to a struct value. Arrays, Dates,\r\n * GeoPoints, etc. are not considered to look like JSON objects since they map\r\n * to specific FieldValue types other than ObjectValue.\r\n */\n\n\nfunction looksLikeJsonObject(input) {\n  return typeof input === 'object' && input !== null && !(input instanceof Array) && !(input instanceof Date) && !(input instanceof Timestamp) && !(input instanceof GeoPoint) && !(input instanceof Bytes) && !(input instanceof DocumentReference) && !(input instanceof FieldValue);\n}\n\nfunction validatePlainObject(message, context, input) {\n  if (!looksLikeJsonObject(input) || !isPlainObject(input)) {\n    const description = valueDescription(input);\n\n    if (description === 'an object') {\n      // Massage the error if it was an object.\n      throw context.createError(message + ' a custom object');\n    } else {\n      throw context.createError(message + ' ' + description);\n    }\n  }\n}\n/**\r\n * Helper that calls fromDotSeparatedString() but wraps any error thrown.\r\n */\n\n\nfunction fieldPathFromArgument$1(methodName, path, targetDoc) {\n  // If required, replace the FieldPath Compat class with with the firestore-exp\n  // FieldPath.\n  path = getModularInstance(path);\n\n  if (path instanceof FieldPath) {\n    return path._internalPath;\n  } else if (typeof path === 'string') {\n    return fieldPathFromDotSeparatedString(methodName, path);\n  } else {\n    const message = 'Field path arguments must be of type string or ';\n    throw createError(message, methodName,\n    /* hasConverter= */\n    false,\n    /* path= */\n    undefined, targetDoc);\n  }\n}\n/**\r\n * Matches any characters in a field path string that are reserved.\r\n */\n\n\nconst FIELD_PATH_RESERVED = new RegExp('[~\\\\*/\\\\[\\\\]]');\n/**\r\n * Wraps fromDotSeparatedString with an error message about the method that\r\n * was thrown.\r\n * @param methodName - The publicly visible method name\r\n * @param path - The dot-separated string form of a field path which will be\r\n * split on dots.\r\n * @param targetDoc - The document against which the field path will be\r\n * evaluated.\r\n */\n\nfunction fieldPathFromDotSeparatedString(methodName, path, targetDoc) {\n  const found = path.search(FIELD_PATH_RESERVED);\n\n  if (found >= 0) {\n    throw createError(`Invalid field path (${path}). Paths must not contain ` + `'~', '*', '/', '[', or ']'`, methodName,\n    /* hasConverter= */\n    false,\n    /* path= */\n    undefined, targetDoc);\n  }\n\n  try {\n    return new FieldPath(...path.split('.'))._internalPath;\n  } catch (e) {\n    throw createError(`Invalid field path (${path}). Paths must not be empty, ` + `begin with '.', end with '.', or contain '..'`, methodName,\n    /* hasConverter= */\n    false,\n    /* path= */\n    undefined, targetDoc);\n  }\n}\n\nfunction createError(reason, methodName, hasConverter, path, targetDoc) {\n  const hasPath = path && !path.isEmpty();\n  const hasDocument = targetDoc !== undefined;\n  let message = `Function ${methodName}() called with invalid data`;\n\n  if (hasConverter) {\n    message += ' (via `toFirestore()`)';\n  }\n\n  message += '. ';\n  let description = '';\n\n  if (hasPath || hasDocument) {\n    description += ' (found';\n\n    if (hasPath) {\n      description += ` in field ${path}`;\n    }\n\n    if (hasDocument) {\n      description += ` in document ${targetDoc}`;\n    }\n\n    description += ')';\n  }\n\n  return new FirestoreError(Code.INVALID_ARGUMENT, message + reason + description);\n}\n/** Checks `haystack` if FieldPath `needle` is present. Runs in O(n). */\n\n\nfunction fieldMaskContains(haystack, needle) {\n  return haystack.some(v => v.isEqual(needle));\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A `DocumentSnapshot` contains data read from a document in your Firestore\r\n * database. The data can be extracted with `.data()` or `.get(<field>)` to\r\n * get a specific field.\r\n *\r\n * For a `DocumentSnapshot` that points to a non-existing document, any data\r\n * access will return 'undefined'. You can use the `exists()` method to\r\n * explicitly verify a document's existence.\r\n */\n\n\nclass DocumentSnapshot$1 {\n  // Note: This class is stripped down version of the DocumentSnapshot in\n  // the legacy SDK. The changes are:\n  // - No support for SnapshotMetadata.\n  // - No support for SnapshotOptions.\n\n  /** @hideconstructor protected */\n  constructor(_firestore, _userDataWriter, _key, _document, _converter) {\n    this._firestore = _firestore;\n    this._userDataWriter = _userDataWriter;\n    this._key = _key;\n    this._document = _document;\n    this._converter = _converter;\n  }\n  /** Property of the `DocumentSnapshot` that provides the document's ID. */\n\n\n  get id() {\n    return this._key.path.lastSegment();\n  }\n  /**\r\n   * The `DocumentReference` for the document included in the `DocumentSnapshot`.\r\n   */\n\n\n  get ref() {\n    return new DocumentReference(this._firestore, this._converter, this._key);\n  }\n  /**\r\n   * Signals whether or not the document at the snapshot's location exists.\r\n   *\r\n   * @returns true if the document exists.\r\n   */\n\n\n  exists() {\n    return this._document !== null;\n  }\n  /**\r\n   * Retrieves all fields in the document as an `Object`. Returns `undefined` if\r\n   * the document doesn't exist.\r\n   *\r\n   * @returns An `Object` containing all fields in the document or `undefined`\r\n   * if the document doesn't exist.\r\n   */\n\n\n  data() {\n    if (!this._document) {\n      return undefined;\n    } else if (this._converter) {\n      // We only want to use the converter and create a new DocumentSnapshot\n      // if a converter has been provided.\n      const snapshot = new QueryDocumentSnapshot$1(this._firestore, this._userDataWriter, this._key, this._document,\n      /* converter= */\n      null);\n      return this._converter.fromFirestore(snapshot);\n    } else {\n      return this._userDataWriter.convertValue(this._document.data.value);\n    }\n  }\n  /**\r\n   * Retrieves the field specified by `fieldPath`. Returns `undefined` if the\r\n   * document or field doesn't exist.\r\n   *\r\n   * @param fieldPath - The path (for example 'foo' or 'foo.bar') to a specific\r\n   * field.\r\n   * @returns The data at the specified field location or undefined if no such\r\n   * field exists in the document.\r\n   */\n  // We are using `any` here to avoid an explicit cast by our users.\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n\n  get(fieldPath) {\n    if (this._document) {\n      const value = this._document.data.field(fieldPathFromArgument('DocumentSnapshot.get', fieldPath));\n\n      if (value !== null) {\n        return this._userDataWriter.convertValue(value);\n      }\n    }\n\n    return undefined;\n  }\n\n}\n/**\r\n * A `QueryDocumentSnapshot` contains data read from a document in your\r\n * Firestore database as part of a query. The document is guaranteed to exist\r\n * and its data can be extracted with `.data()` or `.get(<field>)` to get a\r\n * specific field.\r\n *\r\n * A `QueryDocumentSnapshot` offers the same API surface as a\r\n * `DocumentSnapshot`. Since query results contain only existing documents, the\r\n * `exists` property will always be true and `data()` will never return\r\n * 'undefined'.\r\n */\n\n\nclass QueryDocumentSnapshot$1 extends DocumentSnapshot$1 {\n  /**\r\n   * Retrieves all fields in the document as an `Object`.\r\n   *\r\n   * @override\r\n   * @returns An `Object` containing all fields in the document.\r\n   */\n  data() {\n    return super.data();\n  }\n\n}\n/**\r\n * Helper that calls `fromDotSeparatedString()` but wraps any error thrown.\r\n */\n\n\nfunction fieldPathFromArgument(methodName, arg) {\n  if (typeof arg === 'string') {\n    return fieldPathFromDotSeparatedString(methodName, arg);\n  } else if (arg instanceof FieldPath) {\n    return arg._internalPath;\n  } else {\n    return arg._delegate._internalPath;\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction validateHasExplicitOrderByForLimitToLast(query) {\n  if (query.limitType === \"L\"\n  /* LimitType.Last */\n  && query.explicitOrderBy.length === 0) {\n    throw new FirestoreError(Code.UNIMPLEMENTED, 'limitToLast() queries require specifying at least one orderBy() clause');\n  }\n}\n/**\r\n * An `AppliableConstraint` is an abstraction of a constraint that can be applied\r\n * to a Firestore query.\r\n */\n\n\nclass AppliableConstraint {}\n/**\r\n * A `QueryConstraint` is used to narrow the set of documents returned by a\r\n * Firestore query. `QueryConstraint`s are created by invoking {@link where},\r\n * {@link orderBy}, {@link (startAt:1)}, {@link (startAfter:1)}, {@link\r\n * (endBefore:1)}, {@link (endAt:1)}, {@link limit}, {@link limitToLast} and\r\n * can then be passed to {@link (query:1)} to create a new query instance that\r\n * also contains this `QueryConstraint`.\r\n */\n\n\nclass QueryConstraint extends AppliableConstraint {}\n\nfunction query(query, queryConstraint, ...additionalQueryConstraints) {\n  let queryConstraints = [];\n\n  if (queryConstraint instanceof AppliableConstraint) {\n    queryConstraints.push(queryConstraint);\n  }\n\n  queryConstraints = queryConstraints.concat(additionalQueryConstraints);\n  validateQueryConstraintArray(queryConstraints);\n\n  for (const constraint of queryConstraints) {\n    query = constraint._apply(query);\n  }\n\n  return query;\n}\n/**\r\n * A `QueryFieldFilterConstraint` is used to narrow the set of documents returned by\r\n * a Firestore query by filtering on one or more document fields.\r\n * `QueryFieldFilterConstraint`s are created by invoking {@link where} and can then\r\n * be passed to {@link (query:1)} to create a new query instance that also contains\r\n * this `QueryFieldFilterConstraint`.\r\n */\n\n\nclass QueryFieldFilterConstraint extends QueryConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(_field, _op, _value) {\n    super();\n    this._field = _field;\n    this._op = _op;\n    this._value = _value;\n    /** The type of this query constraint */\n\n    this.type = 'where';\n  }\n\n  static _create(_field, _op, _value) {\n    return new QueryFieldFilterConstraint(_field, _op, _value);\n  }\n\n  _apply(query) {\n    const filter = this._parse(query);\n\n    validateNewFieldFilter(query._query, filter);\n    return new Query(query.firestore, query.converter, queryWithAddedFilter(query._query, filter));\n  }\n\n  _parse(query) {\n    const reader = newUserDataReader(query.firestore);\n    const filter = newQueryFilter(query._query, 'where', reader, query.firestore._databaseId, this._field, this._op, this._value);\n    return filter;\n  }\n\n}\n/**\r\n * Creates a {@link QueryFieldFilterConstraint} that enforces that documents\r\n * must contain the specified field and that the value should satisfy the\r\n * relation constraint provided.\r\n *\r\n * @param fieldPath - The path to compare\r\n * @param opStr - The operation string (e.g \"&lt;\", \"&lt;=\", \"==\", \"&lt;\",\r\n *   \"&lt;=\", \"!=\").\r\n * @param value - The value for comparison\r\n * @returns The created {@link QueryFieldFilterConstraint}.\r\n */\n\n\nfunction where(fieldPath, opStr, value) {\n  const op = opStr;\n  const field = fieldPathFromArgument('where', fieldPath);\n  return QueryFieldFilterConstraint._create(field, op, value);\n}\n/**\r\n * A `QueryCompositeFilterConstraint` is used to narrow the set of documents\r\n * returned by a Firestore query by performing the logical OR or AND of multiple\r\n * {@link QueryFieldFilterConstraint}s or {@link QueryCompositeFilterConstraint}s.\r\n * `QueryCompositeFilterConstraint`s are created by invoking {@link or} or\r\n * {@link and} and can then be passed to {@link (query:1)} to create a new query\r\n * instance that also contains the `QueryCompositeFilterConstraint`.\r\n */\n\n\nclass QueryCompositeFilterConstraint extends AppliableConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(\n  /** The type of this query constraint */\n  type, _queryConstraints) {\n    super();\n    this.type = type;\n    this._queryConstraints = _queryConstraints;\n  }\n\n  static _create(type, _queryConstraints) {\n    return new QueryCompositeFilterConstraint(type, _queryConstraints);\n  }\n\n  _parse(query) {\n    const parsedFilters = this._queryConstraints.map(queryConstraint => {\n      return queryConstraint._parse(query);\n    }).filter(parsedFilter => parsedFilter.getFilters().length > 0);\n\n    if (parsedFilters.length === 1) {\n      return parsedFilters[0];\n    }\n\n    return CompositeFilter.create(parsedFilters, this._getOperator());\n  }\n\n  _apply(query) {\n    const parsedFilter = this._parse(query);\n\n    if (parsedFilter.getFilters().length === 0) {\n      // Return the existing query if not adding any more filters (e.g. an empty\n      // composite filter).\n      return query;\n    }\n\n    validateNewFilter(query._query, parsedFilter);\n    return new Query(query.firestore, query.converter, queryWithAddedFilter(query._query, parsedFilter));\n  }\n\n  _getQueryConstraints() {\n    return this._queryConstraints;\n  }\n\n  _getOperator() {\n    return this.type === 'and' ? \"and\"\n    /* CompositeOperator.AND */\n    : \"or\"\n    /* CompositeOperator.OR */\n    ;\n  }\n\n}\n/**\r\n * Creates a new {@link QueryCompositeFilterConstraint} that is a disjunction of\r\n * the given filter constraints. A disjunction filter includes a document if it\r\n * satisfies any of the given filters.\r\n *\r\n * @param queryConstraints - Optional. The list of\r\n * {@link QueryFilterConstraint}s to perform a disjunction for. These must be\r\n * created with calls to {@link where}, {@link or}, or {@link and}.\r\n * @returns The newly created {@link QueryCompositeFilterConstraint}.\r\n */\n\n\nfunction or(...queryConstraints) {\n  // Only support QueryFilterConstraints\n  queryConstraints.forEach(queryConstraint => validateQueryFilterConstraint('or', queryConstraint));\n  return QueryCompositeFilterConstraint._create(\"or\"\n  /* CompositeOperator.OR */\n  , queryConstraints);\n}\n/**\r\n * Creates a new {@link QueryCompositeFilterConstraint} that is a conjunction of\r\n * the given filter constraints. A conjunction filter includes a document if it\r\n * satisfies all of the given filters.\r\n *\r\n * @param queryConstraints - Optional. The list of\r\n * {@link QueryFilterConstraint}s to perform a conjunction for. These must be\r\n * created with calls to {@link where}, {@link or}, or {@link and}.\r\n * @returns The newly created {@link QueryCompositeFilterConstraint}.\r\n */\n\n\nfunction and(...queryConstraints) {\n  // Only support QueryFilterConstraints\n  queryConstraints.forEach(queryConstraint => validateQueryFilterConstraint('and', queryConstraint));\n  return QueryCompositeFilterConstraint._create(\"and\"\n  /* CompositeOperator.AND */\n  , queryConstraints);\n}\n/**\r\n * A `QueryOrderByConstraint` is used to sort the set of documents returned by a\r\n * Firestore query. `QueryOrderByConstraint`s are created by invoking\r\n * {@link orderBy} and can then be passed to {@link (query:1)} to create a new query\r\n * instance that also contains this `QueryOrderByConstraint`.\r\n *\r\n * Note: Documents that do not contain the orderBy field will not be present in\r\n * the query result.\r\n */\n\n\nclass QueryOrderByConstraint extends QueryConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(_field, _direction) {\n    super();\n    this._field = _field;\n    this._direction = _direction;\n    /** The type of this query constraint */\n\n    this.type = 'orderBy';\n  }\n\n  static _create(_field, _direction) {\n    return new QueryOrderByConstraint(_field, _direction);\n  }\n\n  _apply(query) {\n    const orderBy = newQueryOrderBy(query._query, this._field, this._direction);\n    return new Query(query.firestore, query.converter, queryWithAddedOrderBy(query._query, orderBy));\n  }\n\n}\n/**\r\n * Creates a {@link QueryOrderByConstraint} that sorts the query result by the\r\n * specified field, optionally in descending order instead of ascending.\r\n *\r\n * Note: Documents that do not contain the specified field will not be present\r\n * in the query result.\r\n *\r\n * @param fieldPath - The field to sort by.\r\n * @param directionStr - Optional direction to sort by ('asc' or 'desc'). If\r\n * not specified, order will be ascending.\r\n * @returns The created {@link QueryOrderByConstraint}.\r\n */\n\n\nfunction orderBy(fieldPath, directionStr = 'asc') {\n  const direction = directionStr;\n  const path = fieldPathFromArgument('orderBy', fieldPath);\n  return QueryOrderByConstraint._create(path, direction);\n}\n/**\r\n * A `QueryLimitConstraint` is used to limit the number of documents returned by\r\n * a Firestore query.\r\n * `QueryLimitConstraint`s are created by invoking {@link limit} or\r\n * {@link limitToLast} and can then be passed to {@link (query:1)} to create a new\r\n * query instance that also contains this `QueryLimitConstraint`.\r\n */\n\n\nclass QueryLimitConstraint extends QueryConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(\n  /** The type of this query constraint */\n  type, _limit, _limitType) {\n    super();\n    this.type = type;\n    this._limit = _limit;\n    this._limitType = _limitType;\n  }\n\n  static _create(type, _limit, _limitType) {\n    return new QueryLimitConstraint(type, _limit, _limitType);\n  }\n\n  _apply(query) {\n    return new Query(query.firestore, query.converter, queryWithLimit(query._query, this._limit, this._limitType));\n  }\n\n}\n/**\r\n * Creates a {@link QueryLimitConstraint} that only returns the first matching\r\n * documents.\r\n *\r\n * @param limit - The maximum number of items to return.\r\n * @returns The created {@link QueryLimitConstraint}.\r\n */\n\n\nfunction limit(limit) {\n  validatePositiveNumber('limit', limit);\n  return QueryLimitConstraint._create('limit', limit, \"F\"\n  /* LimitType.First */\n  );\n}\n/**\r\n * Creates a {@link QueryLimitConstraint} that only returns the last matching\r\n * documents.\r\n *\r\n * You must specify at least one `orderBy` clause for `limitToLast` queries,\r\n * otherwise an exception will be thrown during execution.\r\n *\r\n * @param limit - The maximum number of items to return.\r\n * @returns The created {@link QueryLimitConstraint}.\r\n */\n\n\nfunction limitToLast(limit) {\n  validatePositiveNumber('limitToLast', limit);\n  return QueryLimitConstraint._create('limitToLast', limit, \"L\"\n  /* LimitType.Last */\n  );\n}\n/**\r\n * A `QueryStartAtConstraint` is used to exclude documents from the start of a\r\n * result set returned by a Firestore query.\r\n * `QueryStartAtConstraint`s are created by invoking {@link (startAt:1)} or\r\n * {@link (startAfter:1)} and can then be passed to {@link (query:1)} to create a\r\n * new query instance that also contains this `QueryStartAtConstraint`.\r\n */\n\n\nclass QueryStartAtConstraint extends QueryConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(\n  /** The type of this query constraint */\n  type, _docOrFields, _inclusive) {\n    super();\n    this.type = type;\n    this._docOrFields = _docOrFields;\n    this._inclusive = _inclusive;\n  }\n\n  static _create(type, _docOrFields, _inclusive) {\n    return new QueryStartAtConstraint(type, _docOrFields, _inclusive);\n  }\n\n  _apply(query) {\n    const bound = newQueryBoundFromDocOrFields(query, this.type, this._docOrFields, this._inclusive);\n    return new Query(query.firestore, query.converter, queryWithStartAt(query._query, bound));\n  }\n\n}\n\nfunction startAt(...docOrFields) {\n  return QueryStartAtConstraint._create('startAt', docOrFields,\n  /*inclusive=*/\n  true);\n}\n\nfunction startAfter(...docOrFields) {\n  return QueryStartAtConstraint._create('startAfter', docOrFields,\n  /*inclusive=*/\n  false);\n}\n/**\r\n * A `QueryEndAtConstraint` is used to exclude documents from the end of a\r\n * result set returned by a Firestore query.\r\n * `QueryEndAtConstraint`s are created by invoking {@link (endAt:1)} or\r\n * {@link (endBefore:1)} and can then be passed to {@link (query:1)} to create a new\r\n * query instance that also contains this `QueryEndAtConstraint`.\r\n */\n\n\nclass QueryEndAtConstraint extends QueryConstraint {\n  /**\r\n   * @internal\r\n   */\n  constructor(\n  /** The type of this query constraint */\n  type, _docOrFields, _inclusive) {\n    super();\n    this.type = type;\n    this._docOrFields = _docOrFields;\n    this._inclusive = _inclusive;\n  }\n\n  static _create(type, _docOrFields, _inclusive) {\n    return new QueryEndAtConstraint(type, _docOrFields, _inclusive);\n  }\n\n  _apply(query) {\n    const bound = newQueryBoundFromDocOrFields(query, this.type, this._docOrFields, this._inclusive);\n    return new Query(query.firestore, query.converter, queryWithEndAt(query._query, bound));\n  }\n\n}\n\nfunction endBefore(...docOrFields) {\n  return QueryEndAtConstraint._create('endBefore', docOrFields,\n  /*inclusive=*/\n  false);\n}\n\nfunction endAt(...docOrFields) {\n  return QueryEndAtConstraint._create('endAt', docOrFields,\n  /*inclusive=*/\n  true);\n}\n/** Helper function to create a bound from a document or fields */\n\n\nfunction newQueryBoundFromDocOrFields(query, methodName, docOrFields, inclusive) {\n  docOrFields[0] = getModularInstance(docOrFields[0]);\n\n  if (docOrFields[0] instanceof DocumentSnapshot$1) {\n    return newQueryBoundFromDocument(query._query, query.firestore._databaseId, methodName, docOrFields[0]._document, inclusive);\n  } else {\n    const reader = newUserDataReader(query.firestore);\n    return newQueryBoundFromFields(query._query, query.firestore._databaseId, reader, methodName, docOrFields, inclusive);\n  }\n}\n\nfunction newQueryFilter(query, methodName, dataReader, databaseId, fieldPath, op, value) {\n  let fieldValue;\n\n  if (fieldPath.isKeyField()) {\n    if (op === \"array-contains\"\n    /* Operator.ARRAY_CONTAINS */\n    || op === \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    ) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid Query. You can't perform '${op}' queries on documentId().`);\n    } else if (op === \"in\"\n    /* Operator.IN */\n    || op === \"not-in\"\n    /* Operator.NOT_IN */\n    ) {\n      validateDisjunctiveFilterElements(value, op);\n      const referenceList = [];\n\n      for (const arrayValue of value) {\n        referenceList.push(parseDocumentIdValue(databaseId, query, arrayValue));\n      }\n\n      fieldValue = {\n        arrayValue: {\n          values: referenceList\n        }\n      };\n    } else {\n      fieldValue = parseDocumentIdValue(databaseId, query, value);\n    }\n  } else {\n    if (op === \"in\"\n    /* Operator.IN */\n    || op === \"not-in\"\n    /* Operator.NOT_IN */\n    || op === \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    ) {\n      validateDisjunctiveFilterElements(value, op);\n    }\n\n    fieldValue = parseQueryValue(dataReader, methodName, value,\n    /* allowArrays= */\n    op === \"in\"\n    /* Operator.IN */\n    || op === \"not-in\"\n    /* Operator.NOT_IN */\n    );\n  }\n\n  const filter = FieldFilter.create(fieldPath, op, fieldValue);\n  return filter;\n}\n\nfunction newQueryOrderBy(query, fieldPath, direction) {\n  if (query.startAt !== null) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. You must not call startAt() or startAfter() before ' + 'calling orderBy().');\n  }\n\n  if (query.endAt !== null) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. You must not call endAt() or endBefore() before ' + 'calling orderBy().');\n  }\n\n  const orderBy = new OrderBy(fieldPath, direction);\n  validateNewOrderBy(query, orderBy);\n  return orderBy;\n}\n/**\r\n * Create a `Bound` from a query and a document.\r\n *\r\n * Note that the `Bound` will always include the key of the document\r\n * and so only the provided document will compare equal to the returned\r\n * position.\r\n *\r\n * Will throw if the document does not contain all fields of the order by\r\n * of the query or if any of the fields in the order by are an uncommitted\r\n * server timestamp.\r\n */\n\n\nfunction newQueryBoundFromDocument(query, databaseId, methodName, doc, inclusive) {\n  if (!doc) {\n    throw new FirestoreError(Code.NOT_FOUND, `Can't use a DocumentSnapshot that doesn't exist for ` + `${methodName}().`);\n  }\n\n  const components = []; // Because people expect to continue/end a query at the exact document\n  // provided, we need to use the implicit sort order rather than the explicit\n  // sort order, because it's guaranteed to contain the document key. That way\n  // the position becomes unambiguous and the query continues/ends exactly at\n  // the provided document. Without the key (by using the explicit sort\n  // orders), multiple documents could match the position, yielding duplicate\n  // results.\n\n  for (const orderBy of queryOrderBy(query)) {\n    if (orderBy.field.isKeyField()) {\n      components.push(refValue(databaseId, doc.key));\n    } else {\n      const value = doc.data.field(orderBy.field);\n\n      if (isServerTimestamp(value)) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. You are trying to start or end a query using a ' + 'document for which the field \"' + orderBy.field + '\" is an uncommitted server timestamp. (Since the value of ' + 'this field is unknown, you cannot start/end a query with it.)');\n      } else if (value !== null) {\n        components.push(value);\n      } else {\n        const field = orderBy.field.canonicalString();\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. You are trying to start or end a query using a ` + `document for which the field '${field}' (used as the ` + `orderBy) does not exist.`);\n      }\n    }\n  }\n\n  return new Bound(components, inclusive);\n}\n/**\r\n * Converts a list of field values to a `Bound` for the given query.\r\n */\n\n\nfunction newQueryBoundFromFields(query, databaseId, dataReader, methodName, values, inclusive) {\n  // Use explicit order by's because it has to match the query the user made\n  const orderBy = query.explicitOrderBy;\n\n  if (values.length > orderBy.length) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Too many arguments provided to ${methodName}(). ` + `The number of arguments must be less than or equal to the ` + `number of orderBy() clauses`);\n  }\n\n  const components = [];\n\n  for (let i = 0; i < values.length; i++) {\n    const rawValue = values[i];\n    const orderByComponent = orderBy[i];\n\n    if (orderByComponent.field.isKeyField()) {\n      if (typeof rawValue !== 'string') {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. Expected a string for document ID in ` + `${methodName}(), but got a ${typeof rawValue}`);\n      }\n\n      if (!isCollectionGroupQuery(query) && rawValue.indexOf('/') !== -1) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. When querying a collection and ordering by documentId(), ` + `the value passed to ${methodName}() must be a plain document ID, but ` + `'${rawValue}' contains a slash.`);\n      }\n\n      const path = query.path.child(ResourcePath.fromString(rawValue));\n\n      if (!DocumentKey.isDocumentKey(path)) {\n        throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. When querying a collection group and ordering by ` + `documentId(), the value passed to ${methodName}() must result in a ` + `valid document path, but '${path}' is not because it contains an odd number ` + `of segments.`);\n      }\n\n      const key = new DocumentKey(path);\n      components.push(refValue(databaseId, key));\n    } else {\n      const wrapped = parseQueryValue(dataReader, methodName, rawValue);\n      components.push(wrapped);\n    }\n  }\n\n  return new Bound(components, inclusive);\n}\n/**\r\n * Parses the given `documentIdValue` into a `ReferenceValue`, throwing\r\n * appropriate errors if the value is anything other than a `DocumentReference`\r\n * or `string`, or if the string is malformed.\r\n */\n\n\nfunction parseDocumentIdValue(databaseId, query, documentIdValue) {\n  documentIdValue = getModularInstance(documentIdValue);\n\n  if (typeof documentIdValue === 'string') {\n    if (documentIdValue === '') {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. When querying with documentId(), you ' + 'must provide a valid document ID, but it was an empty string.');\n    }\n\n    if (!isCollectionGroupQuery(query) && documentIdValue.indexOf('/') !== -1) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. When querying a collection by ` + `documentId(), you must provide a plain document ID, but ` + `'${documentIdValue}' contains a '/' character.`);\n    }\n\n    const path = query.path.child(ResourcePath.fromString(documentIdValue));\n\n    if (!DocumentKey.isDocumentKey(path)) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. When querying a collection group by ` + `documentId(), the value provided must result in a valid document path, ` + `but '${path}' is not because it has an odd number of segments (${path.length}).`);\n    }\n\n    return refValue(databaseId, new DocumentKey(path));\n  } else if (documentIdValue instanceof DocumentReference) {\n    return refValue(databaseId, documentIdValue._key);\n  } else {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. When querying with documentId(), you must provide a valid ` + `string or a DocumentReference, but it was: ` + `${valueDescription(documentIdValue)}.`);\n  }\n}\n/**\r\n * Validates that the value passed into a disjunctive filter satisfies all\r\n * array requirements.\r\n */\n\n\nfunction validateDisjunctiveFilterElements(value, operator) {\n  if (!Array.isArray(value) || value.length === 0) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid Query. A non-empty array is required for ' + `'${operator.toString()}' filters.`);\n  }\n}\n/**\r\n * Given an operator, returns the set of operators that cannot be used with it.\r\n *\r\n * This is not a comprehensive check, and this function should be removed in the\r\n * long term. Validations should occur in the Firestore backend.\r\n *\r\n * Operators in a query must adhere to the following set of rules:\r\n * 1. Only one inequality per query.\r\n * 2. `NOT_IN` cannot be used with array, disjunctive, or `NOT_EQUAL` operators.\r\n */\n\n\nfunction conflictingOps(op) {\n  switch (op) {\n    case \"!=\"\n    /* Operator.NOT_EQUAL */\n    :\n      return [\"!=\"\n      /* Operator.NOT_EQUAL */\n      , \"not-in\"\n      /* Operator.NOT_IN */\n      ];\n\n    case \"array-contains-any\"\n    /* Operator.ARRAY_CONTAINS_ANY */\n    :\n    case \"in\"\n    /* Operator.IN */\n    :\n      return [\"not-in\"\n      /* Operator.NOT_IN */\n      ];\n\n    case \"not-in\"\n    /* Operator.NOT_IN */\n    :\n      return [\"array-contains-any\"\n      /* Operator.ARRAY_CONTAINS_ANY */\n      , \"in\"\n      /* Operator.IN */\n      , \"not-in\"\n      /* Operator.NOT_IN */\n      , \"!=\"\n      /* Operator.NOT_EQUAL */\n      ];\n\n    default:\n      return [];\n  }\n}\n\nfunction validateNewFieldFilter(query, fieldFilter) {\n  if (fieldFilter.isInequality()) {\n    const existingInequality = getInequalityFilterField(query);\n    const newInequality = fieldFilter.field;\n\n    if (existingInequality !== null && !existingInequality.isEqual(newInequality)) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. All where filters with an inequality' + ' (<, <=, !=, not-in, >, or >=) must be on the same field. But you have' + ` inequality filters on '${existingInequality.toString()}'` + ` and '${newInequality.toString()}'`);\n    }\n\n    const firstOrderByField = getFirstOrderByField(query);\n\n    if (firstOrderByField !== null) {\n      validateOrderByAndInequalityMatch(query, newInequality, firstOrderByField);\n    }\n  }\n\n  const conflictingOp = findOpInsideFilters(query.filters, conflictingOps(fieldFilter.op));\n\n  if (conflictingOp !== null) {\n    // Special case when it's a duplicate op to give a slightly clearer error message.\n    if (conflictingOp === fieldFilter.op) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'Invalid query. You cannot use more than one ' + `'${fieldFilter.op.toString()}' filter.`);\n    } else {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. You cannot use '${fieldFilter.op.toString()}' filters ` + `with '${conflictingOp.toString()}' filters.`);\n    }\n  }\n}\n\nfunction validateNewFilter(query, filter) {\n  let testQuery = query;\n  const subFilters = filter.getFlattenedFilters();\n\n  for (const subFilter of subFilters) {\n    validateNewFieldFilter(testQuery, subFilter);\n    testQuery = queryWithAddedFilter(testQuery, subFilter);\n  }\n} // Checks if any of the provided filter operators are included in the given list of filters and\n// returns the first one that is, or null if none are.\n\n\nfunction findOpInsideFilters(filters, operators) {\n  for (const filter of filters) {\n    for (const fieldFilter of filter.getFlattenedFilters()) {\n      if (operators.indexOf(fieldFilter.op) >= 0) {\n        return fieldFilter.op;\n      }\n    }\n  }\n\n  return null;\n}\n\nfunction validateNewOrderBy(query, orderBy) {\n  if (getFirstOrderByField(query) === null) {\n    // This is the first order by. It must match any inequality.\n    const inequalityField = getInequalityFilterField(query);\n\n    if (inequalityField !== null) {\n      validateOrderByAndInequalityMatch(query, inequalityField, orderBy.field);\n    }\n  }\n}\n\nfunction validateOrderByAndInequalityMatch(baseQuery, inequality, orderBy) {\n  if (!orderBy.isEqual(inequality)) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Invalid query. You have a where filter with an inequality ` + `(<, <=, !=, not-in, >, or >=) on field '${inequality.toString()}' ` + `and so you must also use '${inequality.toString()}' ` + `as your first argument to orderBy(), but your first orderBy() ` + `is on field '${orderBy.toString()}' instead.`);\n  }\n}\n\nfunction validateQueryFilterConstraint(functionName, queryConstraint) {\n  if (!(queryConstraint instanceof QueryFieldFilterConstraint) && !(queryConstraint instanceof QueryCompositeFilterConstraint)) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, `Function ${functionName}() requires AppliableConstraints created with a call to 'where(...)', 'or(...)', or 'and(...)'.`);\n  }\n}\n\nfunction validateQueryConstraintArray(queryConstraint) {\n  const compositeFilterCount = queryConstraint.filter(filter => filter instanceof QueryCompositeFilterConstraint).length;\n  const fieldFilterCount = queryConstraint.filter(filter => filter instanceof QueryFieldFilterConstraint).length;\n\n  if (compositeFilterCount > 1 || compositeFilterCount > 0 && fieldFilterCount > 0) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'InvalidQuery. When using composite filters, you cannot use ' + 'more than one filter at the top level. Consider nesting the multiple ' + 'filters within an `and(...)` statement. For example: ' + 'change `query(query, where(...), or(...))` to ' + '`query(query, and(where(...), or(...)))`.');\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Converts Firestore's internal types to the JavaScript types that we expose\r\n * to the user.\r\n *\r\n * @internal\r\n */\n\n\nclass AbstractUserDataWriter {\n  convertValue(value, serverTimestampBehavior = 'none') {\n    switch (typeOrder(value)) {\n      case 0\n      /* TypeOrder.NullValue */\n      :\n        return null;\n\n      case 1\n      /* TypeOrder.BooleanValue */\n      :\n        return value.booleanValue;\n\n      case 2\n      /* TypeOrder.NumberValue */\n      :\n        return normalizeNumber(value.integerValue || value.doubleValue);\n\n      case 3\n      /* TypeOrder.TimestampValue */\n      :\n        return this.convertTimestamp(value.timestampValue);\n\n      case 4\n      /* TypeOrder.ServerTimestampValue */\n      :\n        return this.convertServerTimestamp(value, serverTimestampBehavior);\n\n      case 5\n      /* TypeOrder.StringValue */\n      :\n        return value.stringValue;\n\n      case 6\n      /* TypeOrder.BlobValue */\n      :\n        return this.convertBytes(normalizeByteString(value.bytesValue));\n\n      case 7\n      /* TypeOrder.RefValue */\n      :\n        return this.convertReference(value.referenceValue);\n\n      case 8\n      /* TypeOrder.GeoPointValue */\n      :\n        return this.convertGeoPoint(value.geoPointValue);\n\n      case 9\n      /* TypeOrder.ArrayValue */\n      :\n        return this.convertArray(value.arrayValue, serverTimestampBehavior);\n\n      case 10\n      /* TypeOrder.ObjectValue */\n      :\n        return this.convertObject(value.mapValue, serverTimestampBehavior);\n\n      default:\n        throw fail();\n    }\n  }\n\n  convertObject(mapValue, serverTimestampBehavior) {\n    return this.convertObjectMap(mapValue.fields, serverTimestampBehavior);\n  }\n  /**\r\n   * @internal\r\n   */\n\n\n  convertObjectMap(fields, serverTimestampBehavior = 'none') {\n    const result = {};\n    forEach(fields, (key, value) => {\n      result[key] = this.convertValue(value, serverTimestampBehavior);\n    });\n    return result;\n  }\n\n  convertGeoPoint(value) {\n    return new GeoPoint(normalizeNumber(value.latitude), normalizeNumber(value.longitude));\n  }\n\n  convertArray(arrayValue, serverTimestampBehavior) {\n    return (arrayValue.values || []).map(value => this.convertValue(value, serverTimestampBehavior));\n  }\n\n  convertServerTimestamp(value, serverTimestampBehavior) {\n    switch (serverTimestampBehavior) {\n      case 'previous':\n        const previousValue = getPreviousValue(value);\n\n        if (previousValue == null) {\n          return null;\n        }\n\n        return this.convertValue(previousValue, serverTimestampBehavior);\n\n      case 'estimate':\n        return this.convertTimestamp(getLocalWriteTime(value));\n\n      default:\n        return null;\n    }\n  }\n\n  convertTimestamp(value) {\n    const normalizedValue = normalizeTimestamp(value);\n    return new Timestamp(normalizedValue.seconds, normalizedValue.nanos);\n  }\n\n  convertDocumentKey(name, expectedDatabaseId) {\n    const resourcePath = ResourcePath.fromString(name);\n    hardAssert(isValidResourceName(resourcePath));\n    const databaseId = new DatabaseId(resourcePath.get(1), resourcePath.get(3));\n    const key = new DocumentKey(resourcePath.popFirst(5));\n\n    if (!databaseId.isEqual(expectedDatabaseId)) {\n      // TODO(b/64130202): Somehow support foreign references.\n      logError(`Document ${key} contains a document ` + `reference within a different database (` + `${databaseId.projectId}/${databaseId.database}) which is not ` + `supported. It will be treated as a reference in the current ` + `database (${expectedDatabaseId.projectId}/${expectedDatabaseId.database}) ` + `instead.`);\n    }\n\n    return key;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Converts custom model object of type T into `DocumentData` by applying the\r\n * converter if it exists.\r\n *\r\n * This function is used when converting user objects to `DocumentData`\r\n * because we want to provide the user with a more specific error message if\r\n * their `set()` or fails due to invalid data originating from a `toFirestore()`\r\n * call.\r\n */\n\n\nfunction applyFirestoreDataConverter(converter, value, options) {\n  let convertedValue;\n\n  if (converter) {\n    if (options && (options.merge || options.mergeFields)) {\n      // Cast to `any` in order to satisfy the union type constraint on\n      // toFirestore().\n      // eslint-disable-next-line @typescript-eslint/no-explicit-any\n      convertedValue = converter.toFirestore(value, options);\n    } else {\n      convertedValue = converter.toFirestore(value);\n    }\n  } else {\n    convertedValue = value;\n  }\n\n  return convertedValue;\n}\n\nclass LiteUserDataWriter extends AbstractUserDataWriter {\n  constructor(firestore) {\n    super();\n    this.firestore = firestore;\n  }\n\n  convertBytes(bytes) {\n    return new Bytes(bytes);\n  }\n\n  convertReference(name) {\n    const key = this.convertDocumentKey(name, this.firestore._databaseId);\n    return new DocumentReference(this.firestore,\n    /* converter= */\n    null, key);\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Create an AggregateField object that can be used to compute the sum of\r\n * a specified field over a range of documents in the result set of a query.\r\n * @param field Specifies the field to sum across the result set.\r\n * @internal TODO (sum/avg) remove when public\r\n */\n\n\nfunction sum(field) {\n  return new AggregateField('sum', fieldPathFromArgument$1('sum', field));\n}\n/**\r\n * Create an AggregateField object that can be used to compute the average of\r\n * a specified field over a range of documents in the result set of a query.\r\n * @param field Specifies the field to average across the result set.\r\n * @internal TODO (sum/avg) remove when public\r\n */\n\n\nfunction average(field) {\n  return new AggregateField('avg', fieldPathFromArgument$1('average', field));\n}\n/**\r\n * Create an AggregateField object that can be used to compute the count of\r\n * documents in the result set of a query.\r\n * @internal TODO (sum/avg) remove when public\r\n */\n\n\nfunction count() {\n  return new AggregateField('count');\n}\n/**\r\n * Compares two 'AggregateField` instances for equality.\r\n *\r\n * @param left Compare this AggregateField to the `right`.\r\n * @param right Compare this AggregateField to the `left`.\r\n * @internal TODO (sum/avg) remove when public\r\n */\n\n\nfunction aggregateFieldEqual(left, right) {\n  var _a, _b;\n\n  return left instanceof AggregateField && right instanceof AggregateField && left._aggregateType === right._aggregateType && ((_a = left._internalFieldPath) === null || _a === void 0 ? void 0 : _a.canonicalString()) === ((_b = right._internalFieldPath) === null || _b === void 0 ? void 0 : _b.canonicalString());\n}\n/**\r\n * Compares two `AggregateQuerySnapshot` instances for equality.\r\n *\r\n * Two `AggregateQuerySnapshot` instances are considered \"equal\" if they have\r\n * underlying queries that compare equal, and the same data.\r\n *\r\n * @param left - The first `AggregateQuerySnapshot` to compare.\r\n * @param right - The second `AggregateQuerySnapshot` to compare.\r\n *\r\n * @returns `true` if the objects are \"equal\", as defined above, or `false`\r\n * otherwise.\r\n */\n\n\nfunction aggregateQuerySnapshotEqual(left, right) {\n  return queryEqual(left.query, right.query) && deepEqual(left.data(), right.data());\n}\n/**\r\n * @license\r\n * Copyright 2017 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction isPartialObserver(obj) {\n  return implementsAnyMethods(obj, ['next', 'error', 'complete']);\n}\n/**\r\n * Returns true if obj is an object and contains at least one of the specified\r\n * methods.\r\n */\n\n\nfunction implementsAnyMethods(obj, methods) {\n  if (typeof obj !== 'object' || obj === null) {\n    return false;\n  }\n\n  const object = obj;\n\n  for (const method of methods) {\n    if (method in object && typeof object[method] === 'function') {\n      return true;\n    }\n  }\n\n  return false;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Metadata about a snapshot, describing the state of the snapshot.\r\n */\n\n\nclass SnapshotMetadata {\n  /** @hideconstructor */\n  constructor(hasPendingWrites, fromCache) {\n    this.hasPendingWrites = hasPendingWrites;\n    this.fromCache = fromCache;\n  }\n  /**\r\n   * Returns true if this `SnapshotMetadata` is equal to the provided one.\r\n   *\r\n   * @param other - The `SnapshotMetadata` to compare against.\r\n   * @returns true if this `SnapshotMetadata` is equal to the provided one.\r\n   */\n\n\n  isEqual(other) {\n    return this.hasPendingWrites === other.hasPendingWrites && this.fromCache === other.fromCache;\n  }\n\n}\n/**\r\n * A `DocumentSnapshot` contains data read from a document in your Firestore\r\n * database. The data can be extracted with `.data()` or `.get(<field>)` to\r\n * get a specific field.\r\n *\r\n * For a `DocumentSnapshot` that points to a non-existing document, any data\r\n * access will return 'undefined'. You can use the `exists()` method to\r\n * explicitly verify a document's existence.\r\n */\n\n\nclass DocumentSnapshot extends DocumentSnapshot$1 {\n  /** @hideconstructor protected */\n  constructor(_firestore, userDataWriter, key, document, metadata, converter) {\n    super(_firestore, userDataWriter, key, document, converter);\n    this._firestore = _firestore;\n    this._firestoreImpl = _firestore;\n    this.metadata = metadata;\n  }\n  /**\r\n   * Returns whether or not the data exists. True if the document exists.\r\n   */\n\n\n  exists() {\n    return super.exists();\n  }\n  /**\r\n   * Retrieves all fields in the document as an `Object`. Returns `undefined` if\r\n   * the document doesn't exist.\r\n   *\r\n   * By default, `serverTimestamp()` values that have not yet been\r\n   * set to their final value will be returned as `null`. You can override\r\n   * this by passing an options object.\r\n   *\r\n   * @param options - An options object to configure how data is retrieved from\r\n   * the snapshot (for example the desired behavior for server timestamps that\r\n   * have not yet been set to their final value).\r\n   * @returns An `Object` containing all fields in the document or `undefined` if\r\n   * the document doesn't exist.\r\n   */\n\n\n  data(options = {}) {\n    if (!this._document) {\n      return undefined;\n    } else if (this._converter) {\n      // We only want to use the converter and create a new DocumentSnapshot\n      // if a converter has been provided.\n      const snapshot = new QueryDocumentSnapshot(this._firestore, this._userDataWriter, this._key, this._document, this.metadata,\n      /* converter= */\n      null);\n      return this._converter.fromFirestore(snapshot, options);\n    } else {\n      return this._userDataWriter.convertValue(this._document.data.value, options.serverTimestamps);\n    }\n  }\n  /**\r\n   * Retrieves the field specified by `fieldPath`. Returns `undefined` if the\r\n   * document or field doesn't exist.\r\n   *\r\n   * By default, a `serverTimestamp()` that has not yet been set to\r\n   * its final value will be returned as `null`. You can override this by\r\n   * passing an options object.\r\n   *\r\n   * @param fieldPath - The path (for example 'foo' or 'foo.bar') to a specific\r\n   * field.\r\n   * @param options - An options object to configure how the field is retrieved\r\n   * from the snapshot (for example the desired behavior for server timestamps\r\n   * that have not yet been set to their final value).\r\n   * @returns The data at the specified field location or undefined if no such\r\n   * field exists in the document.\r\n   */\n  // We are using `any` here to avoid an explicit cast by our users.\n  // eslint-disable-next-line @typescript-eslint/no-explicit-any\n\n\n  get(fieldPath, options = {}) {\n    if (this._document) {\n      const value = this._document.data.field(fieldPathFromArgument('DocumentSnapshot.get', fieldPath));\n\n      if (value !== null) {\n        return this._userDataWriter.convertValue(value, options.serverTimestamps);\n      }\n    }\n\n    return undefined;\n  }\n\n}\n/**\r\n * A `QueryDocumentSnapshot` contains data read from a document in your\r\n * Firestore database as part of a query. The document is guaranteed to exist\r\n * and its data can be extracted with `.data()` or `.get(<field>)` to get a\r\n * specific field.\r\n *\r\n * A `QueryDocumentSnapshot` offers the same API surface as a\r\n * `DocumentSnapshot`. Since query results contain only existing documents, the\r\n * `exists` property will always be true and `data()` will never return\r\n * 'undefined'.\r\n */\n\n\nclass QueryDocumentSnapshot extends DocumentSnapshot {\n  /**\r\n   * Retrieves all fields in the document as an `Object`.\r\n   *\r\n   * By default, `serverTimestamp()` values that have not yet been\r\n   * set to their final value will be returned as `null`. You can override\r\n   * this by passing an options object.\r\n   *\r\n   * @override\r\n   * @param options - An options object to configure how data is retrieved from\r\n   * the snapshot (for example the desired behavior for server timestamps that\r\n   * have not yet been set to their final value).\r\n   * @returns An `Object` containing all fields in the document.\r\n   */\n  data(options = {}) {\n    return super.data(options);\n  }\n\n}\n/**\r\n * A `QuerySnapshot` contains zero or more `DocumentSnapshot` objects\r\n * representing the results of a query. The documents can be accessed as an\r\n * array via the `docs` property or enumerated using the `forEach` method. The\r\n * number of documents can be determined via the `empty` and `size`\r\n * properties.\r\n */\n\n\nclass QuerySnapshot {\n  /** @hideconstructor */\n  constructor(_firestore, _userDataWriter, query, _snapshot) {\n    this._firestore = _firestore;\n    this._userDataWriter = _userDataWriter;\n    this._snapshot = _snapshot;\n    this.metadata = new SnapshotMetadata(_snapshot.hasPendingWrites, _snapshot.fromCache);\n    this.query = query;\n  }\n  /** An array of all the documents in the `QuerySnapshot`. */\n\n\n  get docs() {\n    const result = [];\n    this.forEach(doc => result.push(doc));\n    return result;\n  }\n  /** The number of documents in the `QuerySnapshot`. */\n\n\n  get size() {\n    return this._snapshot.docs.size;\n  }\n  /** True if there are no documents in the `QuerySnapshot`. */\n\n\n  get empty() {\n    return this.size === 0;\n  }\n  /**\r\n   * Enumerates all of the documents in the `QuerySnapshot`.\r\n   *\r\n   * @param callback - A callback to be called with a `QueryDocumentSnapshot` for\r\n   * each document in the snapshot.\r\n   * @param thisArg - The `this` binding for the callback.\r\n   */\n\n\n  forEach(callback, thisArg) {\n    this._snapshot.docs.forEach(doc => {\n      callback.call(thisArg, new QueryDocumentSnapshot(this._firestore, this._userDataWriter, doc.key, doc, new SnapshotMetadata(this._snapshot.mutatedKeys.has(doc.key), this._snapshot.fromCache), this.query.converter));\n    });\n  }\n  /**\r\n   * Returns an array of the documents changes since the last snapshot. If this\r\n   * is the first snapshot, all documents will be in the list as 'added'\r\n   * changes.\r\n   *\r\n   * @param options - `SnapshotListenOptions` that control whether metadata-only\r\n   * changes (i.e. only `DocumentSnapshot.metadata` changed) should trigger\r\n   * snapshot events.\r\n   */\n\n\n  docChanges(options = {}) {\n    const includeMetadataChanges = !!options.includeMetadataChanges;\n\n    if (includeMetadataChanges && this._snapshot.excludesMetadataChanges) {\n      throw new FirestoreError(Code.INVALID_ARGUMENT, 'To include metadata changes with your document changes, you must ' + 'also pass { includeMetadataChanges:true } to onSnapshot().');\n    }\n\n    if (!this._cachedChanges || this._cachedChangesIncludeMetadataChanges !== includeMetadataChanges) {\n      this._cachedChanges = changesFromSnapshot(this, includeMetadataChanges);\n      this._cachedChangesIncludeMetadataChanges = includeMetadataChanges;\n    }\n\n    return this._cachedChanges;\n  }\n\n}\n/** Calculates the array of `DocumentChange`s for a given `ViewSnapshot`. */\n\n\nfunction changesFromSnapshot(querySnapshot, includeMetadataChanges) {\n  if (querySnapshot._snapshot.oldDocs.isEmpty()) {\n    let index = 0;\n    return querySnapshot._snapshot.docChanges.map(change => {\n      const doc = new QueryDocumentSnapshot(querySnapshot._firestore, querySnapshot._userDataWriter, change.doc.key, change.doc, new SnapshotMetadata(querySnapshot._snapshot.mutatedKeys.has(change.doc.key), querySnapshot._snapshot.fromCache), querySnapshot.query.converter);\n      change.doc;\n      return {\n        type: 'added',\n        doc,\n        oldIndex: -1,\n        newIndex: index++\n      };\n    });\n  } else {\n    // A `DocumentSet` that is updated incrementally as changes are applied to use\n    // to lookup the index of a document.\n    let indexTracker = querySnapshot._snapshot.oldDocs;\n    return querySnapshot._snapshot.docChanges.filter(change => includeMetadataChanges || change.type !== 3\n    /* ChangeType.Metadata */\n    ).map(change => {\n      const doc = new QueryDocumentSnapshot(querySnapshot._firestore, querySnapshot._userDataWriter, change.doc.key, change.doc, new SnapshotMetadata(querySnapshot._snapshot.mutatedKeys.has(change.doc.key), querySnapshot._snapshot.fromCache), querySnapshot.query.converter);\n      let oldIndex = -1;\n      let newIndex = -1;\n\n      if (change.type !== 0\n      /* ChangeType.Added */\n      ) {\n        oldIndex = indexTracker.indexOf(change.doc.key);\n        indexTracker = indexTracker.delete(change.doc.key);\n      }\n\n      if (change.type !== 1\n      /* ChangeType.Removed */\n      ) {\n        indexTracker = indexTracker.add(change.doc);\n        newIndex = indexTracker.indexOf(change.doc.key);\n      }\n\n      return {\n        type: resultChangeType(change.type),\n        doc,\n        oldIndex,\n        newIndex\n      };\n    });\n  }\n}\n\nfunction resultChangeType(type) {\n  switch (type) {\n    case 0\n    /* ChangeType.Added */\n    :\n      return 'added';\n\n    case 2\n    /* ChangeType.Modified */\n    :\n    case 3\n    /* ChangeType.Metadata */\n    :\n      return 'modified';\n\n    case 1\n    /* ChangeType.Removed */\n    :\n      return 'removed';\n\n    default:\n      return fail();\n  }\n} // TODO(firestoreexp): Add tests for snapshotEqual with different snapshot\n// metadata\n\n/**\r\n * Returns true if the provided snapshots are equal.\r\n *\r\n * @param left - A snapshot to compare.\r\n * @param right - A snapshot to compare.\r\n * @returns true if the snapshots are equal.\r\n */\n\n\nfunction snapshotEqual(left, right) {\n  if (left instanceof DocumentSnapshot && right instanceof DocumentSnapshot) {\n    return left._firestore === right._firestore && left._key.isEqual(right._key) && (left._document === null ? right._document === null : left._document.isEqual(right._document)) && left._converter === right._converter;\n  } else if (left instanceof QuerySnapshot && right instanceof QuerySnapshot) {\n    return left._firestore === right._firestore && queryEqual(left.query, right.query) && left.metadata.isEqual(right.metadata) && left._snapshot.isEqual(right._snapshot);\n  }\n\n  return false;\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Reads the document referred to by this `DocumentReference`.\r\n *\r\n * Note: `getDoc()` attempts to provide up-to-date data when possible by waiting\r\n * for data from the server, but it may return cached data or fail if you are\r\n * offline and the server cannot be reached. To specify this behavior, invoke\r\n * {@link getDocFromCache} or {@link getDocFromServer}.\r\n *\r\n * @param reference - The reference of the document to fetch.\r\n * @returns A Promise resolved with a `DocumentSnapshot` containing the\r\n * current document contents.\r\n */\n\n\nfunction getDoc(reference) {\n  reference = cast(reference, DocumentReference);\n  const firestore = cast(reference.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientGetDocumentViaSnapshotListener(client, reference._key).then(snapshot => convertToDocSnapshot(firestore, reference, snapshot));\n}\n\nclass ExpUserDataWriter extends AbstractUserDataWriter {\n  constructor(firestore) {\n    super();\n    this.firestore = firestore;\n  }\n\n  convertBytes(bytes) {\n    return new Bytes(bytes);\n  }\n\n  convertReference(name) {\n    const key = this.convertDocumentKey(name, this.firestore._databaseId);\n    return new DocumentReference(this.firestore,\n    /* converter= */\n    null, key);\n  }\n\n}\n/**\r\n * Reads the document referred to by this `DocumentReference` from cache.\r\n * Returns an error if the document is not currently cached.\r\n *\r\n * @returns A `Promise` resolved with a `DocumentSnapshot` containing the\r\n * current document contents.\r\n */\n\n\nfunction getDocFromCache(reference) {\n  reference = cast(reference, DocumentReference);\n  const firestore = cast(reference.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  return firestoreClientGetDocumentFromLocalCache(client, reference._key).then(doc => new DocumentSnapshot(firestore, userDataWriter, reference._key, doc, new SnapshotMetadata(doc !== null && doc.hasLocalMutations,\n  /* fromCache= */\n  true), reference.converter));\n}\n/**\r\n * Reads the document referred to by this `DocumentReference` from the server.\r\n * Returns an error if the network is not available.\r\n *\r\n * @returns A `Promise` resolved with a `DocumentSnapshot` containing the\r\n * current document contents.\r\n */\n\n\nfunction getDocFromServer(reference) {\n  reference = cast(reference, DocumentReference);\n  const firestore = cast(reference.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientGetDocumentViaSnapshotListener(client, reference._key, {\n    source: 'server'\n  }).then(snapshot => convertToDocSnapshot(firestore, reference, snapshot));\n}\n/**\r\n * Executes the query and returns the results as a `QuerySnapshot`.\r\n *\r\n * Note: `getDocs()` attempts to provide up-to-date data when possible by\r\n * waiting for data from the server, but it may return cached data or fail if\r\n * you are offline and the server cannot be reached. To specify this behavior,\r\n * invoke {@link getDocsFromCache} or {@link getDocsFromServer}.\r\n *\r\n * @returns A `Promise` that will be resolved with the results of the query.\r\n */\n\n\nfunction getDocs(query) {\n  query = cast(query, Query);\n  const firestore = cast(query.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  validateHasExplicitOrderByForLimitToLast(query._query);\n  return firestoreClientGetDocumentsViaSnapshotListener(client, query._query).then(snapshot => new QuerySnapshot(firestore, userDataWriter, query, snapshot));\n}\n/**\r\n * Executes the query and returns the results as a `QuerySnapshot` from cache.\r\n * Returns an empty result set if no documents matching the query are currently\r\n * cached.\r\n *\r\n * @returns A `Promise` that will be resolved with the results of the query.\r\n */\n\n\nfunction getDocsFromCache(query) {\n  query = cast(query, Query);\n  const firestore = cast(query.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  return firestoreClientGetDocumentsFromLocalCache(client, query._query).then(snapshot => new QuerySnapshot(firestore, userDataWriter, query, snapshot));\n}\n/**\r\n * Executes the query and returns the results as a `QuerySnapshot` from the\r\n * server. Returns an error if the network is not available.\r\n *\r\n * @returns A `Promise` that will be resolved with the results of the query.\r\n */\n\n\nfunction getDocsFromServer(query) {\n  query = cast(query, Query);\n  const firestore = cast(query.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  return firestoreClientGetDocumentsViaSnapshotListener(client, query._query, {\n    source: 'server'\n  }).then(snapshot => new QuerySnapshot(firestore, userDataWriter, query, snapshot));\n}\n\nfunction setDoc(reference, data, options) {\n  reference = cast(reference, DocumentReference);\n  const firestore = cast(reference.firestore, Firestore);\n  const convertedValue = applyFirestoreDataConverter(reference.converter, data, options);\n  const dataReader = newUserDataReader(firestore);\n  const parsed = parseSetData(dataReader, 'setDoc', reference._key, convertedValue, reference.converter !== null, options);\n  const mutation = parsed.toMutation(reference._key, Precondition.none());\n  return executeWrite(firestore, [mutation]);\n}\n\nfunction updateDoc(reference, fieldOrUpdateData, value, ...moreFieldsAndValues) {\n  reference = cast(reference, DocumentReference);\n  const firestore = cast(reference.firestore, Firestore);\n  const dataReader = newUserDataReader(firestore); // For Compat types, we have to \"extract\" the underlying types before\n  // performing validation.\n\n  fieldOrUpdateData = getModularInstance(fieldOrUpdateData);\n  let parsed;\n\n  if (typeof fieldOrUpdateData === 'string' || fieldOrUpdateData instanceof FieldPath) {\n    parsed = parseUpdateVarargs(dataReader, 'updateDoc', reference._key, fieldOrUpdateData, value, moreFieldsAndValues);\n  } else {\n    parsed = parseUpdateData(dataReader, 'updateDoc', reference._key, fieldOrUpdateData);\n  }\n\n  const mutation = parsed.toMutation(reference._key, Precondition.exists(true));\n  return executeWrite(firestore, [mutation]);\n}\n/**\r\n * Deletes the document referred to by the specified `DocumentReference`.\r\n *\r\n * @param reference - A reference to the document to delete.\r\n * @returns A Promise resolved once the document has been successfully\r\n * deleted from the backend (note that it won't resolve while you're offline).\r\n */\n\n\nfunction deleteDoc(reference) {\n  const firestore = cast(reference.firestore, Firestore);\n  const mutations = [new DeleteMutation(reference._key, Precondition.none())];\n  return executeWrite(firestore, mutations);\n}\n/**\r\n * Add a new document to specified `CollectionReference` with the given data,\r\n * assigning it a document ID automatically.\r\n *\r\n * @param reference - A reference to the collection to add this document to.\r\n * @param data - An Object containing the data for the new document.\r\n * @returns A `Promise` resolved with a `DocumentReference` pointing to the\r\n * newly created document after it has been written to the backend (Note that it\r\n * won't resolve while you're offline).\r\n */\n\n\nfunction addDoc(reference, data) {\n  const firestore = cast(reference.firestore, Firestore);\n  const docRef = doc(reference);\n  const convertedValue = applyFirestoreDataConverter(reference.converter, data);\n  const dataReader = newUserDataReader(reference.firestore);\n  const parsed = parseSetData(dataReader, 'addDoc', docRef._key, convertedValue, reference.converter !== null, {});\n  const mutation = parsed.toMutation(docRef._key, Precondition.exists(false));\n  return executeWrite(firestore, [mutation]).then(() => docRef);\n}\n\nfunction onSnapshot(reference, ...args) {\n  var _a, _b, _c;\n\n  reference = getModularInstance(reference);\n  let options = {\n    includeMetadataChanges: false\n  };\n  let currArg = 0;\n\n  if (typeof args[currArg] === 'object' && !isPartialObserver(args[currArg])) {\n    options = args[currArg];\n    currArg++;\n  }\n\n  const internalOptions = {\n    includeMetadataChanges: options.includeMetadataChanges\n  };\n\n  if (isPartialObserver(args[currArg])) {\n    const userObserver = args[currArg];\n    args[currArg] = (_a = userObserver.next) === null || _a === void 0 ? void 0 : _a.bind(userObserver);\n    args[currArg + 1] = (_b = userObserver.error) === null || _b === void 0 ? void 0 : _b.bind(userObserver);\n    args[currArg + 2] = (_c = userObserver.complete) === null || _c === void 0 ? void 0 : _c.bind(userObserver);\n  }\n\n  let observer;\n  let firestore;\n  let internalQuery;\n\n  if (reference instanceof DocumentReference) {\n    firestore = cast(reference.firestore, Firestore);\n    internalQuery = newQueryForPath(reference._key.path);\n    observer = {\n      next: snapshot => {\n        if (args[currArg]) {\n          args[currArg](convertToDocSnapshot(firestore, reference, snapshot));\n        }\n      },\n      error: args[currArg + 1],\n      complete: args[currArg + 2]\n    };\n  } else {\n    const query = cast(reference, Query);\n    firestore = cast(query.firestore, Firestore);\n    internalQuery = query._query;\n    const userDataWriter = new ExpUserDataWriter(firestore);\n    observer = {\n      next: snapshot => {\n        if (args[currArg]) {\n          args[currArg](new QuerySnapshot(firestore, userDataWriter, query, snapshot));\n        }\n      },\n      error: args[currArg + 1],\n      complete: args[currArg + 2]\n    };\n    validateHasExplicitOrderByForLimitToLast(reference._query);\n  }\n\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientListen(client, internalQuery, internalOptions, observer);\n}\n\nfunction onSnapshotsInSync(firestore, arg) {\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const observer = isPartialObserver(arg) ? arg : {\n    next: arg\n  };\n  return firestoreClientAddSnapshotsInSyncListener(client, observer);\n}\n/**\r\n * Locally writes `mutations` on the async queue.\r\n * @internal\r\n */\n\n\nfunction executeWrite(firestore, mutations) {\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientWrite(client, mutations);\n}\n/**\r\n * Converts a {@link ViewSnapshot} that contains the single document specified by `ref`\r\n * to a {@link DocumentSnapshot}.\r\n */\n\n\nfunction convertToDocSnapshot(firestore, ref, snapshot) {\n  const doc = snapshot.docs.get(ref._key);\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  return new DocumentSnapshot(firestore, userDataWriter, ref._key, doc, new SnapshotMetadata(snapshot.hasPendingWrites, snapshot.fromCache), ref.converter);\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Calculates the number of documents in the result set of the given query,\r\n * without actually downloading the documents.\r\n *\r\n * Using this function to count the documents is efficient because only the\r\n * final count, not the documents' data, is downloaded. This function can even\r\n * count the documents if the result set would be prohibitively large to\r\n * download entirely (e.g. thousands of documents).\r\n *\r\n * The result received from the server is presented, unaltered, without\r\n * considering any local state. That is, documents in the local cache are not\r\n * taken into consideration, neither are local modifications not yet\r\n * synchronized with the server. Previously-downloaded results, if any, are not\r\n * used: every request using this source necessarily involves a round trip to\r\n * the server.\r\n *\r\n * @param query - The query whose result set size to calculate.\r\n * @returns A Promise that will be resolved with the count; the count can be\r\n * retrieved from `snapshot.data().count`, where `snapshot` is the\r\n * `AggregateQuerySnapshot` to which the returned Promise resolves.\r\n */\n\n\nfunction getCountFromServer(query) {\n  const countQuerySpec = {\n    count: count()\n  };\n  return getAggregateFromServer(query, countQuerySpec);\n}\n/**\r\n * Calculates the specified aggregations over the documents in the result\r\n * set of the given query, without actually downloading the documents.\r\n *\r\n * Using this function to perform aggregations is efficient because only the\r\n * final aggregation values, not the documents' data, is downloaded. This\r\n * function can even perform aggregations of the documents if the result set\r\n * would be prohibitively large to download entirely (e.g. thousands of documents).\r\n *\r\n * The result received from the server is presented, unaltered, without\r\n * considering any local state. That is, documents in the local cache are not\r\n * taken into consideration, neither are local modifications not yet\r\n * synchronized with the server. Previously-downloaded results, if any, are not\r\n * used: every request using this source necessarily involves a round trip to\r\n * the server.\r\n *\r\n * @param query The query whose result set to aggregate over.\r\n * @param aggregateSpec An `AggregateSpec` object that specifies the aggregates\r\n * to perform over the result set. The AggregateSpec specifies aliases for each\r\n * aggregate, which can be used to retrieve the aggregate result.\r\n * @example\r\n * ```typescript\r\n * const aggregateSnapshot = await getAggregateFromServer(query, {\r\n *   countOfDocs: count(),\r\n *   totalHours: sum('hours'),\r\n *   averageScore: average('score')\r\n * });\r\n *\r\n * const countOfDocs: number = aggregateSnapshot.data().countOfDocs;\r\n * const totalHours: number = aggregateSnapshot.data().totalHours;\r\n * const averageScore: number | null = aggregateSnapshot.data().averageScore;\r\n * ```\r\n * @internal TODO (sum/avg) remove when public\r\n */\n\n\nfunction getAggregateFromServer(query, aggregateSpec) {\n  const firestore = cast(query.firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n  const internalAggregates = mapToArray(aggregateSpec, (aggregate, alias) => {\n    return new AggregateImpl(alias, aggregate._aggregateType, aggregate._internalFieldPath);\n  }); // Run the aggregation and convert the results\n\n  return firestoreClientRunAggregateQuery(client, query._query, internalAggregates).then(aggregateResult => convertToAggregateQuerySnapshot(firestore, query, aggregateResult));\n}\n/**\r\n * Converts the core aggregration result to an `AggregateQuerySnapshot`\r\n * that can be returned to the consumer.\r\n * @param query\r\n * @param aggregateResult Core aggregation result\r\n * @internal\r\n */\n\n\nfunction convertToAggregateQuerySnapshot(firestore, query, aggregateResult) {\n  const userDataWriter = new ExpUserDataWriter(firestore);\n  const querySnapshot = new AggregateQuerySnapshot(query, userDataWriter, aggregateResult);\n  return querySnapshot;\n}\n/**\r\n * @license\r\n * Copyright 2023 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nclass MemoryLocalCacheImpl {\n  constructor(settings) {\n    this.kind = 'memory';\n    this._onlineComponentProvider = new OnlineComponentProvider();\n\n    if (settings === null || settings === void 0 ? void 0 : settings.garbageCollector) {\n      this._offlineComponentProvider = settings.garbageCollector._offlineComponentProvider;\n    } else {\n      this._offlineComponentProvider = new MemoryOfflineComponentProvider();\n    }\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n\n}\n\nclass PersistentLocalCacheImpl {\n  constructor(settings) {\n    this.kind = 'persistent';\n    let tabManager;\n\n    if (settings === null || settings === void 0 ? void 0 : settings.tabManager) {\n      settings.tabManager._initialize(settings);\n\n      tabManager = settings.tabManager;\n    } else {\n      tabManager = persistentSingleTabManager(undefined);\n\n      tabManager._initialize(settings);\n    }\n\n    this._onlineComponentProvider = tabManager._onlineComponentProvider;\n    this._offlineComponentProvider = tabManager._offlineComponentProvider;\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n\n}\n\nclass MemoryEagerGabageCollectorImpl {\n  constructor() {\n    this.kind = 'memoryEager';\n    this._offlineComponentProvider = new MemoryOfflineComponentProvider();\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n\n}\n\nclass MemoryLruGabageCollectorImpl {\n  constructor(cacheSize) {\n    this.kind = 'memoryLru';\n    this._offlineComponentProvider = new LruGcMemoryOfflineComponentProvider(cacheSize);\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n\n}\n/**\r\n * Creates an instance of `MemoryEagerGarbageCollector`. This is also the\r\n * default garbage collector unless it is explicitly specified otherwise.\r\n */\n\n\nfunction memoryEagerGarbageCollector() {\n  return new MemoryEagerGabageCollectorImpl();\n}\n/**\r\n * Creates an instance of `MemoryLruGarbageCollector`.\r\n *\r\n * A target size can be specified as part of the setting parameter. The\r\n * collector will start deleting documents once the cache size exceeds\r\n * the given size. The default cache size is 40MB (40 * 1024 * 1024 bytes).\r\n */\n\n\nfunction memoryLruGarbageCollector(settings) {\n  return new MemoryLruGabageCollectorImpl(settings === null || settings === void 0 ? void 0 : settings.cacheSizeBytes);\n}\n/**\r\n * Creates an instance of `MemoryLocalCache`. The instance can be set to\r\n * `FirestoreSettings.cache` to tell the SDK which cache layer to use.\r\n */\n\n\nfunction memoryLocalCache(settings) {\n  return new MemoryLocalCacheImpl(settings);\n}\n/**\r\n * Creates an instance of `PersistentLocalCache`. The instance can be set to\r\n * `FirestoreSettings.cache` to tell the SDK which cache layer to use.\r\n *\r\n * Persistent cache cannot be used in a Node.js environment.\r\n */\n\n\nfunction persistentLocalCache(settings) {\n  return new PersistentLocalCacheImpl(settings);\n}\n\nclass SingleTabManagerImpl {\n  constructor(forceOwnership) {\n    this.forceOwnership = forceOwnership;\n    this.kind = 'persistentSingleTab';\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n  /**\r\n   * @internal\r\n   */\n\n\n  _initialize(settings) {\n    this._onlineComponentProvider = new OnlineComponentProvider();\n    this._offlineComponentProvider = new IndexedDbOfflineComponentProvider(this._onlineComponentProvider, settings === null || settings === void 0 ? void 0 : settings.cacheSizeBytes, this.forceOwnership);\n  }\n\n}\n\nclass MultiTabManagerImpl {\n  constructor() {\n    this.kind = 'PersistentMultipleTab';\n  }\n\n  toJSON() {\n    return {\n      kind: this.kind\n    };\n  }\n  /**\r\n   * @internal\r\n   */\n\n\n  _initialize(settings) {\n    this._onlineComponentProvider = new OnlineComponentProvider();\n    this._offlineComponentProvider = new MultiTabOfflineComponentProvider(this._onlineComponentProvider, settings === null || settings === void 0 ? void 0 : settings.cacheSizeBytes);\n  }\n\n}\n/**\r\n * Creates an instance of `PersistentSingleTabManager`.\r\n *\r\n * @param settings Configures the created tab manager.\r\n */\n\n\nfunction persistentSingleTabManager(settings) {\n  return new SingleTabManagerImpl(settings === null || settings === void 0 ? void 0 : settings.forceOwnership);\n}\n/**\r\n * Creates an instance of `PersistentMultipleTabManager`.\r\n */\n\n\nfunction persistentMultipleTabManager() {\n  return new MultiTabManagerImpl();\n}\n/**\r\n * @license\r\n * Copyright 2022 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nconst DEFAULT_TRANSACTION_OPTIONS = {\n  maxAttempts: 5\n};\n\nfunction validateTransactionOptions(options) {\n  if (options.maxAttempts < 1) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Max attempts must be at least 1');\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A write batch, used to perform multiple writes as a single atomic unit.\r\n *\r\n * A `WriteBatch` object can be acquired by calling {@link writeBatch}. It\r\n * provides methods for adding writes to the write batch. None of the writes\r\n * will be committed (or visible locally) until {@link WriteBatch.commit} is\r\n * called.\r\n */\n\n\nclass WriteBatch {\n  /** @hideconstructor */\n  constructor(_firestore, _commitHandler) {\n    this._firestore = _firestore;\n    this._commitHandler = _commitHandler;\n    this._mutations = [];\n    this._committed = false;\n    this._dataReader = newUserDataReader(_firestore);\n  }\n\n  set(documentRef, data, options) {\n    this._verifyNotCommitted();\n\n    const ref = validateReference(documentRef, this._firestore);\n    const convertedValue = applyFirestoreDataConverter(ref.converter, data, options);\n    const parsed = parseSetData(this._dataReader, 'WriteBatch.set', ref._key, convertedValue, ref.converter !== null, options);\n\n    this._mutations.push(parsed.toMutation(ref._key, Precondition.none()));\n\n    return this;\n  }\n\n  update(documentRef, fieldOrUpdateData, value, ...moreFieldsAndValues) {\n    this._verifyNotCommitted();\n\n    const ref = validateReference(documentRef, this._firestore); // For Compat types, we have to \"extract\" the underlying types before\n    // performing validation.\n\n    fieldOrUpdateData = getModularInstance(fieldOrUpdateData);\n    let parsed;\n\n    if (typeof fieldOrUpdateData === 'string' || fieldOrUpdateData instanceof FieldPath) {\n      parsed = parseUpdateVarargs(this._dataReader, 'WriteBatch.update', ref._key, fieldOrUpdateData, value, moreFieldsAndValues);\n    } else {\n      parsed = parseUpdateData(this._dataReader, 'WriteBatch.update', ref._key, fieldOrUpdateData);\n    }\n\n    this._mutations.push(parsed.toMutation(ref._key, Precondition.exists(true)));\n\n    return this;\n  }\n  /**\r\n   * Deletes the document referred to by the provided {@link DocumentReference}.\r\n   *\r\n   * @param documentRef - A reference to the document to be deleted.\r\n   * @returns This `WriteBatch` instance. Used for chaining method calls.\r\n   */\n\n\n  delete(documentRef) {\n    this._verifyNotCommitted();\n\n    const ref = validateReference(documentRef, this._firestore);\n    this._mutations = this._mutations.concat(new DeleteMutation(ref._key, Precondition.none()));\n    return this;\n  }\n  /**\r\n   * Commits all of the writes in this write batch as a single atomic unit.\r\n   *\r\n   * The result of these writes will only be reflected in document reads that\r\n   * occur after the returned promise resolves. If the client is offline, the\r\n   * write fails. If you would like to see local modifications or buffer writes\r\n   * until the client is online, use the full Firestore SDK.\r\n   *\r\n   * @returns A `Promise` resolved once all of the writes in the batch have been\r\n   * successfully written to the backend as an atomic unit (note that it won't\r\n   * resolve while you're offline).\r\n   */\n\n\n  commit() {\n    this._verifyNotCommitted();\n\n    this._committed = true;\n\n    if (this._mutations.length > 0) {\n      return this._commitHandler(this._mutations);\n    }\n\n    return Promise.resolve();\n  }\n\n  _verifyNotCommitted() {\n    if (this._committed) {\n      throw new FirestoreError(Code.FAILED_PRECONDITION, 'A write batch can no longer be used after commit() ' + 'has been called.');\n    }\n  }\n\n}\n\nfunction validateReference(documentRef, firestore) {\n  documentRef = getModularInstance(documentRef);\n\n  if (documentRef.firestore !== firestore) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Provided document reference is from a different Firestore instance.');\n  } else {\n    return documentRef;\n  }\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n// TODO(mrschmidt) Consider using `BaseTransaction` as the base class in the\n// legacy SDK.\n\n/**\r\n * A reference to a transaction.\r\n *\r\n * The `Transaction` object passed to a transaction's `updateFunction` provides\r\n * the methods to read and write data within the transaction context. See\r\n * {@link runTransaction}.\r\n */\n\n\nclass Transaction$1 {\n  /** @hideconstructor */\n  constructor(_firestore, _transaction) {\n    this._firestore = _firestore;\n    this._transaction = _transaction;\n    this._dataReader = newUserDataReader(_firestore);\n  }\n  /**\r\n   * Reads the document referenced by the provided {@link DocumentReference}.\r\n   *\r\n   * @param documentRef - A reference to the document to be read.\r\n   * @returns A `DocumentSnapshot` with the read data.\r\n   */\n\n\n  get(documentRef) {\n    const ref = validateReference(documentRef, this._firestore);\n    const userDataWriter = new LiteUserDataWriter(this._firestore);\n    return this._transaction.lookup([ref._key]).then(docs => {\n      if (!docs || docs.length !== 1) {\n        return fail();\n      }\n\n      const doc = docs[0];\n\n      if (doc.isFoundDocument()) {\n        return new DocumentSnapshot$1(this._firestore, userDataWriter, doc.key, doc, ref.converter);\n      } else if (doc.isNoDocument()) {\n        return new DocumentSnapshot$1(this._firestore, userDataWriter, ref._key, null, ref.converter);\n      } else {\n        throw fail();\n      }\n    });\n  }\n\n  set(documentRef, value, options) {\n    const ref = validateReference(documentRef, this._firestore);\n    const convertedValue = applyFirestoreDataConverter(ref.converter, value, options);\n    const parsed = parseSetData(this._dataReader, 'Transaction.set', ref._key, convertedValue, ref.converter !== null, options);\n\n    this._transaction.set(ref._key, parsed);\n\n    return this;\n  }\n\n  update(documentRef, fieldOrUpdateData, value, ...moreFieldsAndValues) {\n    const ref = validateReference(documentRef, this._firestore); // For Compat types, we have to \"extract\" the underlying types before\n    // performing validation.\n\n    fieldOrUpdateData = getModularInstance(fieldOrUpdateData);\n    let parsed;\n\n    if (typeof fieldOrUpdateData === 'string' || fieldOrUpdateData instanceof FieldPath) {\n      parsed = parseUpdateVarargs(this._dataReader, 'Transaction.update', ref._key, fieldOrUpdateData, value, moreFieldsAndValues);\n    } else {\n      parsed = parseUpdateData(this._dataReader, 'Transaction.update', ref._key, fieldOrUpdateData);\n    }\n\n    this._transaction.update(ref._key, parsed);\n\n    return this;\n  }\n  /**\r\n   * Deletes the document referred to by the provided {@link DocumentReference}.\r\n   *\r\n   * @param documentRef - A reference to the document to be deleted.\r\n   * @returns This `Transaction` instance. Used for chaining method calls.\r\n   */\n\n\n  delete(documentRef) {\n    const ref = validateReference(documentRef, this._firestore);\n\n    this._transaction.delete(ref._key);\n\n    return this;\n  }\n\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * A reference to a transaction.\r\n *\r\n * The `Transaction` object passed to a transaction's `updateFunction` provides\r\n * the methods to read and write data within the transaction context. See\r\n * {@link runTransaction}.\r\n */\n\n\nclass Transaction extends Transaction$1 {\n  // This class implements the same logic as the Transaction API in the Lite SDK\n  // but is subclassed in order to return its own DocumentSnapshot types.\n\n  /** @hideconstructor */\n  constructor(_firestore, _transaction) {\n    super(_firestore, _transaction);\n    this._firestore = _firestore;\n  }\n  /**\r\n   * Reads the document referenced by the provided {@link DocumentReference}.\r\n   *\r\n   * @param documentRef - A reference to the document to be read.\r\n   * @returns A `DocumentSnapshot` with the read data.\r\n   */\n\n\n  get(documentRef) {\n    const ref = validateReference(documentRef, this._firestore);\n    const userDataWriter = new ExpUserDataWriter(this._firestore);\n    return super.get(documentRef).then(liteDocumentSnapshot => new DocumentSnapshot(this._firestore, userDataWriter, ref._key, liteDocumentSnapshot._document, new SnapshotMetadata(\n    /* hasPendingWrites= */\n    false,\n    /* fromCache= */\n    false), ref.converter));\n  }\n\n}\n/**\r\n * Executes the given `updateFunction` and then attempts to commit the changes\r\n * applied within the transaction. If any document read within the transaction\r\n * has changed, Cloud Firestore retries the `updateFunction`. If it fails to\r\n * commit after 5 attempts, the transaction fails.\r\n *\r\n * The maximum number of writes allowed in a single transaction is 500.\r\n *\r\n * @param firestore - A reference to the Firestore database to run this\r\n * transaction against.\r\n * @param updateFunction - The function to execute within the transaction\r\n * context.\r\n * @param options - An options object to configure maximum number of attempts to\r\n * commit.\r\n * @returns If the transaction completed successfully or was explicitly aborted\r\n * (the `updateFunction` returned a failed promise), the promise returned by the\r\n * `updateFunction `is returned here. Otherwise, if the transaction failed, a\r\n * rejected promise with the corresponding failure error is returned.\r\n */\n\n\nfunction runTransaction(firestore, updateFunction, options) {\n  firestore = cast(firestore, Firestore);\n  const optionsWithDefaults = Object.assign(Object.assign({}, DEFAULT_TRANSACTION_OPTIONS), options);\n  validateTransactionOptions(optionsWithDefaults);\n  const client = ensureFirestoreConfigured(firestore);\n  return firestoreClientTransaction(client, internalTransaction => updateFunction(new Transaction(firestore, internalTransaction)), optionsWithDefaults);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Returns a sentinel for use with {@link @firebase/firestore/lite#(updateDoc:1)} or\r\n * {@link @firebase/firestore/lite#(setDoc:1)} with `{merge: true}` to mark a field for deletion.\r\n */\n\n\nfunction deleteField() {\n  return new DeleteFieldValueImpl('deleteField');\n}\n/**\r\n * Returns a sentinel used with {@link @firebase/firestore/lite#(setDoc:1)} or {@link @firebase/firestore/lite#(updateDoc:1)} to\r\n * include a server-generated timestamp in the written data.\r\n */\n\n\nfunction serverTimestamp() {\n  return new ServerTimestampFieldValueImpl('serverTimestamp');\n}\n/**\r\n * Returns a special value that can be used with {@link @firebase/firestore/lite#(setDoc:1)} or {@link\r\n * @firebase/firestore/lite#(updateDoc:1)} that tells the server to union the given elements with any array\r\n * value that already exists on the server. Each specified element that doesn't\r\n * already exist in the array will be added to the end. If the field being\r\n * modified is not already an array it will be overwritten with an array\r\n * containing exactly the specified elements.\r\n *\r\n * @param elements - The elements to union into the array.\r\n * @returns The `FieldValue` sentinel for use in a call to `setDoc()` or\r\n * `updateDoc()`.\r\n */\n\n\nfunction arrayUnion(...elements) {\n  // NOTE: We don't actually parse the data until it's used in set() or\n  // update() since we'd need the Firestore instance to do this.\n  return new ArrayUnionFieldValueImpl('arrayUnion', elements);\n}\n/**\r\n * Returns a special value that can be used with {@link (setDoc:1)} or {@link\r\n * updateDoc:1} that tells the server to remove the given elements from any\r\n * array value that already exists on the server. All instances of each element\r\n * specified will be removed from the array. If the field being modified is not\r\n * already an array it will be overwritten with an empty array.\r\n *\r\n * @param elements - The elements to remove from the array.\r\n * @returns The `FieldValue` sentinel for use in a call to `setDoc()` or\r\n * `updateDoc()`\r\n */\n\n\nfunction arrayRemove(...elements) {\n  // NOTE: We don't actually parse the data until it's used in set() or\n  // update() since we'd need the Firestore instance to do this.\n  return new ArrayRemoveFieldValueImpl('arrayRemove', elements);\n}\n/**\r\n * Returns a special value that can be used with {@link @firebase/firestore/lite#(setDoc:1)} or {@link\r\n * @firebase/firestore/lite#(updateDoc:1)} that tells the server to increment the field's current value by\r\n * the given value.\r\n *\r\n * If either the operand or the current field value uses floating point\r\n * precision, all arithmetic follows IEEE 754 semantics. If both values are\r\n * integers, values outside of JavaScript's safe number range\r\n * (`Number.MIN_SAFE_INTEGER` to `Number.MAX_SAFE_INTEGER`) are also subject to\r\n * precision loss. Furthermore, once processed by the Firestore backend, all\r\n * integer operations are capped between -2^63 and 2^63-1.\r\n *\r\n * If the current field value is not of type `number`, or if the field does not\r\n * yet exist, the transformation sets the field to the given value.\r\n *\r\n * @param n - The value to increment by.\r\n * @returns The `FieldValue` sentinel for use in a call to `setDoc()` or\r\n * `updateDoc()`\r\n */\n\n\nfunction increment(n) {\n  return new NumericIncrementFieldValueImpl('increment', n);\n}\n/**\r\n * @license\r\n * Copyright 2020 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n/**\r\n * Creates a write batch, used for performing multiple writes as a single\r\n * atomic operation. The maximum number of writes allowed in a single {@link WriteBatch}\r\n * is 500.\r\n *\r\n * Unlike transactions, write batches are persisted offline and therefore are\r\n * preferable when you don't need to condition your writes on read data.\r\n *\r\n * @returns A {@link WriteBatch} that can be used to atomically execute multiple\r\n * writes.\r\n */\n\n\nfunction writeBatch(firestore) {\n  firestore = cast(firestore, Firestore);\n  ensureFirestoreConfigured(firestore);\n  return new WriteBatch(firestore, mutations => executeWrite(firestore, mutations));\n}\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nfunction setIndexConfiguration(firestore, jsonOrConfiguration) {\n  var _a;\n\n  firestore = cast(firestore, Firestore);\n  const client = ensureFirestoreConfigured(firestore);\n\n  if (!client._uninitializedComponentsProvider || ((_a = client._uninitializedComponentsProvider) === null || _a === void 0 ? void 0 : _a._offlineKind) === 'memory') {\n    // PORTING NOTE: We don't return an error if the user has not enabled\n    // persistence since `enableIndexeddbPersistence()` can fail on the Web.\n    logWarn('Cannot enable indexes when persistence is disabled');\n    return Promise.resolve();\n  }\n\n  const parsedIndexes = parseIndexes(jsonOrConfiguration);\n  return firestoreClientSetIndexConfiguration(client, parsedIndexes);\n}\n\nfunction parseIndexes(jsonOrConfiguration) {\n  const indexConfiguration = typeof jsonOrConfiguration === 'string' ? tryParseJson(jsonOrConfiguration) : jsonOrConfiguration;\n  const parsedIndexes = [];\n\n  if (Array.isArray(indexConfiguration.indexes)) {\n    for (const index of indexConfiguration.indexes) {\n      const collectionGroup = tryGetString(index, 'collectionGroup');\n      const segments = [];\n\n      if (Array.isArray(index.fields)) {\n        for (const field of index.fields) {\n          const fieldPathString = tryGetString(field, 'fieldPath');\n          const fieldPath = fieldPathFromDotSeparatedString('setIndexConfiguration', fieldPathString);\n\n          if (field.arrayConfig === 'CONTAINS') {\n            segments.push(new IndexSegment(fieldPath, 2\n            /* IndexKind.CONTAINS */\n            ));\n          } else if (field.order === 'ASCENDING') {\n            segments.push(new IndexSegment(fieldPath, 0\n            /* IndexKind.ASCENDING */\n            ));\n          } else if (field.order === 'DESCENDING') {\n            segments.push(new IndexSegment(fieldPath, 1\n            /* IndexKind.DESCENDING */\n            ));\n          }\n        }\n      }\n\n      parsedIndexes.push(new FieldIndex(FieldIndex.UNKNOWN_ID, collectionGroup, segments, IndexState.empty()));\n    }\n  }\n\n  return parsedIndexes;\n}\n\nfunction tryParseJson(json) {\n  try {\n    return JSON.parse(json);\n  } catch (e) {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Failed to parse JSON: ' + (e === null || e === void 0 ? void 0 : e.message));\n  }\n}\n\nfunction tryGetString(data, property) {\n  if (typeof data[property] !== 'string') {\n    throw new FirestoreError(Code.INVALID_ARGUMENT, 'Missing string value for: ' + property);\n  }\n\n  return data[property];\n}\n/**\r\n * @license\r\n * Copyright 2021 Google LLC\r\n *\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *   http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\n\n\nregisterFirestore('node');\nexport { AbstractUserDataWriter, AggregateField, AggregateQuerySnapshot, Bytes, CACHE_SIZE_UNLIMITED, CollectionReference, DocumentReference, DocumentSnapshot, FieldPath, FieldValue, Firestore, FirestoreError, GeoPoint, LoadBundleTask, Query, QueryCompositeFilterConstraint, QueryConstraint, QueryDocumentSnapshot, QueryEndAtConstraint, QueryFieldFilterConstraint, QueryLimitConstraint, QueryOrderByConstraint, QuerySnapshot, QueryStartAtConstraint, SnapshotMetadata, Timestamp, Transaction, WriteBatch, DatabaseId as _DatabaseId, DocumentKey as _DocumentKey, EmptyAppCheckTokenProvider as _EmptyAppCheckTokenProvider, EmptyAuthCredentialsProvider as _EmptyAuthCredentialsProvider, FieldPath$1 as _FieldPath, TestingHooks as _TestingHooks, cast as _cast, debugAssert as _debugAssert, isBase64Available as _isBase64Available, logWarn as _logWarn, validateIsNotUsedTogether as _validateIsNotUsedTogether, addDoc, aggregateFieldEqual, aggregateQuerySnapshotEqual, and, arrayRemove, arrayUnion, average, clearIndexedDbPersistence, collection, collectionGroup, connectFirestoreEmulator, count, deleteDoc, deleteField, disableNetwork, doc, documentId, enableIndexedDbPersistence, enableMultiTabIndexedDbPersistence, enableNetwork, endAt, endBefore, ensureFirestoreConfigured, executeWrite, getAggregateFromServer, getCountFromServer, getDoc, getDocFromCache, getDocFromServer, getDocs, getDocsFromCache, getDocsFromServer, getFirestore, increment, initializeFirestore, limit, limitToLast, loadBundle, memoryEagerGarbageCollector, memoryLocalCache, memoryLruGarbageCollector, namedQuery, onSnapshot, onSnapshotsInSync, or, orderBy, persistentLocalCache, persistentMultipleTabManager, persistentSingleTabManager, query, queryEqual, refEqual, runTransaction, serverTimestamp, setDoc, setIndexConfiguration, setLogLevel, snapshotEqual, startAfter, startAt, sum, terminate, updateDoc, waitForPendingWrites, where, writeBatch }; //# sourceMappingURL=index.node.mjs.map","map":null,"metadata":{},"sourceType":"module"}